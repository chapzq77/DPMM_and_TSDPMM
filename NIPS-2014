kernel mean estimation spectral filtering problem estimating kernel mean reproducing kernel hilbert space rkhs central kernel methods used classical approaches centering kernel pca matrix forms core inference step modern kernel methods kernel based non parametric tests rely embedding probability distributions previous work shown shrinkage constructing estimators kernel mean empirical estimator present paper studies consistency estimators proposes wider class shrinkage estimators improve empirical estimator considering appropriate basis functions using kernel pca basis show estimators constructed using spectral filtering algorithms shown consistent technical assumptions theoretical analysis reveals fundamental connection kernel based supervised learning framework proposed estimators simple implement perform practice 
semi separable hamiltonian monte carlo inference bayesian hierarchical models sampling hierarchical bayesian models difficult mcmc methods because strong correlations model parameters hyperparameters riemannian manifold hamiltonian monte carlo methods significant potential advantages setting computationally expensive introduce method call semi separable hamiltonian monte carlo uses designed mass matrix allows joint hamiltonian model parameters hyperparameters decompose simpler structure call alternating algorithm resulting method faster simpler gibbs sampling being simpler efficient previous instances 
communication efficient distributed machine learning parameter paper describes third generation parameter framework distributed machine learning framework offers relaxations balance system performance algorithm efficiency propose algorithm takes advantage framework solve non convex non smooth problems convergence guarantees present depth analysis large scale machine learning problems ranging regularized logistic regression reconstruction gpus using real data billions samples dimensions demonstrate using examples parameter framework effective straightforward way scale machine learning larger problems systems previously achieved 
infinite mixture infinite gaussian mixtures dirichlet process mixture gaussians used literature clustering density estimation problems however many real world data exhibit cluster distributions captured single gaussian modeling data sets several extraneous clusters clusters relatively defined present infinite mixture infinite gaussian mixtures gmm flexible modeling data sets skewed multi modal cluster distributions using single gaussian cluster standard model generative model gmm uses single cluster individual together centering base distributions atoms higher level prior inference performed collapsed gibbs sampler enables partial parallelization experimental results several artificial real world data sets suggest proposed gmm model predict clusters accurately existing variational bayes gibbs sampler versions 
robust classification sample selection bias many important machine learning applications source distribution used estimate probabilistic classifier differs target distribution classifier used predictions due its asymptotic properties sample reweighted loss minimization commonly employed technique deal difference however given finite amounts labeled source data technique suffers significant estimation errors settings large sample selection bias develop framework robustly learning probabilistic classifier adapts different sample selection biases using minimax estimation formulation approach requires only accurate estimates statistics source distribution otherwise robust possible unknown properties conditional label distribution explicit generalization assumptions demonstrate behavior effectiveness approach synthetic binary classification tasks 
hull learning nonconvex data selecting small informative subset given dataset called column sampling drawn attention machine learning incorporating structured data information column sampling research efforts devoted cases data points fitted clusters general convex paper aims study nonconvex hull learning investigated literature order learn data adaptive nonconvex novel approach proposed based graph theoretic measure leverages graph cycles characterize structural complexities input data points employing measure present greedy algorithmic framework perform structured column sampling process hull involves computation matrix inverse accelerate matrix inversion computation reduce its space complexity exploit low rank approximation graph adjacency matrix using efficient graph technique extensive experimental results show data representation learned achieve state art accuracy image classification tasks 
based low rank trajectory completion reconstruction extracting shape objects monocular videos task known non rigid structure motion far studied only synthetic datasets controlled environments typically objects reconstruct pre exhibit limited rotations full length trajectories assumed order integrate current video analysis pipelines needs consider input realistic incomplete tracking perform spatio temporal segment objects furthermore needs robust noise segmentation tracking drifting segmentation optical flow paper attempt towards goal propose method combines dense optical flow tracking motion trajectory clustering reconstruction objects videos trajectory cluster compute multiple minimizing error rank shape different rank bounds trajectory matrix show dense shape extracted trajectories completed low regions mild relative motion object camera achieve competitive results public benchmark using fixed parameters sequences handling incomplete trajectories contrast existing approaches approach popular video segmentation datasets best knowledge method extract dense object models realistic videos hollywood object specific priors 
sparse space time deconvolution calcium image analysis describe unified formulation algorithm extremely sparse representation calcium image sequences terms cell locations cell shapes spike responses solution single optimization problem yields cell activity estimates par state art need heuristic pre experiments real synthetic data demonstrate viability proposed method 
restricted boltzmann machines modeling human choice extend multinomial logit model represent empirical phenomena frequently observed choices humans phenomena include similarity effect attraction effect effect formally quantify strength phenomena represented choice model flexibility choice model show choice model represented restricted boltzmann machine its parameters learned effectively data numerical experiments real data human choices suggest train choice model way represents typical phenomena choice 
multiscale fields patterns describe framework defining high order image models used variety applications approach involves modeling local patterns multiscale representation image local properties image reflect non local properties original image case binary images local properties defined binary patterns observed small neighborhoods pixel multiscale representation capture frequency patterns observed different scales resolution framework leads expressive priors depend relatively small number parameters inference learning mcmc method block sampling very large blocks evaluate approach example applications involves detection involves binary segmentation 
large scale canonical correlation analysis iterative squares canonical correlation analysis widely used statistical tool established theory favorable performance wide range machine learning problems however computing huge datasets very slow since involves implementing decomposition singular value decomposition huge matrices paper introduce iterative algorithm compute fast huge sparse datasets theory asymptotic convergence finite time accuracy established experiments show outperform fast approximation schemes real datasets 
training strong bounds single layer dropout dropout training originally designed deep neural networks successful high dimensional single layer natural language tasks paper proposes theoretical explanation phenomenon show generative poisson topic model long documents dropout training improves generalization bound empirical risk minimization dropout achieves gain once classifier learns perform reasonably training examples corrupted dropout very set show similar conditions dropout preserves bayes decision boundary therefore induce minimal bias high dimensions 
rounding based moves metric labeling metric labeling special case energy minimization pairwise markov random fields energy function consists arbitrary potentials pairwise potentials given metric distance function label set popular methods solving metric labeling include making algorithms iteratively solve minimum cut problem ii linear programming relaxation based approach order convert solution relaxation integer solution several randomized rounding procedures developed literature consider large class parallel rounding procedures design making algorithms closely mimic prove multiplicative bound making algorithm exactly matches approximation factor corresponding rounding procedure any arbitrary distance function analysis includes known results making algorithms special cases 
parallel double greedy submodular maximization many machine learning problems reduced maximization submodular functions understood setting parallel maximization submodular functions remains open area research results only monotone functions optimal algorithm maximizing general class non monotone submodular functions introduced strongly double greedy logic program analysis work propose methods parallelize double greedy algorithm approach emphasizes speed cost weaker approximation guarantee second control approach guarantees tight approximation quantifiable cost additional reduced parallelism consequence explore trade space guaranteed performance objective optimality implement evaluate algorithms multi core hardware edge graphs demonstrating scalability tradeoffs approach 
multivariate regression propose method named calibrated multivariate regression fitting high dimensional multivariate regression models compared existing methods regularization regression task respect its noise level simultaneously tuning achieves improved finite sample performance computationally develop efficient smoothed proximal gradient algorithm worst case iteration complexity pre specified numerical accuracy theoretically prove achieves optimal rate convergence parameter estimation illustrate usefulness thorough numerical simulations show consistently outperforms high dimensional multivariate regression methods apply brain activity prediction problem competitive handcrafted model created human experts 
exact model selection inference marginal screening develop framework model selection inference marginal screening linear regression core framework result characterizes exact distribution linear functions response conditional model being selected selection framework allows construct valid confidence intervals hypothesis tests regression coefficients account selection procedure contrast work high dimensional statistics results exact non asymptotic require assumptions design matrix furthermore computational cost marginal regression constructing confidence intervals hypothesis testing negligible compared cost linear regression making methods particularly suitable extremely large datasets focus marginal screening illustrate applicability condition selection framework framework broadly applicable show apply proposed framework several selection procedures including orthogonal matching pursuit marginal screening lasso 
theory nonparametric pairwise similarity clustering connecting clustering classification pairwise clustering methods partition data space clusters pairwise similarity data points success pairwise clustering largely depends pairwise similarity function defined data points kernel similarity broadly used paper present novel pairwise clustering framework bridging gap clustering multi class classification pairwise clustering framework learns unsupervised nonparametric classifier data partition search optimal partition data minimizing generalization error learned classifiers associated data partitions consider nonparametric classifiers framework nearest neighbor classifier plug classifier modeling underlying data distribution nonparametric kernel density estimation generalization error bounds unsupervised nonparametric classifiers sum nonparametric pairwise similarity terms data points purpose clustering uniform distribution nonparametric similarity terms induced unsupervised classifiers exhibit known form kernel similarity prove generalization error bound unsupervised plug classifier asymptotically equal weighted cluster boundary low density separation widely used criteria semi supervised learning clustering based derived nonparametric pairwise similarity using plug classifier propose nonparametric based clustering method enhanced discriminative superiority experimental results 
just time learning fast flexible inference research machine learning centered search inference algorithms general purpose efficient problem extremely challenging general inference remains computationally expensive seek address problem observing specific applications model typically only need perform small subset possible inference computations motivated introduce just time learning framework fast flexible inference learns speed inference run time series experiments show framework allow combine flexibility sampling efficiency deterministic message passing 
fundamental limits online distributed algorithms statistical learning estimation many machine learning approaches characterized information constraints interact training data include memory sequential access constraints fast order methods solve stochastic optimization problems communication constraints distributed learning partial access underlying data missing features multi armed bandits however currently little understanding information constraints affect performance independent learning problem semantics example learning problems any algorithm small memory any bounded number bits example certain communication constraints perform worse possible paper describe single set results implies positive above several different settings 
quantized kernel learning feature matching matching local visual features crucial problem computer vision its accuracy greatly depends choice similarity measure generally very difficult design hand similarity kernel perfectly adapted data interest learning automatically assumptions possible however available techniques kernel learning suffer several limitations restrictive parametrization scalability paper introduce simple flexible family non linear kernels refer quantized kernels arbitrary kernels index space data piecewise constant similarities original feature space quantization allows features keep learning tractable result obtain state art matching performance standard benchmark dataset just bits represent feature dimension explicit non linear low dimensional feature mappings access euclidean geometry features 
parallel direction method multipliers consider problem minimizing block separable convex functions subject linear constraints alternating direction method multipliers admm block linear constraints studied theoretically empirically spite preliminary work effective generalizations admm multiple blocks unclear paper propose parallel randomized block coordinate method named parallel direction method multipliers solve optimization problems multi block linear constraints randomly updates blocks parallel parallel randomized block coordinate descent establish global convergence iteration complexity constant step size show randomized block coordinate descent overlapping blocks experimental results show performs better state methods applications robust principal component analysis overlapping group lasso 
label learning label objective learn supervised classifier labels only label observations known setting broad practical relevance particular privacy preserving data processing show mean operator statistic labels sufficient minimization many proper scoring losses linear classifiers using labels provide fast learning algorithm estimates mean operator manifold regularizer guaranteed approximation bounds present iterative learning algorithm uses initialization ground algorithm rademacher style generalization bounds fit setting introducing generalization rademacher complexity label proportion complexity measure algorithm optimizes tractable bounds corresponding bag empirical risk experiments provided domains size observations algorithms scalable tend consistently outperform state art moreover many cases algorithms just auc away oracle learns knowing labels largest domains roughly times total number labels 
stochastic multi armed bandit problem non stationary rewards multi armed bandit problem needs choose round play arms characterized unknown reward distribution reward realizations only observed arm selected objective maximize cumulative expected given horizon play needs acquire information arms exploration simultaneously optimizing rewards exploitation price due trade referred regret main question small price function horizon length problem studied extensively reward distributions change time assumption supports sharp regret violated practical settings paper focus formulation allows broad range temporal uncertainties rewards maintaining mathematical fully characterize regret complexity class problems establishing direct extent reward variation minimal achievable regret establishing connection adversarial stochastic frameworks 
object localization based structural svm using privileged information propose structured prediction algorithm object localization based support vector machines svms using privileged information privileged information provides useful high level knowledge image understanding facilitates learning reliable model small number training examples setting assume information available only training time since may difficult obtain visual data accurately human supervision goal improve performance incorporating privileged information ordinary learning framework adjusting model parameters better generalization tackle object localization problem based novel structural svm using privileged information alternating loss augmented inference procedure employed handle term objective function corresponding privileged information apply proposed algorithm caltech dataset obtain encouraging results investigation benefit privileged information structured prediction 
multi view deep model learning face identity view representations various factors identities views poses coupled face images identity view representations major challenge face recognition existing face recognition systems handcrafted features learn features improve recognition accuracy different behavior human brain intriguingly data human only recognize face identity face images person different viewpoints given single image making face perception brain robust view changes sense human brain learned encoded face models images account paper proposes novel deep neural named multi view identity view features infer full spectrum multi view images meanwhile given single face image identity features achieve superior performance dataset capable interpolate predict images viewpoints unobserved training data 
shape shading using generic viewpoint assumption generic viewpoint assumption states light scene special any estimated parameters observation stable small perturbations object viewpoint light positions analyzed previous works practical vision tasks paper show utilize estimate shape single shading image priors propose novel linearized spherical shading model enables obtain computationally efficient form term together data term build model shape model parameters estimated using alternating direction method multipliers embedded multi scale estimation framework prior framework obtain competitive shape estimation results variety models conditions requiring fewer assumptions competing methods 
parallel sampling using sub cluster splits develop sampling technique hierarchical dirichlet process models parallel algorithm builds fisher proposing large split merge moves based learned sub clusters additional global split merge moves drastically improve convergence experimental results furthermore discover cross validation techniques adequately determine convergence previous sampling methods converge slower previously expected 
map marginals variational inference bayesian submodular models submodular optimization many applications machine learning carry systematic investigation inference probabilistic models defined submodular functions generalizing regular pairwise mrfs determinantal point processes particular present field variational approach general log submodular log distributions based sub obtain lower upper bounds log partition function enables compute probability intervals marginals marginal likelihoods obtain fully factorized approximate posteriors computational cost ordinary submodular optimization framework results convex problems optimizing submodular functions show optimally solve provide theoretical guarantees approximation quality respect curvature function establish natural relations variational approach classical mean field method lastly empirically demonstrate accuracy inference scheme several submodular models 
robust logistic regression classification consider logistic regression arbitrary outliers covariate matrix propose robust logistic regression algorithm called estimates parameter simple linear programming procedure prove robust constant fraction adversarial outliers best knowledge result estimating logistic regression model covariate matrix corrupted any performance guarantees besides regression apply solving binary classification problems fraction training samples corrupted 
extracting uncertainty pairwise classification pairwise similarities work study problem pairwise classification pairwise pairwise similarities usually derived side information underlying class labels goal pairwise classification pairwise similarities infer pairwise class relationships refer pairwise labels examples given subset class relationships small set examples refer labeled examples propose very simple effective algorithm consists simple steps step complete sub matrix corresponding labeled examples second step reconstruct label matrix completed sub matrix provided similarity matrix analysis exhibits several mild recover label matrix small error top space corresponds largest eigenvalues similarity matrix covers column space label matrix subject low coherence number observed pairwise labels sufficiently demonstrate effectiveness proposed algorithm several experiments 
unified semantic embedding relating attributes propose method learns discriminative semantic space object embed auxiliary semantic entities attributes prior work only utilized side information explicitly embed semantic entities space embed categories enables represent category linear combination exploiting unified model semantics enforce category generated sparse combination attributes additional exclusive regularization learn discriminative composition proposed regularization discriminative learning process learn better generalizing model generates compact semantic description category enables humans analyze learned 
multiple environments limited experiments results paper addresses problem transferring causal knowledge collected several heterogeneous domains target domain only passive observations limited experimental data collected paper establishes necessary sufficient condition deciding feasibility causal effects target domain estimable information available proves previously established algorithm computing formula fact complete failure algorithm implies non existence formula finally paper shows complete class 
message passing problem graph partitioning cutting plane method constrained optimization procedure used continuous domain optimization techniques linear convex programs investigate viability similar idea message passing integral solutions context combinatorial problems problem propose factor graph based held formulation exponential number constraint factors exponential sparse form graph partitioning community mining using optimization introduce binary variable model large number constraints enforce cases able derive surprisingly simple message updates lead competitive solutions benchmark instances particular able near optimal solutions time empirically grows demonstrating augmentation practical efficient 
causal inference protection program fundamental problems causal inference estimation causal effect variables difficult study because direct evidence introduce novel approach estimating causal effects exploits conditional suggest paths unknown causal graph widely used faithfulness condition relaxed allow varying degrees imply conditional rule existence causal paths outcome posterior distribution bounds average causal effect linear programming approach bayesian inference approach used regular practice complement tools studies 
incremental clustering case extra clusters amount data available analysis transition batch incremental clustering methods process element time typically store only small subset data paper initiate formal analysis incremental clustering methods focusing types cluster structure able detect incremental setting strictly weaker batch model proving fundamental class cluster structures readily batch setting impossible identify using any incremental method furthermore show limitations incremental clustering overcome allowing additional clusters 
multi scale graphical models spatio temporal processes learning dependency structure spatially distributed observations spatio temporal process important problem many fields geology sciences however estimation systems complicated fact exhibit dynamics multiple scales space time arising due combination diffusion show time series graphical models based vector auto regressive processes capturing multi scale structure paper present hierarchical graphical model derived priors better represents multi scale dynamical systems propose algorithms estimate interaction structure data demonstrate results general class problems arising exploration discovering graphical structure meaningful provide evidence its advantages alternative approaches 
iterative neural distribution estimator training neural density estimator viewed doing step probabilistic inference missing values data propose model extends inference scheme multiple steps easier learn improve reconstruction steps learn reconstruct single inference step proposed model unsupervised building block deep learning combines desirable properties multi predictive training its likelihood computed analytically easy generate independent samples uses inference engine variational inference boltzmann machines proposed competitive state art density estimation datasets tested 
sparse pca covariance thresholding sparse principal component analysis given noisy observations low rank matrix dimension p$ seek reconstruct additional sparsity assumptions particular assume principal components q$ non zero entries respectively study high dimensional regime order influential paper sparse introduced simple algorithm estimates support principal vectors largest entries empirical covariance method shown succeed high probability fail high probability despite considerable amount work years practical algorithm exists provably better support recovery guarantees analyze covariance thresholding algorithm recently proposed confirm empirical evidence presented authors rigorously prove algorithm succeeds high probability order conditional lower bounds computational suggest impossible significantly better key technical component analysis bounds norm kernel random matrices regimes considered before 
low dimensional models neural population activity sensory cortical circuits neural responses visual cortex visual stimuli ongoing spiking activity local circuits important challenge computational neuroscience develop models account features large multi neuron recordings reveal stimulus representations interact depend cortical dynamics introduce statistical model neural population activity integrates nonlinear receptive field model latent dynamical model ongoing cortical activity model captures temporal dynamics effective network connectivity large population recordings correlations due shared stimulus drive common noise moreover because nonlinear stimulus inputs mixed ongoing dynamics model account relatively large number receptive field shapes small number nonlinear inputs low dimensional latent dynamical model introduce fast estimation method using online expectation maximization approximations inference scales linearly population size recording duration apply model multi channel recordings visual cortex show accounts large number individual neural receptive fields using small number nonlinear inputs low dimensional dynamical model 
persistent activity graded neural system persistent activity phenomenon cortical neurons keep firing stimulus initial neuronal responses persistent activity widely believed substrate neural system retaining memory trace stimulus information conventional view persistent activity attractor network dynamics faces challenge closed properly contrast view attractor consider stimulus information encoded state network very exhibits persistent firing duration propose simple effective mechanism achieve goal utilizes property short term plasticity neuronal forms short term short term opposite effects retaining neuronal responses properly combining neural system hold persistent activity graded persistent activity away naturally relying external drive implications results neural information representation 
representation theory ranking functions paper presents representation theory permutation valued functions general form called ranking functions pointwise ranking functions assign score object independently account objects consideration whereas loss functions evaluate set scores assigned objects many supervised learning rank tasks interest ranking functions particular bayes optimal ranking functions themselves especially loss function key using ranking functions lack appropriate representation theory functions show natural assumption call allows explicitly characterize set exchangeable ranking functions analysis draws theories tensor analysis functional analysis present experiments using novel method motivated representation theory 
near optimal sample compression nearest neighbors present sample compression algorithm nearest neighbors non trivial performance guarantees complement guarantees demonstrating matching hardness lower bounds show bound nearly optimal result yields insight margin based nearest neighbor classification metric spaces allows significantly existing bounds encouraging empirical results presented 
combinatorial pure exploration multi armed bandits study combinatorial pure exploration problem stochastic multi armed bandit setting learner set arms objective identifying optimal decision class collection subsets arms certain combinatorial structures size subsets matchings spanning trees paths problem represents rich class pure exploration tasks covers only many existing models novel cases object interest non trivial combinatorial structure paper provide series results general problem present general learning algorithms work decision classes admit offline maximization fixed confidence fixed budget settings prove problem dependent upper bounds algorithms analysis exploits combinatorial structures decision classes introduces analytic tool establish general problem dependent lower bound problem results show proposed algorithms achieve optimal sample complexity logarithmic factors many decision classes addition applying results back problems top arms identification multiple bandit best arms identification recover best available upper bounds constant factors partially resolve conjecture lower bounds 
log hilbert metric positive definite operators hilbert spaces paper introduces novel mathematical computational framework namely log hilbert metric positive definite operators hilbert space generalization log euclidean metric riemannian manifold positive definite matrices infinite dimensional setting general framework applied particular compute distances covariance operators reproducing kernel hilbert space rkhs obtain explicit formulas corresponding gram matrices empirically apply formulation task multi category image classification image represented infinite dimensional rkhs covariance operator several challenging datasets method significantly outperforms approaches based covariance matrices computed directly original input features including using log euclidean metric divergences achieving state art results 
consistency spectral partitioning uniform planted partition model spectral graph partitioning methods received significant attention practitioners computer science notable studies carried regarding behavior methods large sample size provide sufficient confidence practitioners effectiveness methods hand developments computer vision led applications model multi way affinity relations posed uniform hyper graphs paper view models random uniform establish consistency spectral algorithm general setting develop planted partition model stochastic problems using higher order tensors present spectral technique suited purpose study its large sample behavior analysis reveals algorithm consistent uniform larger values rate convergence improves increasing result provides theoretical evidence establishes importance way 
spectral clustering graphs bethe hessian spectral clustering standard approach label nodes graph studying largest lowest eigenvalues symmetric real matrix adjacency laplacian recently argued using complicated non symmetric higher dimensional operator related non walk graph leads improved performance detecting clusters optimal performance stochastic block model propose simpler object symmetric real matrix known bethe hessian operator laplacian show approach combines performances non operator detecting clusters way theoretical limit stochastic block model computational theoretical memory advantages real symmetric matrices 
fast robust squares estimation corrupted linear models subsampling methods recently proposed speed squares estimation large scale settings however algorithms typically robust outliers corruptions observed covariates concept influence developed regression diagnostics used detect corrupted observations shown paper property influence develop randomized approximation motivates proposed subsampling algorithm large scale corrupted linear regression limits influence data points since highly influential points contribute residual error general model corrupted observations show theoretically empirically variety simulated real datasets algorithm improves current state art approximation schemes ordinary squares 
local improved detection sophisticated data methods decision trees remain successful fast rigid object detection achieving top accuracy numerous datasets effective detectors decision trees orthogonal single feature splits topology resulting decision boundary may matched natural topology data given highly correlated data decision trees multiple feature splits effective splits however comes considerable computational expense inspired work discriminative features propose efficient feature transform correlations local neighborhoods result locally representation suited orthogonal decision trees fact orthogonal trees locally features outperform trees trained original features fraction computational cost overall improvement accuracy caltech dataset reduce false positives nearly previous state art 
robust kernel density estimation scaling projection hilbert space robust parameter estimation studied parametric density estimation little investigation robust density estimation nonparametric setting present robust version popular kernel density estimator estimators robust version useful since sample common issue datasets means nonparametric density estimate straightforward topic explore paper construct robust scale traditional project its nearest weighted norm because squared norm point wise errors weighted allocate weight high density regions demonstrate robustness numerical experiments consistency result shows asymptotically recovers density sufficient conditions 
based agnostic active learning study agnostic active learning goal learn classifier pre specified hypothesis class label queries possible making assumptions true function generating labels main algorithms problem based active learning high label margin based active learning only applies fairly restricted settings major challenge algorithm achieves better label complexity consistent agnostic setting applies general classification problems paper provide algorithm solution based novel contributions reduction consistent active learning confidence prediction guaranteed error novel confidence predictor 
bayes adaptive simulation based search value function approximation bayes adaptive planning offers principled solution exploration exploitation trade model uncertainty finds optimal policy belief space explicitly accounts expected effect future rewards uncertainty however bayes adaptive solution typically intractable domains large continuous state spaces present tractable method approximating bayes adaptive solution combining simulation based search novel value function approximation technique belief space method outperforms prior approaches discrete bandit tasks simple continuous navigation control tasks 
state space model decoding attentional competing environment humans able objects complex acoustic scene interplay bottom feature extraction top selective attention brain detailed mechanism underlying process largely unknown ability mimic procedure important problem artificial computational neuroscience consider problem decoding attentional state competing environment recordings human brain develop behaviorally inspired state space model account respect attentional state construct decoder based maximum posteriori map estimate state parameters expectation maximization em algorithm resulting decoder able track attentional multi second resolution using only envelopes speech streams covariates present simulation studies application real data human subjects results reveal proposed decoder provides substantial gains terms temporal resolution complexity decoding accuracy 
active regression propose active learning algorithm parametric linear regression random design provide finite sample convergence guarantees general distributions model active learner setting provably improve passive learning unlike learning settings classification regression passive learning rate general improved called rate convergence characterized distribution dependent risk improved many cases given distribution achieving optimal risk requires prior knowledge distribution following technique monte carlo function integration active learner approaches optimal risk using piecewise constant approximations 
sensory integration density estimation integration partially redundant information multiple sensors standard computational problem agents interacting world integration shown nearly optimal sense error minimization influential generalization notion optimality populations neurons retain information underlying common stimulus recently shown empirically neural network trained perform latent variable density estimation neurons observed data satisfies information preservation criterion model architecture designed match true generative process data prove analytical connection different tasks density estimation sensory integration implies model used does appear true models 
learning deep features scene recognition using places database scene recognition tasks computer vision allowing definition context object recognition whereas tremendous progress object recognition tasks due availability large datasets imagenet rise convolutional neural networks cnns learning high level features performance scene recognition level success may because current deep features trained imagenet competitive tasks introduce scene database called places labeled scenes propose methods compare density diversity image datasets show places dense scene datasets diversity using cnn learn deep features scene recognition tasks establish state art results several scene datasets cnn layers responses allows show differences internal representations object scene networks 
complete variational introduce novel probabilistic tracking algorithm incorporates combinatorial data association constraints model based track management using variational bayes bethe entropy approximation incorporate data association constraints ignored previous probabilistic tracking algorithms aspects method include model based mechanism replace heuristic logic typically used initiate destroy assignment posterior linear computation cost window length opposed exponential scaling previous map based approaches demonstrate applicability method tracking computer vision problems 
spike frequency adaptation tracking continuous attractor neural networks extract motion information brain needs time delays ubiquitous neural signal processing propose simple effective mechanism implement tracking neural systems proposed mechanism utilizes property spike frequency adaptation feature widely observed neuronal responses employ continuous attractor neural networks model describe tracking behaviors neural systems incorporating exhibits intrinsic ability hold self tracking moving stimulus interplay external drive intrinsic network determines tracking performance interestingly regime effectively regime intrinsic speed exceeds external drive depending amplitudes network achieve perfect tracking zero lag input perfect tracking constant leading time input model successfully experimentally observed tracking behaviors light understanding brain processes motion information manner 
efficient sampling learning sparse additive models high dimensions consider problem learning sparse additive models functions form d$ point queries unknown subset coordinate variables d$ assuming smooth propose set points sample efficient randomized algorithm recovers uniform approximation unknown provide rigorous theoretical analysis scheme sample complexity bounds algorithm utilizes results compressive sensing theory novel convex quadratic program recovering robust uniform approximations univariate functions point queries corrupted arbitrary bounded noise lastly theoretically analyze impact noise arbitrary bounded stochastic performance algorithm 
deep joint task learning generic object extraction paper investigates extract objects interest relying hand features sliding approaches aims jointly solve sub tasks rapidly localizing objects images ii accurately segmenting objects based present general joint task learning framework task object localization object segmentation multi layer convolutional neural network networks work boost performance particular propose incorporate latent variables bridging networks joint optimization manner network directly predicts positions scales objects raw images latent variables object feed second network produces object em type method studied joint optimization steps using networks estimates latent variables employing mcmc based sampling method ii optimizes parameters networks back propagation fixed latent variables extensive experiments demonstrate joint learning framework significantly outperforms state art approaches accuracy efficiency times faster competing approaches 
robust bayesian max margin clustering present max margin bayesian clustering general robust framework incorporates max margin criterion bayesian clustering models models demonstrate its flexibility effectiveness dealing different clustering tasks dirichlet process max margin gaussian mixture nonparametric bayesian clustering model underlying gaussian assumption dirichlet process gaussian mixtures incorporating max margin posterior constraints able infer number clusters data extend ideas present max margin clustering topic model learn latent topic representation document time cluster documents max margin fashion extensive experiments performed number real datasets results indicate superior clustering performance methods compared related baselines 
permutation diffusion maps application image association problem computer vision consistently matching images related problem finding clusters images critical components various tasks computer vision including structure motion sfm unfortunately large structures tend currently used matching algorithms leading characteristic final output paper introduce method permutations diffusion maps solve matching problem related affinity measure derived using ideas analysis symmetric group show just using preprocessing step existing sfm pipelines greatly improve reconstruction quality difficult datasets 
bounded regret finite armed structured bandits study type armed bandit problem expected return arm may depend returns arms present algorithm general class problems show certain possible achieve finite expected cumulative regret give problem dependent lower bounds cumulative regret showing special cases algorithm nearly optimal 
coresets segmentation streaming data life video streams financial time series twitter examples high dimensional signals practically unbounded time consider problem computing optimal segmentation signals piecewise linear function using only data maintaining signal enables fast analysis automatic summarization analysis signals core set compact representation data seen far approximates data specific task case segmentation stream show surprisingly segmentation problem admits coresets cardinality only linear number segments independently dimension signal its number points precisely construct representation size provides approximation sum squared distances any given piecewise linear function moreover coresets constructed parallel streaming approach results rely novel statistical problems computational geometry empirically evaluate algorithms very large synthetic real data sets gps video financial domains using machines amazon 
stream convolutional networks action recognition videos investigate architectures trained deep convolutional networks convnets action recognition video challenge capture complementary information frames motion frames aim best performing hand crafted features data driven learning framework contribution fold propose stream convnet architecture incorporates spatial temporal networks second demonstrate convnet trained multi frame dense optical flow able achieve very good performance spite limited training data finally show multi task learning applied different action classification datasets used increase amount training data improve performance architecture trained evaluated standard video actions benchmarks ucf competitive state art exceeds large margin previous attempts deep nets video classification 
discovering structure high dimensional data correlation explanation introduce method learn hierarchy abstract representations complex data based optimizing information theoretic objective intuitively optimization searches set latent factors best explain correlations data measured multivariate mutual information method unsupervised requires model assumptions scales linearly number variables makes attractive approach very high dimensional systems demonstrate correlation explanation automatically discovers meaningful structure data diverse sources including tests human language 
positive curvature hamiltonian monte carlo metric introduced mathematical physics used analyze hamiltonian monte carlo hmc setting step hmc corresponds riemannian manifold metric calculation curvature hmc manifold allows see positive cases sampling high dimensional multivariate gaussian show positive curvature used prove theoretical concentration results hmc markov chains 
learning mixed multinomial logit model ordinal data motivated generating personalized recommendations using ordinal preference data study question learning mixture multinomial logit model parameterized class distributions permutations partial ordinal preference data pair wise comparisons despite its long standing importance including social choice operations research revenue management little known question case single models mixture computationally statistically tractable learning pair wise comparisons feasible however learning mixture model infeasible general given state seek conditions feasible learn mixture model computationally statistically efficient manner present sufficient condition efficient algorithm learning mixed models partial preferences comparisons data particular mixture components objects learnt using samples size scales polynomially n$ model parameters sufficiently algorithm phases learn pair wise marginals component using tensor decomposition second learn model parameters component using introduced process proving results obtain generalization existing analysis tensor decomposition realistic regime only partial information sample available 
near optimal reinforcement learning mdps any reinforcement learning algorithm applies markov decision processes mdps suffer regret mdp elapsed time state action spaces implies time guarantee near optimal policy many settings practical interest due dimensionality learning time establish system known mdp possible achieve regret scales polynomially number parameters encoding mdp may exponentially smaller provide algorithms satisfy near optimal regret bounds context posterior sampling reinforcement learning psrl upper confidence bound algorithm 
efficient learning implicit exploration bandit problems side observations consider online learning problems partial model capturing situations information learner full information bandit feedback simplest variant assume addition its loss learner observe losses actions revealed losses depend learner action directed observation system chosen environment setting propose algorithm enjoys near optimal regret guarantees having know observation system before selecting its actions similar lines define partial information setting models online combinatorial optimization problems feedback received learner semi bandit full feedback predictions algorithm always computed efficiently setting propose another algorithm similar properties benefit always being computationally efficient price complicated tuning mechanism algorithms rely novel exploration strategy called implicit exploration shown efficient computationally information theoretically previously studied exploration strategies problem 
contextual auctions strategic buyers motivated real time analyze problem pricing posted price consider cases maximizing makes decisions every round may react algorithm short term order algorithm setting better future assume good function context vector describes good being give algorithm attaining sublinear regret contextual setting maximizing extend result second price auctions multiple buyers 
recursive inversion models permutations develop exponential family probabilistic model permutations capture hierarchical structure known mallows generalized mallows models describe parameter estimation propose approach structure search class models provide experimental evidence added flexibility improves predictive performance enables deeper understanding collections permutations 
convergence rate decomposable submodular function minimization submodular functions describe variety discrete problems machine learning signal processing computer vision however minimizing submodular functions poses number algorithmic challenges work introduced easy parallelizable algorithm minimizing submodular functions decompose sum simple submodular functions empirically algorithm performs extremely theoretical analysis given paper show algorithm converges linearly provide upper lower bounds rate convergence proof relies geometry submodular draws results spectral graph theory 
rules domain independent lifted map inference lifted inference algorithms probabilistic order logic frameworks markov logic networks mlns received significant attention years algorithms called rules identify symmetries order representation reduce inference problem large probabilistic model inference problem smaller model paper present rules enable fast map inference large class mlns rule uses concept single occurrence equivalence class logical variables define paper rule states map assignment mln recovered smaller mln logical variable single occurrence equivalence class replaced constant object domain variable second rule states remove subset formulas mln equivalence classes variables remaining mln single occurrence formulas subset evaluate true assignments identical truth value groundings prove rules sound demonstrate detailed experimental evaluation approach superior terms scalability map solution quality state art approaches 
pac bayesian auc classification scoring develop scoring classification procedure based pac bayesian approach auc area curve criterion focus class linear score functions derive pac bayesian non asymptotic bounds types prior score parameters gaussian prior spike slab prior makes possible perform feature selection important advantage approach amenable powerful bayesian computational tools derive particular sequential monte carlo algorithm efficient method may used standard expectation propagation algorithm faster approximate method extend method class non linear score functions essentially leading nonparametric procedure considering gaussian process prior 
optimization methods sparse pseudo likelihood graphical model selection sparse high dimensional graphical model selection popular topic contemporary machine learning various useful approaches proposed context penalized estimation gaussian framework many approaches scalable leveraged advances convex optimization depend gaussian functional form address gap convex pseudo likelihood based partial correlation graph estimation method recently proposed method uses cyclic coordinate wise minimization regression based pseudo likelihood shown robust model selection properties comparison gaussian approach direct contrast parallel work gaussian setting however convex pseudo likelihood framework leveraged extensive array methods proposed machine learning literature convex optimization paper address crucial gap proposing proximal gradient methods performing regularized inverse covariance matrix estimation pseudo likelihood framework present timing comparisons coordinate wise minimization demonstrate approach yields tremendous penalized partial correlation graph estimation gaussian setting yielding fastest scalable approach problems theoretical analysis approach rigorously demonstrate convergence derive rates 
prior distributions approximate inference structured variables present general framework constructing prior distributions structured variables prior defined information projection base distribution distributions supported constraint set interest cases projection intractable propose family parameterized approximations indexed subsets domain analyze special case sparse structure optimal prior intractable general show approximate inference using convex subsets tractable equivalent maximizing submodular function subject cardinality constraints result inference using greedy forward selection provably achieves factor optimal objective value work motivated predictive modeling high dimensional functional data task employ gaussian base distribution induced local partial correlations consider design priors capture domain knowledge sparse support experimental results simulated data high dimensional data show effectiveness approach terms support recovery predictive accuracy 
iterative hard thresholding methods high dimensional estimation estimators generalized linear regression models high dimensional settings requires risk minimization hard constraints known methods class projected gradient descent known iterative hard thresholding methods known offer fastest scalable solutions however current state art only able analyze methods extremely restrictive settings hold high dimensional statistical models work bridge gap providing analysis style methods high dimensional statistical setting bounds tight match known minimax lower bounds results rely general analysis framework enables analyze several popular hard thresholding style algorithms high dimensional regression setting finally extend analysis problem low rank matrix recovery 
online stochastic gradient methods non decomposable loss functions modern applications sensitive domains medicine frequently require non decomposable loss functions measure compared point loss functions hinge loss offer fine grained control prediction time present novel challenges terms algorithm design analysis work initiate study online learning techniques non decomposable loss functions aim enable incremental learning design scalable solvers batch problems propose online learning framework loss functions model enjoys several nice properties chief being existence efficient online learning algorithms sublinear regret online batch bounds model provable extension existing online learning models point loss functions popular losses model prove sublinear regret bounds proofs require novel structural ranked lists may independent interest develop scalable stochastic gradient descent solvers non decomposable loss functions show large family loss functions satisfying certain uniform convergence property includes measure methods provably converge empirical risk uniform convergence results known losses establish using novel proof techniques extensive real life benchmark datasets establish method orders magnitude faster recently proposed cutting plane method 
analysis learning positive unlabeled data learning classifier positive unlabeled data important class classification problems many practical applications paper show problem solved cost sensitive learning positive unlabeled data show convex surrogate loss functions hinge loss may lead classification boundary due intrinsic bias problem using non convex loss functions loss analyze excess risk class prior estimated data show classification accuracy sensitive class prior estimation unlabeled data dominated positive data naturally satisfied based outlier detection because dominant unlabeled dataset finally provide generalization error bounds show equal number labeled unlabeled samples generalization error learning only positive unlabeled samples worse times fully supervised case theoretical findings validated experiments 
dimensionality reduction subspace structure preservation modeling data being sampled union independent subspaces widely applied number real world applications however dimensionality reduction approaches theoretically preserve independence assumption studied key contribution show k$ projection vectors sufficient independence preservation any class data sampled union independent subspaces non trivial observation designing dimensionality reduction technique paper propose novel dimensionality reduction algorithm theoretically preserves structure given dataset support theoretical analysis empirical results synthetic real world data achieving state art results compared popular dimensionality reduction techniques 
constrained convex minimization model based gap introduce model based gap technique analyze order primal dual methods constrained convex minimization result construct order primal dual methods optimal convergence rates primal residual primal feasibility gap separately dual smoothing prox center selection strategy framework subsumes augmented lagrangian alternating direction dual fast gradient methods special cases rates apply 
poisson process unknown number rates application neural spike data introduce model rate poisson process modified chinese restaurant process applying mcmc sampler model allows posterior bayesian inference number states poisson data sampler shown accurate results synthetic data apply neuron spike data discrete firing rate states depending orientation stimulus 
probabilistic ode solvers means methods classic family solvers ordinary differential equations basis state art numerical methods return point estimates construct family probabilistic numerical methods return markov process defining probability distribution ode solution contrast prior work construct family posterior means match outputs family exactly proven good properties remaining degrees identified match chosen posterior probability measure fits observed structure ode results light structure solvers direction provide richer probabilistic output low computational cost raise research questions 
optimal decision making time varying evidence previous theoretical experimental work optimal decision making restricted artificial setting sensory evidence constant single trials work presented describes computation optimal decision making realistic case evidence varies time trial shows case optimal behavior determined bound decision maker belief depends only current past furthermore demonstrate simpler heuristics fail match optimal performance certain characteristics process determines time causing drop reward rate 
learning restricted distributions class fundamental sub family regular languages generated string set collection containing string u$ necessarily spite its simplicity problem learning given data known computationally intractable paper study pac present positive results learning problem element wise independent identical distributions distributions statistical query model constrained generalization learning product distributions provided empirical direction propose heuristic algorithm learning given labeled general distributions experiments demonstrate advantage efficiency accuracy algorithm 
discriminative unsupervised feature learning convolutional neural networks current methods training convolutional neural networks depend large amounts labeled samples supervised training paper present approach training convolutional neural network using only unlabeled data train network discriminate set surrogate classes surrogate class applying variety transformations randomly sampled image patch simple feature learning algorithm surprisingly successful applied visual object recognition feature representation learned algorithm achieves classification results matching outperforming current state art unsupervised learning several popular datasets stl cifar caltech 
distance based network recovery feature correlation present inference method gaussian graphical models only pairwise distances objects observed formally problem estimating covariance matrix mahalanobis distances object latent feature space solve problem fully bayesian fashion matrix normal likelihood matrix gamma prior resulting matrix posterior enables network recovery strongly correlated features generalize assumes euclidean distances feature independence spite greatly increased flexibility model statistical power entails computational cost argue extension highly relevant yields significantly better results synthetic real world experiments successfully demonstrated network biological cancer patients 
bandit convex optimization towards tight bounds bandit convex optimization fundamental framework decision making uncertainty generalizes many problems online statistical learning special case linear cost functions understood gap regret nonlinear losses remains important open question paper step towards understanding best regret bounds give efficient near optimal regret algorithm strongly convex smooth loss functions contrast previous works time invariant exploration schemes method employs exploration scheme time 
dictionary pair learning pattern classification discriminative dictionary learning widely studied various pattern classification problems existing methods aim learn synthesis dictionary represent input signal enforcing representation coefficients representation residual discriminative however norm sparsity constraint representation coefficients many methods makes training testing phases time consuming propose discriminative framework namely dictionary pair learning learns synthesis dictionary analysis dictionary jointly achieve goal signal representation compared conventional methods proposed method only greatly reduce time complexity training testing phases lead very competitive accuracies variety visual classification tasks 
provable submodular minimization using algorithm several applications large scale learning vision problems fast submodular function minimization sfm become critical problem theoretically unconstrained sfm performed polynomial time however algorithms practical proposed algorithm minimum euclidean norm point polytope showed algorithm used sfm general submodular functions minimum norm algorithm best empirical performance despite its good practical performance theoretically very little known minimum norm algorithm knowledge only result exponential time analysis due paper give convergence analysis algorithm prove iterations algorithm returns approximate solution min norm point prove robust version theorem shows approximate solution min norm point problem implies exact submodular minimization pseudo polynomial time guarantee minimum norm algorithm submodular function minimization particular show min norm point algorithm solves sfm time upper bound maximum change single element function value 
exploiting easy data online optimization consider problem online optimization learner chooses decision given decision set suffers loss associated decision state environment learner objective minimize its cumulative regret best fixed decision hindsight past decades numerous variants considered many algorithms designed achieve sub linear regret worst case however level robustness comes cost proposed algorithms conservative adapt complexity loss sequence far worst case paper introduce general algorithm provided safe learning algorithm benchmark effectively combine good worst case guarantees improved performance easy data derive general theoretical bounds regret proposed algorithm discuss its implementation wide range applications problem learning experts open problem finally provide numerical simulations setting prediction expert advice comparisons state art 
sparse multi task reinforcement learning multi task reinforcement learning objective simultaneously learn multiple tasks exploit similarity improve performance single task learning paper investigate case tasks accurately represented linear approximation space using small subset original large set features equivalent assuming weight vectors task value functions jointly sparse set non zero components small shared tasks building existing results multi task regression develop multi task extensions fitted iteration algorithm algorithm assumes tasks jointly sparse given representation second learns transformation features attempt finding sparse representation algorithms provide sample complexity analysis numerical simulations 
best arm identification linear bandits study best arm identification problem linear bandit rewards arms depend linearly unknown parameter *$ objective return arm largest reward characterize complexity problem introduce sample allocation strategies arms identify best arm fixed confidence minimizing sample budget particular show importance exploiting global linear structure improve estimate reward near optimal arms analyze proposed strategies compare empirical performance finally product analysis point connection optimality criterion used optimal experimental design 
mind gaussian process classification using privileged noise learning privileged information setting recently attracted attention machine learning community allows integration additional knowledge training process classifier comes form data modality available time show privileged information naturally treated noise latent function gaussian process classifier contrast standard setting latent function just feature becomes natural measure confidence training data likelihood function extensive experiments public datasets show proposed method using privileged noise called improves standard privileged knowledge current state art svm based method svm moreover show advanced neural networks deep learning methods compressed privileged information 
tight bounds influence diffusion networks application paper derive theoretical bounds long term influence node independent cascade model relate bounds spectral particular matrix show behavior sub critical spectral lower specifically point general networks sub critical regime size network upper bound met networks apply results arbitrary networks derive bound critical value connected component arises finally show empirically bounds large family networks 
computational efficiency training neural networks known neural networks computationally hard train hand practice modern day neural networks trained efficiently using sgd variety include different activation functions train networks larger needed regularization paper revisit computational complexity training neural networks modern perspective provide positive negative results yield provably efficient practical algorithms training neural networks 
self templates feature coding hierarchical feed forward networks successfully applied object recognition level hierarchy features extracted encoded followed pooling step processing common trend learn feature coding templates referred entries filters complete basis recently approach does templates shown obtain very promising results second order pooling paper analyze coding pooling scheme testing phase automatically adapts feature coding templates input features using templates learned during training phase finding able bring common concepts coding pooling schemes feature quantization allows significant accuracy improvements standard benchmarks image classification namely caltech voc 
multilabel structured output learning random spanning trees max margin markov networks show usual score function conditional markov networks written expectation scores spanning trees show small random sample output trees attain significant fraction margin obtained complete graph provide conditions perform tractable inference experimental results confirm practical learning scalable realistic datasets using approach 
stochastic network design trees investigate problem stochastic network design trees problem underlying phenomenon behavior disease starts multiple sources tree directions its edges actions taken increase probability propagation edges goal maximize total amount away sources main result rounded dynamic programming approach leads fully polynomial time approximation scheme algorithm optimal solutions any problem instance time polynomial input size algorithm outperforms competing approaches problem computational remove barriers networks restore 
learning convolution filters inverse covariance estimation neural network connectivity consider problem inferring direct neural network connections calcium imaging time series inverse covariance estimation proven fast accurate method learning scale network connectivity brain inverse covariance main component several top solutions including team algorithm however accuracy inverse covariance estimation highly sensitive signal preprocessing calcium time series furthermore brute force optimization methods grid search coordinate ascent signal processing parameters time intensive process learning may several parameters optimize network may generalize networks different size parameters paper show inverse covariance estimation dramatically improved using simple convolution filter prior applying sample covariance furthermore signal processing parameters learned quickly using supervised optimization algorithm particular maximize binomial log likelihood loss function respect convolution filter time series inverse covariance regularization parameter proposed algorithm relatively fast networks size neurons producing auc scores similar accuracy solution training time hours cpu prediction networks size carried minutes time takes read data solution 
spectral ranking using describe algorithm ranking set items given pairwise comparisons items intuitively algorithm assigns similar rankings items compare similarly others does constructing similarity matrix pairwise comparisons using methods matrix construct ranking show spectral algorithm recovers true ranking pairwise comparisons observed consistent total order show ranking reconstruction exact pairwise comparisons corrupted missing based spectral ranking robust noise scoring methods additional benefit formulation allows solve semi supervised ranking problems experiments synthetic real datasets demonstrate based spectral ranking achieves competitive cases superior performance compared classical ranking methods 
variables approximate inference recently proved using graph covers bethe partition function upper bounded true partition function binary pairwise model attractive provide simpler proof principles idea variable particular value attractive model show bethe partition functions sub model obtained any variable only raise hence improve approximation fact derive result may useful implications obtain model cycles bethe approximation exact yields result provide related lower bound broad class approximate partition functions general pairwise multi label models depends only topology demonstrate chosen variables practical value dramatically reducing approximation error 
predictive entropy search efficient global optimization black box functions propose novel information theoretic approach bayesian optimization called predictive entropy search iteration evaluation point maximizes expected information gained respect global maximum intractable function terms expected reduction differential entropy predictive distribution allows obtain approximations accurate efficient alternatives entropy search furthermore easily perform fully bayesian model hyperparameters evaluate synthetic real world applications including optimization problems machine learning finance show increased accuracy leads significant gains optimization performance 
block coordinate descent approach large scale sparse inverse covariance estimation sparse inverse covariance estimation problem arises many statistical applications machine learning signal processing problem inverse covariance matrix multivariate normal distribution estimated assuming sparse regularized log optimization problem typically solved approximate matrices because memory limitations existing algorithms unable handle large scale instances problem paper present block coordinate descent approach solving problem large scale data sets method treats matrix block block using quadratic approximations show approach advantages existing methods several aspects numerical experiments synthetic real gene expression data demonstrate approach outperforms existing state art methods especially large scale problems 
efficient inference continuous markov random fields polynomial potentials paper prove every multivariate polynomial degree decomposed sum convex concave polynomials motivated property exploit concave convex procedure perform inference continuous markov random fields polynomial potentials particular show concave convex decomposition polynomials expressed sum squares optimization efficiently solved semidefinite programming demonstrate effectiveness approach context reconstruction shape shading image denoising show approach significantly outperforms existing approaches terms efficiency quality retrieved solution 
scalable methods nonnegative matrix factorizations near separable matrices numerous algorithms used nonnegative matrix factorization assumption matrix nearly separable paper show algorithms scalable data matrices many rows columns called matrices key component improved methods orthogonal matrix transformation preserves separability nmf problem final methods need read data matrix only once suitable streaming multi core mapreduce architectures demonstrate efficacy algorithms sized matrices computing bioinformatics 
inferring synaptic spike trains biophysically inspired point process model popular approach neural describes neural responses terms cascade linear nonlinear stages linear filter describe stimulus integration followed nonlinear function convert filter output spike rate however real neurons respond stimuli manner depends nonlinear integration excitatory inhibitory synaptic inputs introduce biophysically inspired point process model explicitly incorporates stimulus induced changes synaptic conductance dynamical model neuronal potential work makes important contributions theoretical level offers novel interpretation popular generalized linear model glm neural spike trains show classic glm special case conductance based model stimulus linearly excitatory inhibitory equal opposite fashion model therefore viewed direct extension glm relax constraints resulting model exhibit inhibition time varying changes gain time constant second practical level show model provides tractable model spike responses early sensory neurons accurate interpretable glm importantly show accurately infer synaptic recorded spike trains validate estimates using direct measurements excitatory inhibitory cells show model fit spike trains predict excitatory inhibitory novel stimuli nearly accuracy model trained directly 
expectation backpropagation parameter training multilayer neural networks continuous discrete weights multilayer neural networks commonly trained using gradient descent based methods backpropagation bp inference probabilistic graphical models done using variational bayes methods expectation propagation show based approach used train deterministic specifically approximate posterior weights given data using factorized distribution online setting using online central limit theorem analytical approximation bayes update posterior resulting bayes estimates weights outputs despite different resulting algorithm expectation backpropagation ebp very similar bp form efficiency however several additional advantages training parameter given initial conditions prior architecture useful large scale problems parameter tuning major challenge weights restricted discrete values especially useful implementing trained precision limited hardware improving speed energy efficiency several orders magnitude ebp algorithm numerically binary classification tasks tasks ebp outperforms standard bp optimal constant learning rate previously reported state art interestingly ebp trained binary weights usually perform better continuous real weights average output using inferred posterior 
incremental local gaussian regression locally weighted regression created nonparametric method approximate wide range functions computationally efficient learn very large amounts incrementally collected data interesting feature non stationary functions beneficial property instance control problems however does provide proper generative model function values existing algorithms variety tuning parameters strongly influence bias variance learning speed results gaussian process regression hand does provide generative model black box automatic parameter tuning higher computational cost especially big data sets non stationary model required paper suggest path gaussian process regression locally weighted regression retain best approaches using localizing function basis approximate inference techniques build gaussian process regression algorithm increasingly local nature similar computational complexity empirical evaluations performed several synthetic real datasets increasing complexity big data scale demonstrate consistently achieve par superior performance compared current state art methods retaining principled approach fast incremental regression minimal tuning parameters 
general completion using bayesian nonparametric model heterogeneous databases broad variety applications exists lack tools estimating missing data databases paper provide efficient robust completion tool based bayesian nonparametric latent feature model particular propose general observation model indian buffet process adapted mixed continuous real valued positive real valued discrete ordinal count observations propose inference algorithm scales linearly number observations finally experiments real databases show proposed approach provides robust accurate estimates standard bayesian probabilistic matrix factorization gaussian observations 
universal models consider problem learning models options real time abstract planning setting reward functions specified any time expected returns efficiently computed introduce model independent any reward function called universal model prove construct traditional model given reward function conditional return computed directly single dot product reward function extend linear function approximation show gives solution returns value functions policies options provide stochastic approximation algorithm incrementally learning data prove its consistency demonstrate method domains domain document recommendation user query reward function document relevance expected return simulated random walk document second domain real time strategy game select best game unit dynamically specified tasks experiments show substantially efficient evaluating returns policies previously known methods 
approximating hierarchical sets hierarchical clustering goal hierarchical clustering construct cluster tree viewed modal structure density purpose convex optimization program efficiently estimate family hierarchical dense sets high dimensional distributions extend existing graph based methods approximate cluster tree distribution avoiding direct density estimation method able handle high dimensional data efficiently existing density based approaches present empirical results demonstrate superiority method existing ones 
constant strong convexity fast convergence proximal methods high dimensional settings state art statistical estimators high dimensional problems form regularized hence non smooth convex programs key estimation problems typically strongly convex high dimensional sampling regime hessian matrix becomes rank convexity however proximal optimization methods attain only sublinear rate paper investigate novel variant strong convexity call constant strong convexity require objective function strongly convex only constant subspace show condition naturally satisfied high dimensional statistical estimators analyze behavior proximal methods condition show global linear convergence proximal gradient local quadratic convergence proximal newton method regularization function statistical estimator decomposable corroborate theory numerical experiments show qualitative difference convergence rates proximal algorithms loss function does satisfy condition 
stochastic gradient descent weighted sampling randomized algorithm improve linear convergence sgd smooth strongly convex objectives reducing quadratic dependence strong convexity linear dependence furthermore show sampling distribution importance sampling necessary order improve convergence obtain linear dependence average smoothness previous results broadly importance sampling sgd improve convergence scenarios results based connection sgd randomized algorithm allows transfer ideas literature studying methods 
framework testing identifiability bayesian models perception bayesian models very effective describing human performance tasks recover hidden representations priors likelihoods loss functions data however intrinsic bayesian framework multiple combinations elements yield empirically results question model identifiability propose novel framework systematic testing identifiability significant class bayesian models practical applications improving experimental design examine theoretical identifiability inferred internal representations case studies show experimental designs work better remove underlying time interval estimation task second reconstructed representations speed perception task slow speed prior fairly robust 
planning markov decision processes using generative model consider problem online planning markov decision process discounted rewards any given initial state consider pac sample complexity problem computing probability optimal action using smallest possible number generative model provides reward state samples design algorithm called stochastic planning based face uncertainty principle used general setting requires only generative model enjoys complexity bound only depends local structure mdp 
gaussian process model prediction time changing variances important task modeling financial data standard models limited assume rigid functional relationships evolution variance moreover functional parameters usually learned maximum likelihood lead overfitting address problems introduce novel non parametric model time changing variances based gaussian processes model capture highly flexible functional relationships variances furthermore introduce online algorithm fast inference method faster current offline inference procedures avoids overfitting problems following fully bayesian approach experiments financial data show performs significantly better current standard alternatives 
safe screening rule sparse logistic regression regularized logistic regression sparse logistic regression widely used method simultaneous classification feature selection many efforts devoted its efficient implementation its application high dimensional data poses significant challenges paper present fast effective sparse logistic regression screening rule identify zero components solution vector may lead substantial reduction number features optimization appealing feature data set needs only once run screening its computational cost negligible compared solving sparse logistic regression problem moreover independent solvers sparse logistic regression integrated any existing solver improve efficiency evaluated using high dimensional data sets different applications extensive experimental results demonstrate outperforms existing state art screening rules efficiency solving sparse logistic regression improved magnitude general 
hardness parameter estimation graphical models consider problem learning canonical parameters specifying undirected graphical model markov random field mean parameters graphical models representing minimal exponential family canonical parameters determined mean parameters problem feasible principle goal paper investigate computational feasibility statistical task main result shows parameter estimation general intractable algorithm learn canonical parameters generic pair wise binary graphical model mean parameters time bounded polynomial number variables indeed result believed true see proof known proof gives polynomial time reduction approximating partition function hard core model known hard learning approximate parameters reduction entails showing marginal polytope boundary inherent property optimization procedure polytope does any knowledge its structure required ellipsoid method others 
learning neural network policies guided policy search unknown dynamics present policy search method uses iteratively local linear models optimize trajectory distributions large continuous problems trajectory distributions used framework guided policy search learn policies arbitrary method fits time varying linear dynamics models speed learning does rely learning global model difficult dynamics complex show hybrid approach requires many fewer samples model methods handle complex nonsmooth dynamics pose challenge model based techniques present experiments showing method used learn complex neural network policies successfully simulated tasks partially observed environments numerous 
magnitude sensitive preference understanding neural computations ability animals choose options advanced synthesis computational modeling brain imaging behavioral choice experiments remains theories preference learning accounts real economic choices humans face life choices usually amount item paper develop theory magnitude sensitive preference learning agent infer its preferences items compared options different show theory yields classical demand predicts choices large accurate phenomena utility functions suggest theory proposed realistic viable 
extreme bandits many areas medicine security life sciences want allocate limited resources different sources order detect extreme values paper study efficient way allocate resources sequentially limited feedback sequential design experiments studied bandit theory commonly optimized property regret respect maximum mean reward however problems network intrusion detection interested detecting extreme value output sources therefore work study extreme regret measures efficiency algorithm compared oracle policy selecting source propose algorithm provide its analysis evaluate empirically synthetic real world experiments 
distributed estimation information loss exponential families distributed learning probabilistic models multiple data minimum communication increasingly important study simple communication efficient learning framework calculates local maximum likelihood estimates mle based data subsets combines local mles achieve best possible approximation global mle based dataset jointly study statistical properties framework showing loss efficiency compared global setting relates underlying distribution families full exponential families drawing connection theory information loss fisher rao show full exponential family represents lower bound error rate arbitrary combinations local mles achieved kl divergence based combination method common linear combination method study empirical properties kl linear combination methods showing kl method significantly outperforms linear combination practical settings issues model non convexity heterogeneous data partitions 
non convex robust pca propose provable method robust pca task recover low rank matrix corrupted sparse perturbations method consists simple alternating projections set low rank sparse matrices intermediate steps prove correct recovery low rank sparse components tight recovery conditions match state art convex relaxation techniques method extremely simple implement low computational complexity n$ input matrix method running time rank low rank component accuracy contrast convex relaxation methods running time scalable large problem instances running time nearly matches usual pca non robust achieve worlds low computational complexity provable recovery robust pca analysis represents instances global convergence guarantees non convex methods 
simultaneous model selection optimization parameter stochastic learning stochastic gradient descent algorithms training linear kernel predictors importance thanks scalability various methods proposed speed convergence model selection phase ignored fact theoretical works time assumptions example prior knowledge norm optimal solution practical world validation methods remain only viable approach paper propose kernel based stochastic gradient descent algorithm performs model selection training parameters any form cross validation algorithm builds online learning theory unconstrained settings estimate time right regularization data dependent way optimal rates convergence proved standard smoothness assumptions target function preliminary empirical results 
learning weakly supervised data expectation loss svm svm algorithm many situations confidence binary label continuous value range bounded interval quantifies training data positive class propose novel learning algorithm called expectation loss svm svm devoted problems only binary label training sample available svm algorithm readily extended learn segment classifiers weak supervision exact value training example unobserved experiments show svm algorithm effectively address segment proposal classification task strong supervision pixel level annotations available weak supervision only bounding box annotations available outperforms alternative approaches besides validate method major tasks computer vision semantic segmentation object detection method achieves state art object detection performance pascal voc dataset 
message passing inference large scale graphical models high order potentials keep big data challenge parallelized algorithms based dual decomposition proposed perform inference markov random fields despite parallelization current algorithms energy high order terms graph connected paper propose partitioning strategy followed message passing algorithm able exploit pre computations only updates high order factors passing machines demonstrate effectiveness approach task joint semantic segmentation estimation single images show approach orders magnitude faster current methods 
encoding high dimensional local features sparse coding based fisher vectors deriving gradient vector generative model local features fisher vector coding identified effective coding method image classification implementations employ gaussian mixture model gmm characterize generation process local features choice shown sufficient traditional low dimensional local features sift typically good performance achieved only gaussian distributions however number gaussians insufficient model feature space higher dimensional local features become popular recently order improve modeling capacity high dimensional features turns inefficient computationally impractical simply increase number gaussians paper propose model local feature drawn gaussian distribution mean vector sampled subspace certain approximation model converted sparse coding procedure learning inference problems readily solved standard sparse coding methods gradient vector proposed model derive fisher vector encoding strategy termed sparse coding based fisher vector coding moreover adopt recently developed deep convolutional neural network cnn high dimensional local feature implement image classification proposed experimental evaluations demonstrate method only significantly outperforms traditional gmm based fisher vector encoding achieves state art performance generic object recognition scene fine grained image classification problems 
dependent nonparametric trees dynamic hierarchical clustering hierarchical clustering methods offer intuitive powerful way model wide variety data sets however assumption fixed hierarchy restrictive working data generated period time expect structure hierarchy parameters clusters time paper present distribution collections time dependent infinite dimensional trees used model evolving present efficient scalable algorithm performing approximate inference model demonstrate efficacy model inference algorithm synthetic data real world document corpora 
causal strategic inference performing major challenge economic policy making propose causal strategic inference framework apply large basic solution consists modeling learning parameters model real world data designing algorithms various computational problems question adopt nash equilibrium solution concept model special case model show equilibrium point always exists equilibrium interest rates unique general case give proof existence equilibrium point empirical study based data learn models show causal strategic inference policy evaluating outcomes various types loss making imposing interest rate 
learning multiple tasks parallel shared introduce multi task framework online learners sharing single limited bandwidth round learners receives input makes prediction label input shared stochastic mechanism inputs annotated learner receives feedback label may update its prediction rule proceed round develop online algorithm multi task binary classification learns setting bound its performance worst case setting additionally show algorithm used solve bandits problems contextual bandits dueling bandits context allowed exploration exploitation empirical study data prediction project document classification shows algorithm outperforms algorithms uses uniform allocation essentially makes accuracy 
reducing rank relational factorization models including observable patterns tensor factorizations become popular methods learning multi relational data context rank factorization important parameter determines runtime generalization ability determine conditions factorization efficient approach learning relational data derive upper lower bounds rank required recover adjacency tensors based findings propose novel additive tensor factorization model learning latent observable patterns multi relational data present scalable algorithm computing factorization experimentally show proposed approach does only improve predictive performance pure latent variable methods reduces required rank therefore runtime memory complexity significantly 
clustering labels time varying graphs present general framework graph clustering label observed pair nodes allows very rich encoding various types pairwise interactions nodes propose tractable approach problem based maximum likelihood estimator convex optimization analyze algorithm general generative model provide necessary sufficient conditions successful recovery underlying clusters theoretical results cover wide range existing graph clustering results including planted partition weighted clustering partially observed graphs furthermore result applicable novel settings including time varying graphs insights gained solving problems theoretical findings supported empirical results synthetic real data 
stochastic fast rates empirical risk minimization erm fundamental learning rule statistical learning problems data generated according unknown distribution returns hypothesis chosen fixed class small loss parametric setting depending erm slow fast rates convergence excess risk function sample size exist several results give sufficient conditions fast rates terms joint properties margin condition condition non statistical prediction expert advice setting analogous slow fast rate phenomenon characterized terms loss being role notion stochastic builds bridge models learning reducing classical special case present paper presents direct proof fast rates erm terms stochastic doing provides insight fast rates phenomenon proof exploits result solution general moment problem show partial suggests fast rates erm terms stochastic possible 
recovery coherent data low rank dictionary pursuit recently established method provides convenient way restore low rank matrices corrupted observations elegant theory powerful ultimate solution low rank matrix recovery problem indeed its performance may perfect data strictly low rank because ignores clustering structures data ubiquitous applications number cluster grows coherence data increasing recovery performance show challenges coherent data data high coherence low rank representation provided dictionary appropriately precisely mathematically prove dictionary low rank coherence parameter increases underlying cluster number provides elementary principle dealing coherent data naturally leads practical algorithm obtaining proper unsupervised environments experiments randomly generated matrices real motion sequences verify see full paper arxiv 
inferring sparse representations continuous signals continuous orthogonal matching pursuit many signals spike trains recorded multi channel recordings may represented sparse sum scaled timing amplitudes interest aggregate signal may seek estimate identities amplitudes translations signal present fast method recovering identities amplitudes translations method involves selecting component estimates amplitudes translations moving iteratively steps process analogous known orthogonal matching pursuit algorithm approach modeling translations continuous basis pursuit extend several ways selecting subspace optimally captures convex optimization problem greedy approach moving fourier domain precisely estimate time resulting method call continuous orthogonal matching pursuit simulated neural data shows gains speed accuracy 
analysis variational bayesian latent dirichlet allocation weaker sparsity map latent dirichlet allocation lda popular generative model various objects texts images object expressed mixture latent topics paper theoretically investigate variational bayesian vb learning lda specifically analytically derive leading term vb energy asymptotic setup show exist transition thresholds dirichlet hyperparameters sparsity inducing behavior drastically changes theoretically reveal notable phenomenon vb induce weaker sparsity map lda model opposed models experimentally demonstrate practical validity asymptotic theory real world music data 
discovering learning exploiting relevance paper consider problem learning online information consider making sequential decisions formalize contextual multi armed bandit problem high dimensional dimensional context vector learner needs select action maximize its expected reward time step dimension context vector called type assume exists unknown relation actions types called relevance relation reward action only depends contexts relevant types relation function reward action only depends context single type expected reward action lipschitz continuous context its relevant type propose algorithm achieves regret high probability algorithm achieves learning unknown relevance relation whereas prior contextual bandit algorithms exploit existence relevance relation regret algorithm exploiting does require reward observations guarantees high probability actions greater selected proposed method applied variety learning applications including recommender systems popularity prediction social networks network security instance time vast amounts different types information available decision maker effect action depends only single type 
divide conquer learning hull reduce broad class machine learning problems usually em sampling problem finding extremal spanning hull data point set lead global solution interpretable model outperform em sampling generalization error propose novel divide conquer learning scheme problem type sub problems different low random solved any solver sub problem present non iterative solver only needs compute array cosine values its max min entries provides faster methods check point hull improves algorithm design multiple dimensions brings significant speedup learning apply method gmm lda nmf subspace clustering show its competitive performance scalability methods rich datasets 
extended gaussian processes present methods inference gaussian process models general nonlinear likelihoods inference based variational framework gaussian posterior assumed likelihood linearized variational posterior mean using series expansion statistical show parameter updates obtained algorithms equivalent state update equations iterative extended kalman filters respectively hence refer algorithms extended gps treats likelihood black box requiring its inference applies non differentiable likelihood models evaluate performance algorithms number synthetic inversion problems binary classification dataset 
spectral methods meet em provably optimal algorithm crowdsourcing estimator widely used inferring true labels noisy labels provided non expert crowdsourcing workers however since estimator maximizes non convex log likelihood function hard theoretically its performance paper propose stage efficient algorithm multi class crowd labeling problems stage uses spectral method obtain initial estimate parameters second stage refines estimation optimizing objective function estimator em algorithm show algorithm achieves optimal convergence rate logarithmic factor conduct extensive experiments synthetic real datasets experimental results demonstrate proposed algorithm comparable accurate empirical approach outperforming several recently proposed methods 
exploiting linear structure convolutional networks efficient evaluation present techniques speeding time evaluation large convolutional networks designed object recognition tasks models deliver impressive accuracy image evaluation requires millions point operations making scale clusters computation dominated convolution operations lower layers model exploit redundancy present convolutional filters derive approximations significantly reduce required computation using large state art models demonstrate speedups convolutional layers cpu gpu factor keeping accuracy original model 
learning discover efficient mathematical identities paper explore machine learning techniques applied discovery efficient mathematical identities introduce attribute grammar framework representing symbolic given grammar operators build trees combine different ways looking compositions analytically equivalent target expression lower computational complexity however space trees grows exponentially complexity target expression brute force search impractical simplest consequently introduce novel learning approaches able learn simpler tree search simple gram model being recursive neural network show approaches enable derive complex identities reach brute force search human 
large margin mechanism differentially private maximization basic problem design privacy preserving algorithms private maximization problem goal item approximately maximizes data dependent function constraint differential privacy problem used sub many privacy preserving algorithms statistics machine learning previous algorithms problem range dependent utility size only apply very restricted function classes work provides general purpose range independent algorithm private maximization guarantees approximate differential privacy its applicability demonstrated fundamental tasks data mining machine learning 
distributed factorization tensors present technique significantly speeding alternating squares gradient descent widely used algorithms tensor factorization exploiting properties rao product show efficiently address computationally challenging sub step algorithms algorithm only requires matrix vector products easy parallelize only scalable average times faster competing algorithms variety datasets instance only takes seconds machines perform iteration algorithm seconds perform iteration algorithm dimensional tensor non zero entries 
data kernel means clustering application cancer many modern applications example bioinformatics computer vision samples multiple feature representations coming different data sources multiview learning algorithms exploit available information obtain better learner scenarios paper propose novel multiple kernel learning algorithm extends kernel means clustering multiview setting combines kernels views way better capture sample specific characteristics data demonstrate better performance data approach human cancer data set clustering patients method finds relevant groups global data methods evaluate results respect commonly used 
conditional regret conditional correlated equilibrium introduce natural extension notion regret conditional regret allows action modifications conditioned action history prove series results conditional regret minimization present algorithms minimizing conditional regret bounded conditioning history extend results case conditional considered only subset actions define notion equilibrium conditional correlated equilibrium connected notion conditional regret players follow conditional regret minimization strategies empirical distribution approaches equilibrium finally extend results multi armed bandit scenario 
mode estimation high dimensional discrete tree graphical models paper studies following problem given samples high dimensional discrete distribution want estimate leading modes underlying distributions point defined mode local optimum density neighborhood metric increase parameter neighborhood size increases total number modes monotonically decreases sequence modes reveal intrinsic information underlying distributions mode finding problem generally intractable high dimensions paper distribution approximated tree graphical model mode significantly easier efficient algorithm provable theoretical guarantees proposed applied applications data analysis multiple predictions 
large scale using mapreduce applied effective parameter estimation method various machine learning algorithms since increasing demand deal massive instances variables important scale parallelize effectively distributed system paper study problem algorithm large clusters tens thousands shared machines show naive implementation using map reduce requires significant amount memory large number map reduce steps negative performance impact second propose algorithm called vector avoids expensive dot product operations loop greatly improves computation efficiency great degree parallelism algorithm scales very enables variety machine learning algorithms handle massive number variables large datasets prove mathematical equivalence vector demonstrate its excellent performance scalability using real world machine learning problems billions variables production clusters 
submodular attribute selection action recognition video real world action recognition problems low level features adequately characterize rich spatial temporal structures action videos work encode actions based attributes describes actions high level concepts forward motion base analysis types action attributes type action attributes generated humans second type data driven attributes learned data using dictionary learning methods attribute based representation may exhibit high variance due noisy redundant attributes propose discriminative compact attribute based representation selecting subset discriminative attributes large attribute set attribute selection criteria proposed formulated submodular optimization problem greedy optimization algorithm presented guaranteed approximation optimum experimental results olympic sports ucf datasets demonstrate proposed attribute based representation significantly boost performance action recognition algorithms outperform recently proposed recognition approaches 
efficient structured matrix rank minimization study problem finding structured low rank matrices using nuclear norm regularization structure encoded linear map contrast known approaches linearly structured rank minimization full svd augmented lagrangian techniques solve linear systems iteration formulate problem amenable generalized conditional gradient method results practical improvement low iteration computational cost numerical results show approach significantly outperforms state art competitors terms running time effectively recovering low rank solutions stochastic system realization spectral compressed sensing problems 
integrated clustering outlier detection model joint clustering outlier detection problem using extension location formulation advantages combining clustering outlier selection include resulting clusters tend compact semantically coherent ii clusters robust data perturbations iii outliers clusters interpretable provide practical based algorithm problem study theoretical properties algorithm terms approximation convergence extensive evaluation synthetic real data sets quality scalability proposed method 
drifting games analysis online learning applications boosting provide general mechanism design online learning algorithms based minimax analysis drifting games framework different online learning settings hedge multi armed bandit problems online convex optimization studied converting various kinds drifting games original minimax analysis drifting games used generalized applying series relaxations starting choosing convex surrogate loss function different choices only recover existing algorithms propose algorithms parameter enjoy useful properties moreover drifting games framework naturally allows study high probability bounds resorting any concentration results generalized notion regret measures good algorithm compared top small fraction candidates finally hedge algorithm adaptive boosting algorithm computationally faster shown experiments since ignores large number examples round 
markov random field parameters fast mixing markov chain monte carlo mcmc algorithms simple extremely powerful techniques sample arbitrary distributions practice large unknown amount time converge stationary distribution paper gives sufficient conditions guarantee univariate gibbs sampling markov random fields mrfs fast mixing precise sense algorithm given project set fast mixing parameters euclidean norm following work give example project various divergence measures comparing univariate marginals obtained sampling projection common variational methods gibbs sampling original parameters 
automatic discovery cognitive improve prediction student learning physics acquire set cognitive domain experts determine select practice particular propose technique uses student performance data automatically discover needed technique assigns latent student expected accuracy sequence improves monotonically practice identified experts technique incorporates nonparametric prior assignments based expert provided weighted chinese restaurant process technique datasets different intelligent systems designed ranging age middle obtain surprising results datasets inferred technique support significantly improved predictions student performance expert provided second expert provided little value technique predicts student performance nearly ignores domain attempts leverage discuss explanations surprising results relationship discovery technique alternative approaches 
near optimal sample estimators spherical gaussian mixtures many important distributions high dimensional modeled gaussian mixtures derive sample efficient polynomial time estimator high dimensional spherical gaussian mixtures based intuitive spectral reasoning approximates mixtures spherical gaussians dimensions distance using samples computation time show any estimator requires samples hence algorithm sample complexity nearly optimal dimension implied time complexity factor exponential smaller previously known construct simple estimator dimensional gaussian mixtures uses samples computation time 
variational inference gaussian process models develop variational method approximate inference gaussian process models posteriors intractable using mixture gaussians variational distribution show variational objective its gradients approximated efficiently sampling univariate gaussian distributions ii gradients hyperparameters obtained analytically regardless model likelihood propose instances variational distribution covariance matrices parametrized linearly number observations results allow gradient based optimization done efficiently black box manner approach thoroughly verified models using benchmark datasets performing exact hard implementations running orders magnitude faster alternative mcmc sampling approaches method tool practitioners investigate models minimal effort deriving model specific inference algorithms 
learning mixtures submodular functions image collection summarization address problem image collection summarization learning mixtures submodular functions argue very natural problem show number previously used scoring functions submodular property explicitly provide classes submodular functions capturing necessary properties summaries namely coverage likelihood diversity learn mixtures submodular functions scoring functions formulate summarization supervised learning problem using large margin structured prediction furthermore introduce novel evaluation metric call automatic summary scoring similar metric called successfully applied document summarization metric known quality image collection summaries provide dataset consisting real world image collections many human generated ground truth summaries collected using mechanical turk extensively compare method previously explored methods problem show learning approach outperforms competitors dataset paper provides knowledge systematic approach problem image collection summarization dataset image collections human summaries 
robust tensor decomposition paper study statistical performance robust tensor decomposition observations noisy realization superposition low rank tensor *$ sparse tensor *$ unlike conventional noise bounded variance previous convex tensor decomposition analysis magnitude arbitrary large show certain conditions true low rank tensor sparse tensor recovered simultaneously theory yields norm estimation error bounds tensor separately show numerical experiments theory precisely predict scaling behavior practice 
provable tensor factorization missing data study problem low rank tensor factorization presence missing data following question many sampled entries need efficiently exactly reconstruct tensor low rank orthogonal propose novel alternating minimization based method iteratively refines estimates singular vectors show certain standard assumptions method recover mode n$ dimensional rank tensor exactly randomly sampled entries process proving result solve challenging sub problems tensors missing data analyzing initialization step prove generalization result spectrum random graphs prove global convergence alternating minimization good initialization simulations suggest dependence sample size dimensionality indeed tight 
parallel convex approximation nonsmooth nonconvex optimization consider problem minimizing sum smooth possibly non convex convex possibly nonsmooth function involving large number variables popular approach solve problem block coordinate descent method iteration only variable block remaining variables held fixed advances developments multi core parallel processing technology desirable parallelize method allowing multiple blocks simultaneously iteration algorithm work propose inexact parallel approach iteration subset variables parallel minimizing convex approximations original objective function investigate convergence parallel method randomized cyclic variable selection rules analyze asymptotic non asymptotic convergence behavior algorithm convex non convex objective functions numerical experiments suggest special case lasso minimization problem cyclic block selection rule outperform randomized rule 
using convolutional neural networks recognize recordings eeg recordings perception contain information distinguish different types identify themselves apply convolutional neural networks cnns analyze classify eeg data recorded perception study stimuli presented loop seconds participants investigate impact data representation pre processing steps classification tasks compare different network structures using cnns able recognize individual eeg mean classification accuracy chance level subjects looking seconds single channel aggregating predictions multiple mean accuracy achieved individual subjects 
tree graphical models combine ideas trees gaussian graphical models form nonparametric family graphical models approach arbitrary graphs collection nonparametric trees tree edges chosen variables violate joint non tree edges partitioned disjoint groups assigned tree nodes using nonparametric partial correlation statistic grown group using established methods based graphical lasso result factorization respect union tree defining high dimensional joint density efficiently estimated evaluated points theoretical properties experiments simulated real data demonstrate effectiveness trees 
model based reinforcement learning dimension consider problem learning optimize unknown markov decision process mdp show mdp parameterized known function class obtain regret bounds scale dimensionality cardinality system characterize dependence explicitly time elapsed k$ dimension dimension represent unified regret bounds model based reinforcement learning provide state art guarantees several important settings moreover present simple computationally efficient algorithm posterior sampling reinforcement learning psrl satisfies bounds 
minimax optimal inference partial rankings paper studies problem rank aggregation model goal infer global ranking related scores items based partial rankings provided multiple users multiple subsets items question particular interest optimally assign items users ranking many item assignments needed achieve target estimation error any assumptions items assigned users derive oracle lower bound rao lower bound estimation error prove upper bound estimation error achieved maximum likelihood estimator show upper bound rao lower bound depend spectral gap laplacian appropriately defined comparison graph since random comparison graphs known large spectral suggests random assignments control precisely matching oracle lower bound upper bound estimation error imply maximum likelihood estimator together random assignment minimax optimal logarithmic factor analyze popular rank breaking scheme decompose partial rankings pairwise comparisons show applies maximum likelihood estimator assumes independence pairwise comparisons dependent due rank breaking minimax optimal performance achieved logarithmic factor 
spectral methods indian buffet process inference indian buffet process versatile statistical tool modeling distributions binary matrices provide efficient spectral algorithm alternative variational bayes sampling based algorithms derive novel moments indian buffet process proper its applications give computationally efficient iterative inference algorithm concentration measure bounds reconstruction guarantees algorithm provides superior accuracy cheaper computation comparable variational bayesian approach number problems 
statistical consistency plug classifiers non decomposable performance measures study consistency properties algorithms non decomposable performance measures expressed sum losses individual data points measure used retrieval several performance measures used class settings work designing algorithms performance measures limited understanding theoretical properties algorithms recently showed consistency results algorithms optimize measure results apply only idealized setting precise knowledge underlying probability distribution form posterior class probability available learning algorithm work consider plug algorithms learn classifier applying empirically determined threshold suitable class probability provide general methodology show consistency methods any non decomposable measure expressed continuous function true positive rate true negative rate bayes optimal classifier class probability function thresholded derive consistency results plug algorithms measure geometric mean precision knowledge results measures addition continuous distributions show consistency plug algorithms any performance measure continuous monotonically increasing function experimental results confirm theoretical findings 
top rank optimization linear time bipartite ranking aims learn real valued ranking function orders positive instances before negative instances efforts bipartite ranking focused optimizing ranking accuracy top ranked existing approaches optimize task specific metrics extend rank loss error associated top ranked instances leading high computational cost super linear number training instances propose highly efficient approach optimizing accuracy top computational complexity linear number training instances present novel analysis bounds generalization error top ranked instances proposed approach empirical study shows proposed approach highly competitive state art approaches times faster 
spectral methods supervised topic models supervised topic models simultaneously model latent topic structure large collections documents response variable associated document existing inference methods based variational approximation monte carlo sampling paper presents novel spectral decomposition algorithm recover parameters supervised latent dirichlet allocation models spectral algorithm provably correct computationally efficient prove sample complexity bound subsequently derive sufficient condition identifiability thorough experiments diverse range synthetic real world datasets verify theory demonstrate practical effectiveness algorithm 
graphical models recovering probabilistic causal queries missing data address problem deciding causal probabilistic query estimable data corrupted missing entries given model process extend results presenting general conditions recovering probabilistic queries form causal queries form show causal queries may factors identifying specifically derive graphical conditions recovering causal effects form its mechanism separable finally apply results problems characterize recovery causal effects data corrupted 
sparse pca oracle property paper study estimation dimensional sparse principal subspace covariance matrix high dimensional setting aim recover oracle principal subspace solution principal subspace estimator obtained assuming true support known priori propose family estimators based semidefinite relaxation sparse pca novel regularizations particular weak assumption magnitude population projection matrix estimator family exactly recovers true support high probability exact rank attains statistical rate convergence being subspace sparsity level sample size compared existing support recovery results sparse pca approach does hinge spiked covariance model limited correlation condition complement estimator enjoys oracle property prove another estimator family achieves statistical rate convergence standard semidefinite relaxation sparse pca previous assumption magnitude projection matrix violated validate theoretical results numerical experiments synthetic datasets 
unsupervised music present probabilistic model music audio symbolic form model process discrete events give rise acoustic signals produce observed data result inference procedure model naturally resolves source separation problem introduced order adapt properties acoustic environment being learn recording specific spectral temporal envelopes unsupervised fashion system outperforms best approaches standard task achieving relative gain note onset real audio 
variational gaussian inference variational gaussian inference methods optimize lower bound marginal likelihood popular approach bayesian inference methods fast easy being reasonably accurate difficulty remains computation lower bound latent dimensionality large lower bound concave many models its computation requires optimization variational parameters efficient schemes reduce number parameters give inaccurate solutions destroy concavity leading slow convergence propose variational inference brings best worlds together maximizes lagrangian lower bound reducing number parameters number data examples obtained unique recovers lower bound bound concave second method maximizes lower bound using sequence convex problems data examples computes gradient efficiently overall approach avoids direct computations covariance only requiring its linear projections theoretically method converges rate existing methods case concave lower bounds remaining convergent reasonable rate non concave case 
estimation norm regularization analysis estimation error associated structured statistical recovery based norm regularized regression lasso needs consider aspects norm loss function design matrix noise vector paper presents generalizations estimation error analysis aspects compared existing literature characterize restricted error set establish relations error sets constrained regularized problems present estimation error bound applicable any norm precise bound presented variety noise vectors design matrices including sub gaussian dependent samples loss functions including squares generalized linear models gaussian widths measure size suitable sets associated tools play key role generalized analysis 
parameter estimation problems propose technique parameter learning problem bayesian networks independent learning problems technique applies incomplete datasets exploits variables hidden observed given dataset show empirically proposed technique lead orders magnitude savings learning time explain analytically empirically reported savings compare proposed technique related ones sometimes used inference algorithms 
stochastic proximal gradient descent techniques proximal gradient descent stochastic proximal gradient descent popular methods solving regularized risk minimization problems machine learning statistics paper propose analyze accelerated variant methods mini batch setting method incorporates techniques method variance reduction stochastic gradient accelerated proximal gradient descent proximal stochastic variance reduction gradient prox svrg trade relationship show method appropriate mini batch size achieves lower overall complexity prox svrg 
learning optimize information directed sampling propose information directed sampling algorithm online optimization problems decision maker balance exploration exploitation learning partial feedback action sampled manner minimizes ratio square expected single period regret measure information gain mutual information optimal action observation establish expected regret bound information directed sampling applies very general class models scales entropy optimal action distribution widely studied linear bandit models demonstrate simulation performance popular approaches including upper confidence bound algorithms sampling knowledge gradient present simple analytic examples illustrating information directed sampling dramatically outperform upper confidence bound algorithms sampling due way measures information gain 
covariance shrinkage data accurate estimation covariance matrices essential many signal processing machine learning algorithms high dimensional settings sample covariance known perform hence regularization strategies analytic shrinkage applied standard setting data assumed however practice time series typically exhibit strong structure introduces pronounced estimation bias work extended shrinkage framework data contribute work showing estimator being consistent high dimensional limit suffers high bias finite sample sizes propose alternative estimator unbiased sensitive choice yields superior performance simulations data real world data set eeg based brain computer experiment 
convnets learn convolutional neural nets convnets trained massive labeled datasets substantially improved state art image classification object detection however visual understanding requires establishing correspondence finer level object category given large pooling regions training image labels clear convnets derive success accurate correspondence model used precise localization paper study effectiveness convnet activation features tasks requiring correspondence present evidence convnet features finer scale receptive field sizes used perform conventional hand engineered features outperform conventional features prediction objects pascal voc 
bandit learning adaptive feedback study online learning setting player feedback time different action model adaptive feedback naturally scenarios environment player actions requires time recover stabilize algorithm actions motivates variant multi armed bandit problem call multi armed bandit feedback given algorithm whenever arms develop efficient online learning algorithms problem prove guarantee asymptotic regret optimal algorithms standard multi armed bandit problem result contrast another result states adding cost standard multi armed bandit makes substantially harder learn provides direct comparison feedback loss contribute difficulty online learning problem extend results general prediction framework bandit linear optimization attaining near optimal regret bounds 
convex optimization procedure clustering theoretical revisit paper present theoretical analysis ~a convex optimization procedure clustering using sum norms regularization recently proposed convex particular show samples drawn being cluster provably identify cluster membership provided distance larger threshold linearly depends size cube ratio numbers samples cluster best knowledge paper provide rigorous analysis understand why works may provide important insights develop novel convex optimization based algorithms clustering 
sparse bayesian structure learning relevance priors many problem settings parameter vectors sparse dependent way non zero coefficients tend cluster together refer form dependency classical sparse regression methods lasso automatic relevance model parameters independent priori therefore exploit dependencies introduce hierarchical model smooth region sparse weight vectors tensors linear regression setting approach represents hierarchical extension relevance framework gaussian process model dependencies prior variances regression weights combine structured model prior variances fourier coefficients high frequencies resulting prior weights region sparse different bases simultaneously develop efficient approximate inference methods show substantial improvements comparable methods group lasso smooth simulated real datasets brain imaging 
weakly supervised discovery visual pattern weakly labeled data gives rise growing demand object detection methods cope minimal supervision propose approach automatically identifies discriminative visual patterns characteristic given object class formulate problem constrained submodular optimization problem demonstrate benefits finding informative positive negative training examples together lead state art weakly supervised detection results challenging pascal voc dataset 
fast incremental gradient method support non strongly convex composite objectives work introduce fast incremental gradient method sdca svrg improves theory svrg better theoretical convergence rates support composite objectives proximal operator used unlike sdca supports non strongly convex problems directly adaptive any inherent strong convexity problem give experimental results showing effectiveness method 
exclusive feature learning arbitrary structures norm group lasso widely used enforce structural sparsity achieves sparsity inter group level paper propose formulation called group lasso brings sparsity intra group level context feature selection proposed exclusive group lasso applicable any feature structures regardless overlapping non overlapping structures give analysis properties exclusive group lasso propose effective iteratively weighted algorithm solve corresponding optimization problem rigorous convergence analysis show applications exclusive group lasso uncorrelated feature selection extensive experiments synthetic real world datasets indicate good performance proposed methods 
time data tradeoffs aggressive smoothing paper proposes tradeoff sample complexity computation time applies statistical estimators based convex optimization amount data increases smooth optimization problems achieve accurate estimates quickly work provides theoretical experimental evidence tradeoff class regularized linear inverse problems 
distributed power law graph computing theoretical empirical analysis big graphs variety real applications social networks machine learning based distributed graph frameworks attracted attention big data machine learning community frameworks graph strategy plays key role affect performance including balance communication cost typically degree distributions natural graphs real applications follow skewed power laws makes challenging task recently many methods proposed solve problem however existing methods achieve performance applications power law graphs paper propose novel vertex cut method called degree based hashing makes effective skewed degree distributions theoretically prove achieve lower communication cost existing methods simultaneously guarantee good balance furthermore empirical results several large power law graphs show outperform state art 
multi world approach question real world scenes based input propose method automatically questions images together advances natural language processing computer vision combine discrete reasoning predictions multi world approach represents uncertainty perceived world bayesian framework approach handle human questions high complexity realistic scenes range counts object classes instances lists system directly trained question pairs establish benchmark task seen modern attempt visual 
efficient partial prior information partial general model online learning limited feedback learner chooses actions sequential manner chooses outcomes every round learner suffers loss receives feedback based action outcome goal learner minimize cumulative loss applications range dynamic pricing label efficient prediction dueling bandits paper assume given prior information distribution based generates outcomes propose family efficient algorithms core track outcome distribution ellipsoid centered estimated distribution show algorithm provably enjoys near optimal regret rate locally observable partial problems stochastic demonstrated experiments synthetic real world data algorithm outperforms previous approaches very priors order magnitude smaller regret lower running time 
distributed parameter estimation probabilistic graphical models paper presents theoretical results distributed parameter estimation undirected probabilistic graphical models introduces general condition composite likelihood models guarantees global consistency distributed estimators provided local estimators consistent 
unsupervised deep graphs classification high dimensional data defined graphs particularly difficult graph geometry unknown introduce transform graphs computes invariant signal implemented deep cascade values iteratively compute orthogonal transforms multiscale neighborhoods unknown graphs estimated minimizing average total variation pair matching algorithm polynomial complexity supervised classification dimension reduction tested data bases images signals sampled unknown sphere 
online optimization max norm regularization max norm regularizer extensively studied decade promotes effective low rank estimation underlying data however max norm regularized problems typically formulated solved batch manner processing big data due possible memory bottleneck paper propose online algorithm solving max norm regularized problems scalable large problems particularly consider matrix decomposition problem example analysis applied problems matrix completion key technique algorithm max norm matrix factorization form consisting basis component coefficients way solve optimal basis coefficients alternatively prove basis produced algorithm converges stationary point asymptotically experiments demonstrate encouraging results effectiveness robustness algorithm see full paper arxiv 
probabilistic low rank matrix completion finite task reconstructing matrix given sample observed entries known matrix completion problem consideration arises wide variety problems including recommender systems collaborative filtering dimensionality reduction image processing physics multi class classification works focused recovering unknown real valued low rank matrix randomly sub sampling its entries investigate case observations finite numbers values corresponding examples ratings recommender systems labels multi class classification consider general sampling scheme non necessarily uniform matrix entries performance nuclear norm penalized estimator analyzed theoretically precisely derive bounds kullback leibler divergence true estimated distributions practice proposed efficient algorithm based lifted coordinate gradient descent order tackle potentially high dimensional settings 
articulated pose estimation graphical model image dependent pairwise relations present method estimating articulated human pose single static image based graphical model novel pairwise relations adaptive local image measurements precisely specify graphical model human pose exploits fact local image measurements used detect parts predict spatial relationships image dependent pairwise relations spatial relationships represented mixture model deep convolutional neural networks learn conditional probabilities presence parts spatial relationships image patches hence model combines flexibility graphical models efficiency statistical power method significantly outperforms state art methods datasets performs very dataset any training 
bayesian inference structured spike slab priors sparse signal recovery addresses problem solving linear inverse problems subject sparsity constraint propose novel prior formulation structured spike slab prior allows incorporate priori knowledge sparsity pattern imposing spatial gaussian process spike slab probabilities prior information structure sparsity pattern encoded using generic covariance functions furthermore provide bayesian inference scheme proposed model based expectation propagation framework using numerical experiments synthetic data demonstrate benefits model 
bayesian nonlinear support vector machines discriminative factor modeling bayesian formulation developed nonlinear support vector machines svms based gaussian process svm hinge loss expressed scaled mixture integrate bayesian svm factor model feature learning nonlinear classifier design performed jointly previous work discriminative feature learning assumed linear classifier inference performed expectation conditional maximization markov chain monte carlo mcmc extensive set experiments demonstrate utility using nonlinear bayesian svm discriminative feature learning factor modeling accuracy 
generalized higher order orthogonal iteration tensor decomposition completion low rank tensor estimation frequently applied many real world problems despite successful applications existing norm minimization methods may become very slow applicable large scale problems address difficulty therefore propose efficient scalable core tensor norm minimization method simultaneous tensor decomposition completion lower computational complexity induce equivalence relation norm low rank tensor its core tensor norm core tensor used replace tensor leads smaller scale matrix problem finally efficient algorithm rank increasing scheme developed solve proposed problem convergence guarantee extensive experimental results show method usually accurate state art methods orders magnitude faster 
making pairwise binary graphical models attractive computing partition function constant given pairwise binary graphical model hard general result partition function typically estimated approximate inference algorithms belief propagation bp tree reweighted belief propagation provides reasonable estimates practice convergence issues better convergence properties typically provides estimates work propose novel scheme better convergence properties bp provably provides better partition function estimates many instances particular given arbitrary pairwise binary graphical model construct specific cover explore properties special cover show used construct algorithm desired properties 
low rank approximation lower bounds row update streams study low rank approximation streaming model rows d$ matrix presented time arbitrary order stream streaming algorithm output d$ matrix k$ best rank approximation deterministic streaming algorithm improved analysis provides streaming algorithm using words space natural question smaller space possible give matching lower bound bits space randomized algorithms succeed only constant probability lower bound matches upper bound word size improving simple space lower bound 
deep convolutional neural network image deconvolution many fundamental image related problems involve deconvolution operators real blur degradation deal linear convolution model due camera noise saturation image compression perfectly modeling outliers challenging generative model perspective develop deep convolutional neural network capture characteristics degradation note directly applying existing deep neural networks does produce reasonable results solution establish connection traditional optimization based schemes neural network architecture novel separable structure introduced reliable support robust deconvolution network contains trained supervised manner proper initialization yield performance non blind image deconvolution compared previous generative model based methods 
joint training convolutional network graphical model human pose estimation paper proposes hybrid architecture consists deep convolutional network markov random field show architecture successfully applied challenging problem articulated human pose estimation monocular images architecture exploit structural domain constraints geometric relationships joint locations show joint training model improves performance allows significantly outperform existing state art techniques 
learning generative models visual attention attention long proposed important efficiently dealing massive amounts sensory stimulus inspired attention models visual neuroscience need object centered data generative models propose deep learning based generative framework using attention attentional mechanism signals region interest scene aligned canonical representation generative modeling scene background generative model its resources object interest convolutional neural employed provide good initializations during posterior inference uses hamiltonian monte carlo learning images faces model robustly face region novel subjects importantly model learn generative models faces novel dataset large images face locations known 
metric learning temporal sequence alignment paper propose learn mahalanobis distance perform alignment multivariate time series learning examples task time series true alignment known cast alignment problem structured prediction task propose realistic losses alignments optimization tractable provide experiments real data audio audio context show learning similarity measure leads improvements performance alignment task propose metric learning framework perform feature selection basic audio features build combination better alignment performance 
learning optimal overcome game theoretic algorithms security impressive real world impact algorithms compute optimal strategy commit game strategy best order build game model potential various outcomes estimated inaccurate estimates lead significant design algorithm optimizes strategy prior information observing responses randomized resources learning contrast previous work algorithm requires number queries polynomial representation game 
hard distribution norm reinforcement learning rl state art algorithms require large number samples state action pair estimate transition kernel many problems good approximation needed instance state action pair only states value learning accurately only its support paper aims capturing behavior defining novel hardness measure markov decision processes mdps call distribution norm distribution norm ~a measure defined zero mean functions standard variation respect provide concentration inequality dual distribution norm allows replace generic loose concentration inequalities used previous analysis rl algorithms benefit hardness measure show several common rl benchmarks low hardness measured using norm distribution norm captures finer properties number states used difficulty mdps 
near optimal density estimation near linear time using variable width unknown arbitrary probability distribution consider problem density estimation learning algorithm given draws high probability output hypothesis distribution close main contribution paper highly efficient density estimation algorithm learning using variable width hypothesis distribution piecewise constant probability density function detail any give algorithm makes draws runs time outputs hypothesis distribution piecewise constant pieces high probability hypothesis satisfies total variation distance statistical distance universal constant smallest total variation distance any piecewise constant distribution sample size running time algorithm optimal logarithmic factors factor present result inherent problem prove algorithm sample size bounded terms achieve regardless hypothesis distribution uses 
autoencoder approach learning word representations cross language learning allows training data language build models different language many approaches learning require word level alignment sentences parallel corpora work explore autoencoder based methods cross language learning word representations aligned languages relying word level alignments show simply learning reconstruct bag words representations aligned sentences languages fact learn high quality representations word alignments empirically investigate success approach problem cross language classification classifier trained given language english learn generalize different language experiments language pairs show approach achieves state art performance outperforming method exploiting word alignments strong machine translation baseline 
sequential monte carlo graphical models propose framework sequential monte carlo algorithms inference probabilistic graphical models sequential decomposition sequence auxiliary distributions defined monotonically increasing sequence probability spaces auxiliary distributions using able approximate full joint distribution defined key merits sampler provides unbiased estimate partition function model show used particle markov chain monte carlo framework order construct high dimensional block sampling algorithms general 
optimal regret minimization posted price auctions strategic buyers study revenue optimization learning algorithms posted price auctions strategic buyers analyze very broad family monotone regret minimization algorithms problem includes previous best known algorithm show algorithm family admits strategic regret favorable introduce algorithm achieves strategic regret differing lower bound only factor exponential improvement previous best algorithm algorithm admits natural analysis simpler proofs ideas its design general report results empirical evaluations comparing algorithm previous best algorithm show consistent exponential improvement several different scenarios 
optimal prior dependent neural population codes shared input noise brain uses population codes form distributed noise tolerant representations sensory motor variables work examined theoretical optimality codes order gain insight principles population codes brain however majority population coding literature conditionally independent neurons neurons noise governed stimulus independent covariance matrix analyze population coding simple alternative model latent input noise stimulus before encoded population provides convenient tractable description uncertainty overcome adding neurons stimulus dependent correlations mimic certain aspects correlations observed real populations examine prior dependent bayesian optimal coding populations using exact analyses cases posterior approximately gaussian analyses extend previous results independent poisson population codes yield analytic expression squared loss tight upper bound mutual information show homogeneous populations input domain optimal tuning curve width depends prior loss function resource constraint amount input noise framework provides practical issues optimality noise correlation coding fidelity realistic neural populations 
deep embeddings image sentence mapping introduce model retrieval images sentences deep multi modal embedding visual natural language data unlike previous models directly map images sentences common embedding space model works finer level images objects sentences dependency tree relations common space introduce structured max margin objective allows model explicitly modalities extensive experimental evaluation shows reasoning global level images sentences finer level respective improves performance image sentence retrieval tasks additionally model provides interpretable predictions image sentence retrieval task since inferred inter modal alignment explicit 
flexible transfer learning support model transfer learning algorithms used sufficient training data supervised learning task source training domain only very limited training data second task target domain similar identical previous work transfer learning focused relatively restricted settings specific parts model considered carried tasks work covariate matching marginal distributions observations domains similarly work target conditional matching marginal distributions labels adjusting conditional distributions matched domains however covariate assumes support support training training set richer set target conditional makes similar assumption moreover work transfer learning considered case labels domain available little work done marginal conditional distributions allowed change changes smooth paper consider general case support model change domains transform location scale achieve transfer tasks since allow flexible transformations proposed method yields better results synthetic data real world data 
probabilistic differential dynamic programming present data driven probabilistic trajectory optimization framework systems unknown dynamics called probabilistic differential dynamic programming takes account uncertainty explicitly dynamics models using gaussian processes gps based second order local approximation value function performs dynamic programming trajectory gaussian belief spaces different typical gradient based policy search methods does require policy learns locally optimal time varying control policy demonstrate effectiveness efficiency proposed algorithm using tasks compared classical state art based policy search method offers superior combination data efficiency learning speed applicability 
predicting useful neighborhoods local learning local learning methods train classifier fly time using only subset training instances relevant novel example goal classifier properties data example existing methods assume instances useful building local model strictly example however fails account fact success resulting classifier depends full distribution selected training instances simply example nearest neighbors propose predict subset training data jointly relevant training its local model develop approach discover patterns queries good neighborhoods using large scale multi label classification compressed sensing given novel point estimate composition size training subset yield accurate local model demonstrate approach image classification tasks sun show outperforms traditional global local approaches 
modeling deep temporal dependencies recurrent grammar cells propose modeling time series representing transformations frame time frame time show linear model transformations autoencoder recurrent network training predict future frames current inferred transformation using time show stacking multiple layers units recurrent makes possible represent complicated time series outperform standard recurrent neural networks terms prediction accuracy variety tasks 
generalized application support norm propose generalized linear models any norm encoding parameter structure leveraged estimation investigate computational statistical aspects based proximal operator flexible inexact admm framework designed solving non asymptotic high probability bounds established estimation error rely gaussian widths unit norm ball error set consider non trivial example using support norm derive efficient method compute proximal operator support norm since existing methods setting statistical analysis provide upper bounds gaussian widths needed analysis yielding statistical recovery guarantee estimation support norm experimental results confirm theoretical analysis 
neurons monte carlo samplers bayesian learning spiking networks propose layer spiking network capable performing approximate inference learning hidden markov model lower layer sensory neurons detect noisy measurements hidden world states higher layer neurons recurrent connections infer posterior distribution world states spike trains generated sensory neurons show neuronal network synaptic plasticity implement form bayesian inference similar monte carlo methods particle filtering spike population inference neurons represents sample particular hidden world state spiking activity neural population approximates posterior distribution hidden state model provides functional explanation poisson noise commonly observed cortical responses uncertainties spike times provide necessary variability sampling during inference unlike previous models hidden world state observed sensory neurons temporal dynamics hidden state unknown demonstrate network sequentially learn hidden markov model using spike timing dependent learning rule achieve power law convergence rates 
bayesian case model generative approach case based reasoning classification present bayesian case model bcm general framework bayesian case based reasoning classification clustering bcm brings intuitive power bayesian generative framework bcm learns observations best represent clusters dataset performing joint inference cluster labels important features simultaneously bcm sparsity learning subspaces sets features play important subspace representation provides quantitative benefits preserving classification accuracy human subject experiments verify statistically significant improvements participants understanding using explanations produced bcm compared given prior art 
latent support measure machines bag words data classification many classification problems input represented set features bag words representation documents support vector machines svms widely used tools classification problems performance svms generally determined kernel values data points defined properly however svms representations major occurrence different semantically similar words reflected kernel calculation overcome propose kernel based discriminative classifier data call latent support measure machine latent latent latent vector associated vocabulary term document represented distribution latent vectors words document represent distributions efficiently kernel embeddings distributions hold high order moment information distributions latent finds separating maximizes distributions different classes estimating latent vectors words improve classification performance experiments show latent achieves state art accuracy classification robust respect its hyper parameters useful words 
local linear convergence forward backward partial smoothness paper consider forward backward proximal splitting algorithm minimize sum proper closed convex functions having lipschitz continuous gradient being smooth relatively active manifold propose generic framework show forward backward correctly identifies active manifold finite number iterations ii local linear convergence regime characterize precisely gives unified explanation typical observed numerically many problems framework including lasso group lasso lasso nuclear norm regularization results may numerous applications including signal image processing processing sparse recovery machine learning 
benefits robustness approximating aggregated mdps reinforcement learning describe robust markov decision processes value function approximation state aggregation robustness serves reduce sensitivity approximation error sub optimal policies comparison classical methods fitted value iteration results reducing bounds gamma discounted infinite horizon performance loss factor gamma preserving polynomial time computational complexity experimental results show using robust representation significantly improve solution quality minimal additional computational cost 
deep learning face representation joint identification key challenge face recognition develop effective feature representations reducing intra personal variations inter personal differences paper show solved deep learning using face identification signals supervision deep identification features learned carefully designed deep convolutional networks face identification task increases inter personal variations drawing features extracted different identities apart face task reduces intra personal variations features extracted identity together essential face recognition learned features generalized identities unseen training data challenging dataset face accuracy achieved compared best previous deep learning result error rate significantly reduced 
provable svd based algorithm learning topics dominant admixture corpus topic models latent dirichlet allocation lda documents drawn distributions words known topics inference problem recovering topics collection documents drawn hard making strong assumption called separability provable algorithm inference widely used lda model provable algorithm using tensor methods learn topic vectors bounded error natural measure probability vectors aim develop model makes intuitive empirically supported assumptions design algorithm natural simple components svd provably solves inference problem model bounded error topic lda models essentially characterized group words motivated introduce topic specific group words occur strictly greater frequency topic any topic individually required high frequency together individually major contribution paper show realistic assumption empirically verified real corpora singular value decomposition svd based algorithm crucial pre processing step thresholding provably recover topics collection documents drawn dominant dominant convex combination distributions distribution significantly higher contribution others apart simplicity algorithm sample complexity near optimal dependence lowest probability topic dominant better empirical evidence shows several real world corpora dominant admixture assumptions hold proposed algorithm substantially outperforms state art 
quadratic approximation approach statistical models paper develop family algorithms optimizing superposition statistical estimators high dimensional problems involving minimization sum smooth loss function hybrid regularization current approaches order methods including proximal gradient alternating direction method multipliers admm propose family second order methods approximate loss function using quadratic approximation superposition structured regularizer leads subproblem efficiently solved alternating minimization propose general active subspace selection approach speed solver low dimensional structure given regularizers provide convergence guarantees algorithm empirically show approach times faster state art order approaches latent variable graphical model selection problems multi task learning problems regularizer problems approach appears algorithm extend active subspace ideas multiple regularizers 
general stochastic networks classification extend generative stochastic networks supervised learning representations particular introduce hybrid training objective considering generative discriminative cost function governed trade parameter lambda variant network training involving noise training jointly optimize multiple network layers additional regularization constraints norms dropout variants pooling convolutional layers added able obtain state art performance mnist dataset using permutation invariant digits outperform baseline models sub variants mnist dataset significantly 
spatio temporal representations uncertainty spiking neural networks long argued because inherent ambiguity noise brain needs represent uncertainty form probability distributions neural encoding distributions remains however highly present novel circuit model representing real valued distributions using spike based spatio temporal code model combines computational advantages currently competing models probabilistic codes exhibits realistic neural responses variety classic measures furthermore model challenges associated interpreting neural activity relation behavioral uncertainty points alternative population level approaches experimental validation distributed representations 
attentional neural network feature selection using cognitive feedback attentional neural network framework integrates top cognitive bias bottom feature extraction coherent architecture top influence especially effective dealing high noise difficult segmentation problems system extensible easy train cheap run complex behaviors obtain classification accuracy better competitive state art results mnist variation dataset successfully digits high success rates view general purpose framework essential foundation larger system cognitive brain 
convolutional neural network architectures matching natural language sentences semantic matching central importance many natural language tasks semantic successful matching algorithm needs adequately model internal structures language objects interaction step goal propose convolutional neural network models matching sentences convolutional strategy vision speech proposed models only represent hierarchical structures sentences layer layer composition pooling capture rich matching patterns different levels models generic requiring prior knowledge language hence applied matching tasks different nature different languages empirical study variety matching tasks demonstrates efficacy proposed model variety matching tasks its superiority competitor models 
scalable non linear learning adaptive polynomial effectively learn nonlinear representation time comparable linear describe algorithm explicitly adaptively expands higher order interaction features base linear representations algorithm designed extreme computational efficiency extensive experimental study shows its computation prediction tradeoff ability compares very favorably strong baselines 
relations lfps neural spike trains neuroscience identify neural networks important behaviors environments work proposes strategy identifying neural networks characterized time frequency dependent connectivity patterns using convolutional dictionary learning spike train data local field potentials lfps multiple areas brain analytical contributions modeling dynamic relationships lfps spikes ii describing relationships spikes lfps analyzing ability predict lfp data region based spiking information brain iii development clustering methodology allows inference similarities neurons multiple regions results based data sets spike lfp data recorded simultaneously brain regions 
diverse sequential subset selection supervised video summarization video summarization challenging problem great application potential whereas prior approaches largely unsupervised nature focus sampling useful frames summaries consider video summarization supervised subset selection problem idea system learn human created summaries select informative diverse subsets best meet evaluation metrics derived human perceived quality propose sequential determinantal point process probabilistic model diverse sequential subset selection novel inherent sequential structures video data deficiency standard dpp treats video frames randomly items meanwhile power modeling diverse subsets essential summarization extensive results videos datasets demonstrate superior performance method compared only existing unsupervised methods naive applications standard dpp model 
self learning diversity self learning recently proposed learning regime inspired learning process humans animals incorporates easy complex samples training existing methods limited important aspect learning diversity incorporate information propose approach called self learning diversity preference easy diverse samples general regularizer regularization term independent learning objective easily generalized various learning tasks non convex optimization variables regularization term sample selection globally solved time demonstrate method significantly outperforms conventional real world datasets specifically achieves best map far reported literature hollywood olympic sports datasets 
feature cross adversarial classification success machine learning particularly supervised settings led numerous attempts apply adversarial settings detection core challenge class applications adversaries static data effort classifiers deployed detect investigate problem modeling objectives adversaries algorithmic problem rational objective driven adversaries particular demonstrate shortcomings feature reduction adversarial settings using several natural adversarial objective functions observation particularly pronounced able similar features example replace words replace words offer simple heuristic method making learning robust feature cross present general approach based mixed integer linear programming constraint generation implicitly trades overfitting feature selection adversarial setting using sparse regularizer model approach method combining adversarial classification algorithm very general class models adversarial classifier show algorithmic approach significantly outperforms state art alternatives 
deep recursive neural networks language recursive neural networks class architecture operate structured input previously successfully applied model natural language using tree based structural representations architectures deep structure lack capacity hierarchical representation exists conventional deep feed forward networks recently investigated deep recurrent neural networks work introduce architecture deep recursive neural network deep rnn constructed stacking multiple recursive layers evaluate proposed model task fine grained sentiment classification results show deep rnns outperform associated shallow counterparts employ number parameters furthermore approach outperforms previous baselines sentiment analysis task including multiplicative rnn variant recently introduced vectors achieving state art results provide analyses effect multiple layers show capture different aspects language 
inference learning speeding graphical model optimization coarse fine cascade pruning classifiers propose general versatile framework significantly graphical model optimization maintaining excellent solution accuracy proposed approach inference learning relies multi scale pruning scheme reduces solution space coarse fine cascade learnt classifiers thoroughly experiment classic computer vision related problems novel framework yields significant time speed respect efficient inference methods obtains accurate solution directly optimizing code available 
filtering approach stochastic variational inference stochastic variational inference svi uses stochastic optimization scale bayesian computation massive data present alternative perspective svi approximate parallel coordinate ascent svi trades bias variance step close unknown true coordinate optimum given batch variational bayes vb define model process model infers location vb optimum sequence noisy realizations consequence construction update variational parameters using bayes rule hand crafted optimization model kalman filter procedure recover original svi algorithm svi adaptive steps may encode additional assumptions model heavy tailed noise doing algorithm outperforms original svi state art adaptive svi algorithm diverse domains 
optimizing measures cost sensitive classification present theoretical analysis measures binary multiclass multilabel classification performance measures non linear many scenarios pseudo linear functions class false negative false positive rate based observation present general reduction measure maximization cost sensitive classification unknown costs propose algorithm provable guarantees obtain approximately optimal classifier measure solving series cost sensitive classification problems strength analysis valid any dataset any class classifiers extending existing theoretical results measures asymptotic nature present numerical experiments illustrate relative importance cost thresholding learning linear classifiers various measure optimization tasks 
layer feature reduction sparse group lasso decomposition convex sets sparse group lasso shown powerful regression technique simultaneously discovering group group sparse patterns using combination norms however large scale applications complexity regularizers entails great computational challenges paper propose novel layer feature reduction method decomposition its dual feasible set layer reduction able quickly identify groups features respectively guaranteed sparse representation optimization existing feature reduction methods only applicable sparse models sparsity inducing regularizer best knowledge capable dealing multiple sparsity inducing regularizers moreover very low computational cost integrated any existing solvers experiments synthetic real data sets show improves efficiency orders magnitude 
improved multimodal deep learning variation information deep learning successfully applied multimodal representation learning problems common strategy learning joint representations shared multiple modalities top layers modality specific networks remains question learn good association data modalities particular good generative model multimodal data able reason missing data modality given data modalities paper propose novel multimodal representation learning framework explicitly aims goal learning maximum likelihood train model minimize variation information provide theoretical insight why proposed learning objective sufficient estimate data generating joint distribution multimodal data apply method restricted boltzmann machines introduce learning methods based divergence multi prediction training addition extend deep networks recurrent encoding structure network experiments demonstrate state art visual recognition performance database pascal voc database features 
patch based exponentially weighted aggregation image denoising patch based methods widely used noise reduction years paper propose general statistical aggregation method combines image patches several commonly used algorithms show weakly versions input image obtained standard methods serve compute efficient patch based aggregated aggregation estimator resulting approach based mcmc sampling nice statistical foundation producing denoising results comparable current state art demonstrate performance denoising algorithm real images compare results several competitive methods 
elementary estimators graphical models propose class closed form estimators sparsity structured graphical models expressed exponential family distributions high dimensional settings approach builds observing precise manner classical graphical model mle high dimensional settings estimator uses carefully constructed defined closed form backward map performs thresholding operations ensure desired sparsity structure provide rigorous statistical analysis shows surprisingly simple class estimators recovers asymptotic convergence rates regularized mles difficult compute corroborate statistical performance significant computational advantages simulations discrete gaussian graphical models 
birkhoff polytope convex relaxations vector permutation problems birkhoff polytope convex hull set permutation matrices represented using variables constraints frequently relaxations optimization problems permutations using construction show optimizing convex hull permutation vectors reduce number variables constraints theory practice modify convex formulation sum problem introduced polytope demonstrate attain results similar quality significantly computational time large knowledge usage compact formulation convex optimization problem introduce simpler regularization scheme convex formulation sum problem yields good empirical results 
neural word embedding implicit matrix factorization analyze gram negative sampling word embedding method introduced show implicitly word context matrix cells pointwise mutual information respective word context pairs global constant another embedding method implicitly similar matrix cell log conditional probability word given its context show using sparse positive word context matrix represent words improves results word similarity tasks tasks dense low dimensional vectors exact factorization svd achieve solutions good solutions word similarity tasks questions remains superior svd conjecture weighted nature factorization 
multi resolution multiclass object detection algorithm learning fast multiclass object detection introduced produces multi resolution early stages binary target vs non target detectors eliminate false positives stages multiclass classifiers discriminate target classes middle stages intermediate numbers classes determined data driven manner structure achieved biased boosting algorithm extends previous multiclass boosting approaches boosting mechanisms shown implement complementary data driven biases standard bias towards examples difficult classify bias towards difficult classes shown structural biases implemented generalizing class based bias encourage desired structure generalized definition multiclass margin includes set bias parameters boosting algorithm maximization margin interpreted standard multiclass boosting algorithm augmented margin thresholds cost sensitive boosting algorithm costs defined bias parameters stage adaptive bias policy introduced determine bias parameters data driven manner shown produce high detection rate computationally efficient experiments multiclass object detection show improved performance previous solutions 
median selection subset aggregation parallel inference massive data sets efficient computation commonly relies distributed algorithms store process subsets data different machines minimizing communication costs focus regression classification problems involving many features variety distributed algorithms proposed context challenges arise defining algorithm low communication theoretical guarantees excellent practical performance general settings propose median selection subset aggregation estimator message algorithm attempts solve problems algorithm applies feature selection parallel subset using lasso another method calculates feature index estimates coefficients selected features parallel subset estimates algorithm simple involves very minimal communication scales efficiently sample feature size theoretical guarantees particular show model selection consistency coefficient estimation efficiency extensive experiments show excellent performance variable selection estimation prediction computation time relative usual competitors 
recurrent models visual attention applying convolutional neural networks large images computationally expensive because amount computation scales linearly number image present novel recurrent neural network model capable extracting information image video adaptively selecting sequence regions locations only processing selected regions high resolution convolutional neural networks proposed model degree translation invariance amount computation performs controlled independently input image size model non differentiable trained using reinforcement learning methods learn task specific policies evaluate model several image classification tasks significantly outperforms convolutional neural network baseline images dynamic visual control problem learns track simple object explicit training signal doing 
tree structured gaussian process approximations gaussian process regression accelerated constructing small pseudo dataset observed data idea many approximation schemes approach requires number pseudo scaled range input space accuracy approximation maintained presents problems time series settings spatial datasets large numbers pseudo required since computation typically scales pseudo dataset size paper devise approximation complexity grows linearly number pseudo achieved imposing tree chain structure pseudo approximation using kullback leibler kl inference learning performed efficiently using gaussian belief propagation algorithm demonstrate validity approach set challenging regression tasks including missing data imputation audio spatial datasets trace speed accuracy trade method show obtained large number existing approximation techniques 
active learning best response dynamics consider setting low power distributed sensors making highly noisy measurements unknown target function center accurately learn function small number sensors impossible due high noise rate question address local communication sensors together natural best response dynamics appropriately game system true signal allow center succeed only small number active queries prove positive negative results denoising power several natural dynamics show experimentally combined agnostic active learning algorithms process achieve low error very queries performing substantially better active passive learning denoising dynamics passive learning denoising 
analog balanced rate based network neurons persistent graded activity observed cortical circuits sometimes seen retrieval stored synaptic however despite decades theoretical work subject mechanisms support storage retrieval remain unclear previous proposals dynamics memory networks short incorporating key constraints unified way specifically models violate law allow neurons excitatory inhibitory others restrict representation binary induce states neurons fire rates close saturation propose novel control theoretic framework build attractor networks satisfy set relevant constraints directly optimize networks excitatory inhibitory neurons force sets arbitrary analog patterns become stable fixed points dynamics resulting networks operate balanced regime robust corruptions memory ongoing noise explain reduction trial trial variability following stimulus onset ubiquitously observed sensory motor results constitute step forward understanding neural substrate memory 
fast sampling based inference balanced neuronal networks multiple lines evidence support notion brain performs probabilistic inference multiple cognitive domains including perception decision making evidence probabilistic inference may implemented brain quasi stochastic activity neural circuits producing samples appropriate posterior distributions effectively implementing markov chain monte carlo algorithm however time becomes fundamental bottleneck sampling based probabilistic representations quality inferences depends fast neural circuit generates uncorrelated samples its stationary distribution posterior explore bottleneck simple linear gaussian latent variable model posterior sampling achieved stochastic neural networks linear dynamics known langevin sampling far only sampling algorithm continuous variables neural implementation suggested naturally fits dynamical framework however show analytically simulations symmetry synaptic weight matrix implied yields slow mixing posterior high dimensional using methods control theory construct networks optimally fast hence orders magnitude faster being far biologically plausible networks strong selective external noise generates spatially correlated activity fluctuations posterior intriguingly detailed balance inhibition dynamically maintained detailed balance markov chain steps resulting sampler violated consistent findings statistical overcome speed limitation random domains 
spectral learning mixture hidden markov models paper propose learning approach mixture hidden markov models based method moments computational advantages learning amenable large data sets possible directly learn existing learning approaches due permutation ambiguity estimation process show possible resolve ambiguity using spectral properties global transition matrix presence estimation noise demonstrate validity approach synthetic real data 
subspace embeddings polynomial kernel sketching powerful dimensionality reduction tool statistical learning algorithms however its applicability limited certain extent since crucial called subspace embedding only applied data spaces explicit representation column row matrix many settings learning done high dimensional space implicitly defined data matrix kernel transformation propose fast subspace embeddings able embed space induced non linear kernel explicitly mapping data high dimensional space particular propose embedding mappings induced polynomial kernel using subspace embeddings obtain fastest known algorithms computing implicit low rank approximation higher dimension mapping data matrix computing approximate kernel pca data doing approximate kernel principal component regression 
boosting framework grounds online learning exploiting boosting online learning present boosting framework proves extremely powerful thanks employing vast knowledge available online learning area using framework develop various algorithms address multiple practically theoretically interesting questions including sparse boosting smooth distribution boosting agnostic learning product generalization double projection online learning algorithms 
dual algorithm computation brain study early system attempt explain its characterized structure dynamics propose its computational function recovery high dimensional sparse signals small number measurements detailed experimental knowledge system rules standard algorithmic solutions problem show solving dual formulation corresponding problem yields structure dynamics good agreement biological data biological constraints lead reduced form dual formulation system uses independent component analysis adapt its environment allow accurate sparse recovery work demonstrates challenges rewards attempting detailed understanding experimentally characterized systems 
advances learning bayesian networks bounded work presents novel algorithms learning bayesian networks bounded exact approximate methods developed exact method combines mixed integer linear programming formulations structure learning computation approximate method consists sampling trees graphs subsequently selecting exactly approximately best structure graph subgraph tree approaches empirically compared state art methods collection public data sets variables 
learning learning rate prediction expert advice standard algorithms prediction expert advice depend parameter called learning rate learning rate needs large fit data small prevent overfitting exponential weights algorithm sequence prior work established theoretical guarantees higher higher data dependent learning rate allow increasingly aggressive learning practice theoretical perform worse measured regret hoc tuning higher learning rate close gap theory practice introduce approach learn learning rate factor logarithmic number experts inverse learning rate method performs know empirically best learning rate large range includes conservative small values values higher formal guarantees previously available method employs grid learning rates runs linear time regardless size grid 
information theoretic limits learning ising models provide general framework computing lower bounds sample complexity recovering underlying graphs ising models given samples results specific graph classes involve fairly extensive technical arguments specific graph class contrast key graph structural used specify sample complexity lower bounds presence structural properties makes graph class hard learn derive main result only recover existing results provide lower bounds novel graph classes considered previously extend framework random graph setting derive renyi graphs certain dense setting 
efficient optimization average precision svm accuracy information retrieval systems measured using average precision given set positive relevant negative non relevant samples parameters retrieval system estimated using svm framework minimizes regularized convex upper bound empirical loss however high computational complexity loss augmented inference required learning svm its large training datasets deficiency propose complementary approaches approach guarantees asymptotic decrease computational complexity loss augmented inference exploiting problem structure second approach takes advantage fact require full ranking during loss augmented inference helps avoid expensive step sorting negative samples according individual scores third approach approximates loss samples loss difficult samples example binary svm ensuring correct classification remaining samples using pascal voc action classification object detection datasets show approaches provide significant speed during training accuracy svm 
asymmetric sublinear time maximum product search present provably sublinear time hashing algorithm approximate maximum product search searching normalized product underlying similarity measure known difficult problem finding hashing schemes considered hard existing sensitive hashing framework insufficient solving paper extend framework allow asymmetric hashing schemes proposal based key observation problem finding maximum products independent asymmetric transformations converted problem approximate near neighbor search classical settings key observation makes efficient sublinear hashing scheme possible extended asymmetric framework paper provides example explicit construction provably fast hashing scheme proposed algorithm simple easy implement proposed hashing scheme leads significant computational savings popular conventional schemes sign random projection ii hashing based stable distributions norm collaborative filtering task item recommendations datasets 
framework studying synaptic plasticity neural spike train data learning memory brain implemented complex time varying changes neural computational rules according synaptic weights change time subject research precisely understood recently limitations experimental methods challenging hypotheses synaptic plasticity large scale however data become available barriers lifted becomes necessary develop analysis techniques validate plasticity models present highly extensible framework modeling arbitrary synaptic plasticity rules spike train data populations neurons synaptic weights potentially nonlinear dynamical system embedded fully bayesian generalized linear model glm addition provide algorithm inferring synaptic weight trajectories parameters glm learning rules using method perform model comparison proposed variants known spike timing dependent plasticity stdp rule nonlinear effects play substantial role synthetic data generated neuron show recover weight trajectories pattern connectivity underlying learning rules 
randomized experimental design causal graph discovery examine number controlled experiments required discover causal graph showed number experiments required logarithmic cardinality maximum undirected clique essential graph lower bounds however assume experiment randomization selecting experiments show significant improvements possible randomization adversarial worst case setting recover causal graph using log log experiments expectation bound improved show tight causal graphs show non adversarial average case setting larger improvements possible causal graph chosen uniformly random model expected number experiments discover causal graph constant finally present computer simulations complement theoretic results work exploits structural essential graphs based set orientation operations results show distinction operations important worst case average case settings 
multiplicative model learning distributed based attribute representations paper propose general framework learning distributed representations attributes characteristics representations jointly learned word embeddings attributes wide variety concepts document learn sentence vectors language learn distributed language representations meta data side information age representations authors describe third order model word context attribute vectors interact predict word sequence leads notion conditional word similarity words change conditioned different attributes perform several experimental tasks including sentiment classification cross document classification qualitatively evaluate conditional word attribute conditioned generation 
learning markov networks dynamic programming present algorithm finding markov network maximizes any given decomposable scoring function algorithm based recursive clique trees runs time vertices vertex benchmark instance implementation turns times faster recently proposed constraint based algorithm nips hours able solve instances vertices restrict maximum clique size study performance integer linear programming algorithm results suggest bound clique sizes currently only dynamic programming algorithm guaranteed solve instances vertices 
depth map prediction single image using multi scale deep network predicting depth essential component understanding geometry scene images local correspondence suffices estimation finding depth relations single image straightforward requiring integration global local information various moreover task inherently ambiguous large source uncertainty coming overall scale paper present method addresses task employing deep network makes coarse global prediction based entire image another refines prediction locally apply scale invariant error measure depth relations scale leveraging raw datasets large sources training data method achieves state art results depth matches detailed depth boundaries need 
proximal quasi newton computationally intensive regularized estimators consider class optimization problems arising computationally intensive regularized estimators function gradient values very expensive compute particular instance interest regularized mle learning conditional random fields popular class statistical models structured prediction problems sequence labeling alignment classification label regularized mles particularly expensive optimize since computing gradient values requires expensive inference step work propose carefully constructed proximal quasi newton algorithm computationally intensive estimation problems employ aggressive active set selection technique key contribution paper show proximal quasi newton algorithm provably super linearly convergent strong convexity leveraging restricted variant strong convexity experiments proposed algorithm converges faster current state art problems sequence labeling hierarchical classification 
probabilistic framework multimodal retrieval using indian buffet process propose multimodal retrieval procedure based latent feature models procedure consists nonparametric bayesian framework learning underlying semantically meaningful abstract features multimodal dataset probabilistic retrieval model allows cross modal queries extension model relevance feedback experiments multimodal datasets pascal sentence sun attribute demonstrate effectiveness proposed retrieval procedure comparison state art algorithms learning binary codes 
searching decay modes deep learning particle enable fundamental nature matter observing particles produced high energy because experimental measurements necessarily incomplete machine learning algorithms play major role analysis experimental data high energy physics community typically relies machine learning software analysis substantial effort towards improving statistical power hand high level features derived raw measurements paper train artificial neural networks detect decay dataset simulated events demonstrate deep neural network architectures particularly suited task ability automatically discover high level features data increase discovery 
structure regularization structured prediction many studies weight regularization study structure regularization rare many existing systems structured prediction focus increasing level structural dependencies model however trend because study suggests complex structures generalization ability structured prediction control structure based overfitting propose structure regularization framework structure decomposition decomposes training samples mini samples simpler structures deriving model better generalization power show theoretically empirically structure regularization effectively control overfitting risk lead better accuracy product proposed method substantially accelerate training speed method theoretical results apply general graphical models arbitrary structures experiments known tasks demonstrate method easily benchmark systems highly competitive tasks achieving record breaking accuracies substantially faster training speed 
multiplicative multitask feature learning investigate general framework multiplicative multitask feature learning decomposes task model parameters multiplication components components used tasks component task specific several previous methods proposed special cases framework study theoretical properties framework different regularization conditions applied decomposed components prove framework mathematically equivalent widely used multitask feature learning methods based joint regularization model parameters general form regularizers analytical formula derived task component related task specific component regularizers leading better understanding shrinkage effect study framework motivates multitask learning algorithms propose learning formulations varying parameters proposed framework empirical studies revealed relative advantages formulations comparing state art provides insights feature learning problem multiple tasks 
multivariate divergence estimation confidence problem divergence estimation important fields machine learning information theory statistics several divergence estimators exist relatively known convergence properties particular estimators convergence rates known asymptotic distributions unknown establish asymptotic recently proposed ensemble estimator divergence distributions finite number samples estimator convergence rate simple implement performs high dimensions theory enables perform divergence based inference tasks testing pairs distributions based empirical samples experimentally validate theoretical results empirically bound best achievable classification error 
generalized unsupervised manifold alignment paper propose generalized unsupervised manifold alignment method build connections different correlated datasets any known based assumption datasets usually similar manifold structures formulated explicit integer optimization problem considering structure matching preserving criteria feature corresponding points mutual embedding space main benefits model include simultaneous discovery alignment manifold structures fully unsupervised matching any pre specified efficient iterative alignment computations permutation cases experimental results dataset matching real world applications demonstrate effectiveness manifold alignment method 
smoothed gradients stochastic variational inference stochastic variational inference svi scale bayesian computation massive data uses stochastic optimization fit variational distribution following easy compute noisy natural gradients traditional stochastic optimization methods svi takes unbiased stochastic gradients equal true gradients paper explore idea following biased stochastic gradients svi method natural gradient similarly constructed vector uses fixed window moving average its previous terms demonstrate many advantages technique its computational cost svi storage requirements only constant factor second enjoys significant variance reduction unbiased estimates smaller bias gradients leads smaller mean squared error full gradient method latent dirichlet allocation large corpora 
recursive context propagation network semantic scene labeling propose deep feed forward neural network architecture pixel wise semantic scene labeling uses novel recursive neural network architecture context propagation referred maps local visual features semantic space followed bottom aggregation local information global representation entire image top propagation aggregated information takes place contextual information local feature therefore information every location image propagated every location experimental results background sift flow datasets show proposed method outperforms previous approaches orders magnitude faster previous methods takes only seconds gpu pixel wise labeling image starting raw pixel values given super pixel takes additional seconds using shelf implementation 
sparse random feature algorithm coordinate descent hilbert space paper propose sparse random feature algorithm learns sparse non linear predictor minimizing regularized objective function hilbert space induced kernel function interpreting algorithm randomized coordinate descent infinite dimensional space show proposed approach converges solution comparable precision exact kernel method drawing number random features type convergence achieved monte carlo analysis current random feature literature experiments sparse random feature algorithm obtains sparse solution requires memory prediction time comparable performance tasks regression classification approximate solver infinite dimensional regularized problem randomized approach converges better solution boosting approach greedy step boosting performed exactly 
optimal teaching limited capacity human learners basic decisions person involve novel stimuli work finds category guided small set examples retrieved memory decision time limited stochastic retrieval places limits human performance probabilistic classification decisions light capacity limitation work finds training items ambiguous cases reduced improves human performance novel items previous work category distributions idealized hoc heuristic fashion contribution principles approach constructing idealized training sets apply machine teaching procedure cognitive model limited capacity humans capacity machine learning systems predicted machine idealized training sets human learners perform best training recommendations machine based limited capacity model predicted extent learning model used machine true nature human learners recommendations machine prove effective results provide normative basis given capacity constraints procedures offer novel selection procedure models human learning 
social activity users events online social network roughly endogenous events users just respond actions neighbors network events users actions due drives external network external drive provided user network activity towards target paper model social events using multivariate processes capture endogenous derive time dependent linear relation events overall network activity exploiting connection develop convex optimization framework determining required level external drive order network reach desired activity level data twitter show method activity network accurately alternatives 
analysis brain states multi region lfp time series local field potential lfp source information broad patterns brain activity frequencies present time series measurements highly correlated regions believed regions may jointly constitute state relating behavior infinite hidden markov model proposed model evolution brain states based lfp data measured multiple brain regions brain state spectral content region measured lfp state dependent tensor factorization employed brain regions spectral properties lfps characterized terms gaussian processes gps lfps modeled mixture gps state region dependent mixture weights spectral content data encoded spectral mixture covariance kernels model able estimate number brain states number mixture components mixture gps variational bayesian split merge algorithm employed inference model infers state changes function external covariates novel datasets using lfp data recorded simultaneously multiple brain regions results validated interpreted subject matter experts 
based filtering crowdsourcing paper study problem aggregating noisy labels crowd workers infer underlying true labels binary tasks unlike prior work examined problem random paradigm consider broader class adversarial workers specific assumptions labeling strategy key contribution design computationally efficient algorithm identify filter adversarial workers crowdsourcing systems algorithm uses concept optimal semi matchings penalties based label assign score every provide strong theoretical guarantees deterministic adversarial strategies extreme case sophisticated adversaries analyze worst case behavior algorithm finally show algorithm significantly improve accuracy existing label aggregation algorithms real world crowdsourcing datasets 
multi class deep boosting present ensemble learning algorithms multi class classification algorithms base classifier set family deep decision trees rich complex families benefit strong generalization guarantees give data dependent learning bounds convex ensembles multi class classification setting expressed terms rademacher complexities sub families base classifier set mixture weight assigned sub family bounds finer existing ones thanks improved dependency number classes favorable complexity term expressed average rademacher complexities based mixture weights introduce discuss several multi class ensemble algorithms guarantees prove positive results consistency several report results experiments showing performance compares favorably multi class versions logistic regression regularized counterparts 
differential equation modeling accelerated gradient method theory insights derive second order ordinary differential equation ode limit accelerated gradient method ode exhibits approximate equivalence scheme serve tool analysis show continuous time ode allows better understanding scheme byproduct obtain family schemes similar convergence rates ode interpretation suggests scheme leading algorithm rigorously proven converge linear rate whenever objective strongly convex 
difference convex functions programming reinforcement learning large markov decision processes mdps usually solved using approximate dynamic programming methods approximate value iteration approximate policy iteration main contribution paper show alternatively optimal state action value function estimated using difference convex functions programming study minimization norm optimal residual q$ *$ called optimal operator controlling residual allows controlling distance optimal action value function show minimizing empirical norm sense finally frame optimization problem program allows using large related literature programming address reinforcement rl problem 
design principles cognitive map place fields shown reflect behaviorally relevant aspects space instance place fields tend skewed commonly directions cluster locations constrained geometric structure environment set design principles cognitive map explain place fields represent space way facilitates navigation reinforcement learning particular suggest place fields encode just information current location predictions future locations current transition distribution model variety place field phenomena arise naturally structure rewards barriers biases reflected transition policy furthermore demonstrate representation space support efficient reinforcement learning propose grid cells compute place fields part because useful segmenting natural boundaries applied segmentation used discover hierarchical decomposition space grid cells computing hierarchical reinforcement learning 
deep symmetry networks chief difficulty object recognition objects classes large number extraneous sources variability pose part deformation sources variation represented symmetry groups sets transformations preserve object identity convolutional neural networks convnets achieve degree invariance computing feature maps translation group handle groups result groups effects approximated small translations requires datasets leads high sample complexity paper introduce deep symmetry networks generalization convnets forms feature maps arbitrary symmetry groups kernel based interpolation parameters pool symmetry spaces any dimension convnets trained backpropagation composition feature transformations layers provides approach deep learning experiments mnist show group greatly reduce sample complexity relative convnets better capturing symmetries data 
nonparametric bayesian inference multivariate exponential families develop model choosing maximum entropy distribution set models satisfying certain smoothness independence criteria show inference model generalizes local kernel estimation context bayesian inference stochastic processes model enables bayesian inference contexts standard techniques gaussian process inference expensive apply exact inference model possible any likelihood function exponential family inference highly efficient requiring only log time space run time demonstrate algorithm several problems show quantifiable improvement speed performance relative models based gaussian process 
optimal rates density mode estimation present related contributions independent interest high probability finite sample rates density estimation practical mode estimators based attain minimax optimal rates surprisingly general distributional conditions 
feedforward learning mixture models develop biologically plausible learning rule provably converges class means general mixture models rule generalizes classical bcm neural rule tensor framework substantially increasing generality learning problem solves achieves incorporating samples mixtures provides novel information processing interpretation spike timing dependent plasticity provide proofs convergence close fit experimental data stdp 
diverse randomized agents investigate power voting diverse randomized software agents computer agents mind develop novel theoretical model stage noisy voting builds work machine learning model allows reason collection agents different biases determined stage noise models furthermore apply randomized algorithms evaluate alternatives produce votes captured second stage noise models analytically demonstrate uniform team consisting multiple instances any single agent significant number whereas diverse team converges number agents grows experiments computer agents strong agents provide evidence effectiveness voting agents diverse 
ranking robust binary classification propose ranking algorithm motivated observing close connection evaluation metrics learning rank loss functions robust classification algorithm shows very competitive performance standard benchmark datasets algorithms literature large scale problems explicit feature vectors scores given algorithm efficiently parallelized large number machines task requires pairwise interactions items ranked algorithm finds solutions dramatically higher quality state art competitor algorithm given amount wall clock time computation 
distributed balanced clustering mapping coresets large scale clustering data points metric spaces important problem mining big data sets many applications face explicit implicit size constraints cluster leads problem clustering capacity constraints clustering problem balanced clustering problem widely studied developing theoretically sound distributed algorithm remains open problem present paper develop general framework based coresets tackle issue wide range clustering objective functions center median means techniques give distributed algorithms balanced clustering match best known single machine approximation ratios 
data parallel probabilistic modeling implementing inference procedures probabilistic model time consuming error probabilistic programming addresses problem allowing user specify model automatically generating inference procedure practical important generate high performance inference code modern architectures high performance requires parallel execution paper present probabilistic modeling language bayesian networks designed effective data parallel architectures gpus show generate data parallel inference code scalable thousands gpu cores making conditional independence relationships bayesian network 
learning mixtures ranking models work learning probabilistic models ranking data heterogeneous population specific problem study learning parameters mallows mixture model despite being widely studied current heuristics problem theoretical guarantees local optima present polynomial time algorithm provably learns parameters mixture mallows models key component algorithm novel tensor decomposition techniques learn top rankings before work question identifiability case mixture mallows models 
controlling privacy recommender systems recommender systems involve inherent trade accuracy recommendations extent users information preferences paper explore notion privacy small set users share preferences large set users require privacy guarantees show theoretically demonstrate empirically moderate number public users access private user information already suffices reasonable accuracy moreover introduce privacy concept relational information private users maintaining order demonstrate gains controlled access private user preferences 
convolutional kernel networks important goal visual recognition devise image representations invariant particular transformations paper address goal type convolutional neural network cnn invariance encoded reproducing kernel unlike traditional approaches neural networks learned represent data solving classification task network learns approximate kernel feature map training data approach enjoys several benefits classical ones teaching cnns invariant obtain simple network architectures achieve similar accuracy complex ones being easy train robust overfitting second bridge gap neural network literature kernels natural tools model invariance evaluate methodology visual recognition tasks cnns proven perform digit recognition mnist dataset challenging cifar stl datasets accuracy competitive state art 
multi agent sequential decision making define solution criterion multi agent decision making problems agents local criterion aims maximize worst performance agents consideration overall performance develop simple linear programming approach scalable game theoretic approach computing optimal policy game theoretic approach optimization player zero sum game employs iterative algorithm finding nash equilibrium corresponding optimal policy scale approach exploiting problem structure value function approximation experiments resource allocation problems show criterion provides favorable solution criterion game theoretic approach significantly faster linear programming 
submodular meets structured finding diverse subsets exponentially large structured item sets cope high level ambiguity domains computer vision natural language processing robust prediction methods search diverse set high quality candidate solutions proposals structured prediction problems becomes task solution space image sentence exponentially large study greedy algorithms finding diverse subset solutions structured output spaces drawing connections submodular functions combinatorial item sets high order potentials studied graphical models specifically show examples marginal gains submodular diversity functions allow structured representations enables efficient sub linear time approximate maximization reducing greedy augmentation step inference factor graph appropriately constructed discuss benefits tradeoffs show lead significantly better proposals 
deep nets need currently deep neural networks state art problems speech recognition computer vision paper empirically demonstrate shallow feed forward nets learn complex functions previously learned deep nets achieve accuracies previously only achievable deep models moreover cases shallow nets learn deep functions using number parameters original deep models recognition cifar image recognition tasks shallow nets trained perform similarly complex engineered deeper convolutional models 
dynamic rank factor model streams propose semi parametric dynamic rank factor model topic modeling capable discovering topic prevalence time learning contemporary multi scale dependence structures providing topic word correlations byproduct high dimensional time evolving ordinal rank observations word counts arbitrary monotone transformation underlying dynamic sparse factor model framework naturally admits heavy tailed capable inferring temporal importance topics posterior inference performed straightforward gibbs sampling based forward filtering backward sampling algorithm moreover efficient data subsampling scheme leveraged speed inference massive datasets modeling framework real datasets state union address collection science 
generative adversarial nets propose framework estimating generative models adversarial nets simultaneously train models generative model captures data distribution discriminative model estimates probability sample training data training procedure maximize probability making framework corresponds minimax player game space arbitrary functions unique solution exists recovering training data distribution equal case defined multilayer entire system trained backpropagation need any markov chains approximate inference networks during training generation samples experiments demonstrate potential framework qualitative quantitatively evaluation generated samples 
testing gaussian graphical models global markov property gaussian graphical models graph separation implies conditional independence specifically node set graph nodes u$ conditionally independent given s$ opposite direction need true s$ need imply node separator does relation s$ called paper provide relations provide algorithm faithfulness based only knowledge conditional relations form s$ 
global sensitivity analysis map inference graphical models study sensitivity map discrete probabilistic graphical model respect perturbations its parameters perturbations global sense simultaneous perturbations parameters any chosen subset allowed main contribution exact algorithm check map robust respect given perturbations its complexity essentially obtaining map used minimal effort algorithm identify largest global perturbation does induce change map successfully apply robustness measure practical scenarios prediction action units posed images classification multiple real public data sets strong correlation proposed robustness measure accuracy verified scenarios 
deconvolution high dimensional mixtures boosting application diffusion weighted human brain diffusion weighted imaging only methods measure structure matter human brain diffusion signal modelled combined contribution many individual passing location matter typically done basis pursuit estimation exact directions limited due discretization difficulties inherent modeling data shared many problems involving fitting non parametric mixture models proposed approach continuous basis pursuit overcome discretization error dimensional case spike sorting propose general algorithm fits mixture models any dimensionality discretization algorithm uses principles boost together weights pruning parameters addition steps boost algorithm its accuracy refer resulting algorithm elastic basis pursuit ebp since expands active set kernels needed show contrast existing approaches fitting mixtures boosting framework enables selection optimal bias variance tradeoff solution path scales high dimensional problems simulations ebp yields better parameter estimates non negative squares approach standard model used tensor model serves basis diffusion tensor imaging demonstrate utility method data acquired parts brain containing multiple 
efficient minimax signal detection graphs several problems network intrusion community detection disease described observations nodes edges graph applications presence intrusion community disease characterized novel observations unknown connected subgraph problems formulated terms optimization suitable objectives connected problem generally computationally difficult overcome connectivity embedding connected linear matrix inequalities computationally efficient tests optimizing convex objective functions subject constraints prove means novel euclidean embedding tests minimax optimal exponential family distributions show internal conductance connected subgraph family plays fundamental role 
constrained principal component analysis estimating vector noisy quadratic observations task arises naturally many contexts dimensionality reduction synchronization phase retrieval problems case additional information available unknown vector instance sparsity sign magnitude its entries many authors propose non convex quadratic optimization problems aim exploiting optimally information however solving problems typically hard consider simple model noisy quadratic observation unknown vector unknown vector constrained optimal estimation appears intractable general problems class provide evidence tractable convex efficient projection surprising since corresponding optimization problem non convex worst case perspective hard characterize resulting minimax risk terms statistical dimension already known control risk estimation gaussian observations random linear measurements surprising plays role estimation risk quadratic measurements 
communication cost distributed statistical estimation dimensionality explore connection dimensionality communication cost distributed learning problems specifically study problem estimating mean unknown dimensional gaussian distribution distributed setting problem samples unknown distribution distributed different machines goal estimate mean optimal minimax rate bits possible show setting communication cost scales linearly number dimensions needs deal different dimensions individually applying result previous lower bounds dimension setting improved bounds simultaneous setting prove lower bounds bits communication needed achieve minimax squared loss simultaneous settings respectively complement demonstrate achieving minimax squared loss bits communication improves simple simultaneous logarithmic factor given strong lower bounds general setting initiate study distributed parameter estimation problems structured parameters specifically parameter sparse show simple thresholding based achieves squared loss s$ factor communication conjecture tradeoff communication squared loss demonstrated essentially optimal logarithmic factor 
computing nash generalized security games study computational complexity computing nash generalized security games traditional games originally introduced risk experts decade generalized games model decisions potential direct risk transfer risk agents distinct feature generalized games however full reduce transfer risk result depending transfer risk reduction level generalized games may exhibit strategic strategic consider variants generalized games players exhibit only only show determining pure strategy nash equilibrium type games complete computing single type games takes worst case polynomial time problem computing mixed strategy nash efficiently produce partial whenever agent game terms transfer risk agents case originally studied context traditional games nips paper compute satisfy constraints polynomial time game variants computational barrier general transfer case show computational problem hard pure nash extension problem originally introduced complete variants finally experimentally examine discuss practical impact additional protection transfer risk allowed generalized games solving several randomly generated instances type games graph structures taken several real world datasets 
consistent binary classification generalized performance metrics performance metrics binary classification designed capture tradeoffs fundamental population quantities true positives false positives true false despite significant interest theoretical applied communities little known optimal classifiers consistent algorithms optimizing binary classification performance metrics special cases consider fairly large family performance metrics given ratios linear combinations fundamental population quantities family includes many known binary classification metrics classification accuracy measure measure similarity coefficient special cases analysis identifies optimal classifiers sign thresholded conditional probability positive class performance metric dependent threshold optimal threshold constructed using simple plug estimators performance metric linear combination population quantities alternative techniques required general case propose algorithms estimating optimal classifiers prove statistical consistency algorithms straightforward modifications standard approaches address key challenge optimal threshold selection simple implement practice algorithm combines plug estimate conditional probability positive class optimal threshold selection second algorithm leverages work calibrated asymmetric surrogate losses construct candidate classifiers present empirical comparisons algorithms benchmark datasets 
greedy subspace clustering consider problem subspace clustering given points lie near union many low dimensional linear subspaces recover subspaces identifies sets points close subspace uses sets estimate subspaces geometric structure clusters linear subspaces proper performance general distance based approaches means many model specific methods proposed paper provide simple efficient algorithms problem statistical analysis shows algorithms guaranteed exact perfect clustering performance certain conditions number points affinity subspaces conditions weaker considered standard statistical literature experimental results synthetic data generated standard subspaces model demonstrate theory show algorithm performs state art algorithms real world applications motion segmentation face clustering simpler implementation lower computational cost 
deterministic symmetric positive semidefinite matrix completion consider problem recovering symmetric positive semidefinite matrix subset its entries possibly corrupted noise contrast previous matrix recovery work drop assumption random sampling entries deterministic sampling principal matrix develop set sufficient conditions recovery matrix set its principal present results based set conditions develop algorithm exactly recover matrix conditions met proposed algorithm naturally generalized problem noisy matrix recovery provide worst case bound reconstruction error scenario finally demonstrate algorithm utility noisy simulated datasets 
multi step stochastic admm high dimensions applications sparse optimization matrix decomposition paper consider multi step version stochastic admm method efficient guarantees high dimensional problems analyze simple setting optimization problem consists loss function single regularizer sparse optimization extend multi block setting multiple regularizers multiple variables matrix decomposition sparse low rank components sparse optimization problem method achieves minimax rate sparse problems dimensions steps any method constant factors matrix decomposition problem general loss function analyze multi step admm multiple blocks establish rate efficient scaling size matrix grows natural noise models independent noise convergence rate minimax optimal establish tight convergence guarantees multi block admm high dimensions experiments show sparse optimization matrix decomposition problems algorithm outperforms state art methods 
online combinatorial optimization stochastic decision sets adversarial losses work sequential learning assumes fixed set actions available time however practice actions subsets sensors may break time time segments paper study learning algorithms able deal stochastic availability unreliable composite actions propose analyze algorithms based follow perturbed prediction method several learning settings differing feedback provided learner algorithms rely novel loss estimation technique call counting times deliver regret bounds algorithms previously studied full information semi bandit settings natural middle point call restricted information setting special consequence results significant improvement best known performance guarantees achieved efficient algorithm bandit problem stochastic availability finally evaluate algorithms empirically show improvement known approaches 
sampling inference probabilistic models fast bayesian propose novel sampling framework inference probabilistic models active learning approach converges quickly wall clock time markov chain monte carlo mcmc benchmarks central challenge probabilistic inference numerical integration average ensembles models unknown hyper parameters example compute marginal likelihood partition function mcmc provided approaches numerical integration deliver state art inference suffer sample poor convergence diagnostics bayesian techniques offer model based solution problems computation costs introduce model probabilistic likelihoods known non negative cheap active learning scheme optimally select sample locations algorithm demonstrated offer faster convergence seconds relative simple monte carlo importance sampling synthetic real world examples 
multi scale spectral decomposition massive graphs computing dominant eigenvalues eigenvectors massive graphs key numerous machine learning applications however popular solvers suffer slow convergence especially reasonably large paper propose analyze novel multi scale spectral decomposition method clusters graph smaller clusters spectral decomposition computed efficiently independently show theoretically empirically union cluster subspaces significant overlap dominant subspace original graph provided graph clustered appropriately eigenvectors clusters serve good initializations block algorithm used compute spectral decomposition original graph hierarchical clustering speed computation adopt fast early strategy compute quality approximations method outperforms widely used solvers terms convergence speed approximation quality furthermore method naturally parallelizable exhibits significant speedups shared memory parallel settings example graph nodes edges takes hours single core machine randomized svd takes hours obtain similar approximation top eigenvectors using cores reduce time minutes 
limits squared euclidean distance regularization simplest loss functions considered machine learning square loss logistic loss hinge loss common family algorithms including gradient descent weight decay always predict linear combination past instances give random construction sets examples target linear weight vector trivial learn any algorithm above family drastically sub optimal lower bound algorithms holds algorithms enhanced arbitrary kernel function type result known square loss however develop techniques prove hardness results any loss function satisfying minimal requirements loss function including above show algorithms squared euclidean distance easily random features finally conclude related open problems regarding feed forward neural networks conjecture hardness results hold any training algorithm based squared euclidean distance regularization back propagation weight decay heuristic 
alternating direction method multipliers mirror descent algorithm generalizes gradient descent using divergence replace squared euclidean distance paper similarly generalize alternating direction method multipliers admm admm allows choice different divergences exploit structure problems provides unified framework admm its variants including generalized admm inexact admm bethe admm establish global convergence iteration complexity cases faster admm factor dimensionality solving linear program mass transportation problem leads massive parallelism easily run gpu several times faster highly optimized commercial software gurobi 
multitask learning meets tensor factorization task imputation convex optimization study multitask learning problem task parametrized weight vector indexed pair time weight vectors collected tensor rank tensor controls amount sharing information tasks types convex relaxations recently proposed tensor rank however argue optimal context multitask learning dimensions rank typically heterogeneous propose norm call scaled latent trace norm analyze excess risk norms results apply various settings including matrix tensor completion multitask learning multitask learning theory experiments support advantage norm tensor equal sized priori know mode low rank 
model parallelization strategies distributed machine learning distributed machine learning typically data parallel perspective big data partitioned multiple workers algorithm different data subsets various synchronization schemes ensure speed correctness problem received relatively attention ensure efficient correct model parallel execution algorithms parameters program partitioned different workers iterative updates argue model data different challenges system design algorithmic theoretical analysis paper develop system model parallelism provides programming parameter updates discovering leveraging changing structural properties programs enables flexible tradeoff efficiency fidelity intrinsic dependencies models improves memory efficiency distributed demonstrate efficacy model parallel algorithms implemented versus popular implementations topic modeling matrix factorization lasso 
scalable inference neuronal connectivity calcium imaging calcium imaging provides potentially powerful tool inferring connectivity neural circuits thousands neurons however key challenge using calcium imaging connectivity detection current systems temporal response frame rate orders magnitude slower underlying neural spiking process bayesian inference based expectation maximization em proposed overcome limitations computationally demanding since step em procedure typically involves state estimation high dimensional nonlinear dynamical system work propose computationally fast method state estimation based hybrid belief propagation approximate message passing key insight neural system viewed calcium imaging factorized simple dynamical systems neuron linear neurons using structure updates proposed hybrid methodology computed set dimensional state estimation procedures linear transforms connectivity matrix yields computationally scalable method inferring connectivity large neural circuits simulations method realistic neural networks demonstrate good accuracy computation times potentially significantly faster current approaches based markov chain monte carlo methods 
structure learning ising models paper investigate computational complexity learning graph structure underlying discrete undirected graphical model samples result computational lower bound learning general graphical models nodes maximum degree class statistical algorithms recently introduced construction related notoriously difficult learning parities noise problem computational learning theory lower bound shows runtime required search algorithm significantly improved restricting class models structural assumptions graph being tree tree structure learning assume model correlation decay property indeed focusing ising models showed known low complexity algorithms fail learn simple graphs interaction strength exceeds number related correlation decay threshold second set results gives class models opposite behavior very strong allows efficient learning time provide algorithm performance depending strength 
noisy power method meta algorithm applications provide robust convergence analysis known power method computing dominant singular vectors matrix call noisy power method result characterizes convergence behavior algorithm large amount noise introduced matrix vector multiplication noisy power method seen meta algorithm recently number important applications broad range machine learning problems including alternating minimization matrix completion streaming principal component analysis pca privacy preserving spectral analysis general analysis subsumes several existing hoc convergence bounds resolves number open problems multiple applications work nips gives space efficient algorithm pca streaming model samples drawn spiked covariance model give simpler general analysis applies arbitrary distributions moreover spiked covariance model result gives quantitative improvements natural parameter regime second application provide algorithm differentially private principal component analysis runs nearly linear time input sparsity achieves nearly tight worst case error bounds worst case bounds show error dependence algorithm matrix dimension replaced essentially tight dependence coherence matrix result resolves main problem open leads strong average case improvements optimal worst case bound 
algorithm selection rational model human strategy selection selecting right algorithm important problem computer science because algorithm exploit structure input efficient human mind faces challenge therefore solutions algorithm selection problem models human strategy selection vice versa view algorithm selection problem special case derive solution outperforms existing methods sorting algorithm selection apply theory model choose cognitive strategies its prediction behavioral experiment quickly learn adaptively choose cognitive strategies choices experiment consistent model previous theories human strategy selection rational appears promising framework engineering choose cognitive strategies results better solutions algorithm selection problem 
extremal mechanisms local differential privacy local differential privacy recently strong measure privacy contexts personal information remains private data working setting data data want maximize utility statistical inferences performed data study fundamental tradeoff local differential privacy information theoretic utility functions introduce family extremal mechanisms call mechanisms prove contains optimal mechanism maximizes utility show information theoretic utility functions studied paper maximizing utility equivalent solving linear program outcome optimal mechanism however solving linear program computationally expensive since number variables exponential data size account show simple mechanisms binary randomized response mechanisms optimal high low privacy regimes respectively approximate intermediate regime 
global belief recursive neural networks recursive neural networks recently obtained state art performance several natural language processing tasks however because feedforward architecture correctly predict phrase word labels determined context problem tasks aspect specific sentiment classification instance predict word positive sentence beats introduce global belief recursive neural networks rnns based idea extending feedforward neural networks include step during inference allows phrase level predictions representations give feedback words show effectiveness model task contextual sentiment analysis show dropout improve rnn training combination unsupervised supervised word vector representations performs better step improves performance standard rnn task obtains state art performance challenge accurately predict sentiment specific entities 
statistical model tensor pca consider principal component analysis problem large tensors arbitrary order single spike rank noise model hand information theory results probability theory establish necessary sufficient conditions principal component estimated using unbounded computational resources turns possible signal noise ratio beta becomes larger log particular beta remain bounded problem dimensions increase hand analyze several polynomial time estimation algorithms based tensor power iteration message passing ideas graphical models show signal noise ratio system dimensions approaches succeeds possibly related fundamental limitation computationally tractable estimators problem moderate dimensions propose hybrid approach uses together power iteration show outperforms significantly baseline methods finally consider case additional side information available unknown signal characterize amount side information allow iterative algorithms converge good estimate 
real time decoding integrate fire neuronal encoding models range detailed biophysically based model statistical linear time invariant model specifying firing rates terms signal decoding becomes intractable does adequately capture nonlinearities present neuronal encoding system practical applications wish record output neurons namely spikes decode signal fast order drive machine example introduce causal real time decoder biophysically based integrate fire encoding neuron model show upper bound real time reconstruction error decreases polynomially time norm error bounded constant depends density spikes bandwidth decay input signal numerically validate effect parameters reconstruction error 
tolerant algorithms asynchronous distributed online learning analyze online gradient descent algorithms distributed systems large delays gradient computations corresponding updates using insights adaptive gradient methods develop algorithms adapt only sequence gradients precise update delays occur give impractical algorithm achieves regret bound precisely quantifies impact delays analyze algorithm efficiently achieves comparable guarantees key algorithmic technique appropriately efficiently learning rate used previous gradient steps experimental results show delays grow large updates algorithms perform significantly better standard adaptive gradient methods 
number linear regions deep neural networks study complexity functions deep feedforward neural networks piecewise linear terms symmetries number linear regions deep networks able sequentially map layer input space output way deep models compute functions react complicated patterns different inputs structure functions enables pieces computation exponentially terms network depth paper investigates complexity maps theoretical results regarding advantage depth neural networks piecewise linear activation functions particular analysis specific single family models example employ networks improve complexity bounds pre existing work investigate behavior units higher layers 
identifying point problem high dimensional non convex optimization central challenge many fields science engineering involves minimizing non convex error functions continuous high dimensional spaces gradient descent quasi newton methods ubiquitously used perform main source difficulty local methods global minimum local minima higher error global minimum argue based results statistical physics random matrix theory neural network theory empirical evidence deeper difficulty points local minima especially high dimensional problems practical interest points high error dramatically slow learning give existence local minimum motivated arguments propose approach second order optimization newton method rapidly high dimensional points unlike gradient descent quasi newton methods apply algorithm deep recurrent neural network training provide numerical evidence its superior optimization performance 
extracting latent structure multiple interacting neural populations developments neural recording technology rapidly recording populations neurons multiple brain areas simultaneously identification types neurons being recorded excitatory vs inhibitory growing need statistical methods study interaction multiple labeled populations neurons attempting identify direct interactions neurons number interactions grows number neurons squared propose extract smaller number latent variables population study latent variables interact specifically propose extensions probabilistic canonical correlation analysis capture temporal structure latent variables distinguish population dynamics population interactions termed group latent auto regressive analysis applied methods populations neurons recorded simultaneously visual areas provides better description recordings work provides foundation studying multiple populations neurons interact interaction supports brain function 
learning kernels paper propose framework supervised semi supervised learning based learning problem regularized integral equation approach fits naturally kernel framework interpreted constructing data dependent kernels call kernels proceed discuss noise assumption semi supervised learning provide evidence evidence theoretical experimental kernels effectively utilize unlabeled data noise assumption demonstrate methods based learning show very competitive performance standard semi supervised learning setting 
ball auxiliary sampling factorial hidden markov models introduce novel sampling algorithm markov chain monte carlo based bayesian inference factorial hidden markov models algorithm based auxiliary variable construction model space allowing iterative exploration polynomial time sampling approach limitations common conditional gibbs samplers asymmetric updates become easily local modes method uses symmetric moves allows joint latent sequences improves mixing illustrate application approach simulated real data example 
optimizing energy production using policy search predictive state representations consider challenging practical problem optimizing power production complex power involves control continuous action variables uncertainty amount variety constraints need satisfied propose policy search based approach coupled predictive modelling address problem approach key advantages compared alternatives dynamic programming policy representation search algorithm conveniently incorporate domain knowledge resulting policies easy algorithm naturally parallelizable algorithm obtains policy outperforms solution dynamic programming quantitatively qualitatively 
scaling importance sampling markov logic networks markov logic networks mlns weighted order logic templates generating large ground markov networks lifted inference algorithms bring power logical inference probabilistic inference algorithms operate possible compact order level grounding mln only necessary result lifted inference algorithms scalable algorithms operate directly larger ground network unfortunately existing lifted inference algorithms suffer problems scalability practice real world mlns having complex structure unable exploit symmetries grounding atoms grounding problem second suffer evidence problem arises because evidence symmetries diminishing power lifted inference paper address problems presenting scalable lifted importance sampling based approach grounds full mln specifically show scale main steps importance sampling sampling proposal distribution weight computation scalable sampling achieved using easy sample proposal distribution derived compressed mln representation fast weight computation achieved only small subset sampled groundings formula its possible groundings show algorithm yields asymptotically unbiased estimate experiments several mlns demonstrate promise approach 
optimal neural codes control estimation agents natural world aim selecting appropriate actions based noisy partial sensory observations many behaviors leading decision making action selection closed loop setting naturally control theoretic framework framework optimal control theory usually given cost function selecting control law based observations standard control settings sensors assumed fixed biological systems gain extra flexibility optimizing sensors themselves however sensory adaptation towards control perception assumed work show sensory adaptation control differs sensory adaptation perception simple control implies consistently experimental results studying sensory adaptation essential account task being performed 
graph clustering missing data convex algorithms analysis consider problem finding clusters graph graph partially observed analyze programs works dense graphs works sparse dense graphs requires priori knowledge total cluster size based convex optimization approach low rank matrix recovery using nuclear norm minimization commonly used stochastic block model obtain explicit bounds parameters problem size sparsity clusters amount observed data regularization parameter characterize success failure programs corroborate theoretical findings extensive simulations run algorithm real data set obtained crowdsourcing image classification task amazon mechanical turk observe significant performance improvement traditional methods means 
scale adaptive blind presence noise small scale structures usually leads large kernel estimation errors blind image empirically total failure present scale space perspective blind algorithms introduce scale space formulation blind formulation suggests natural approach robust noise small scale structures estimation multiple scales contributions different scales automatically learning data proposed formulation allows handle non uniform blur straightforward extension experiments benchmark dataset real world images validate effectiveness proposed method surprising finding based approach blur kernel estimation necessarily best scale 
weighted importance sampling policy learning linear function approximation importance sampling essential component policy model reinforcement learning algorithms however its effective variant weighted importance sampling does carry easily function approximation because utilized existing policy learning algorithms paper steps bridging gap show weighted importance sampling viewed special case error individual training samples theoretical empirical benefits similar weighted importance sampling second show benefits extend weighted importance sampling version policy lambda show empirically lambda algorithm result reliable convergence conventional policy lambda 
information based learning agents unbounded state spaces idea animals information driven planning explore unknown environment build internal model proposed time work demonstrated agents using principle efficiently learn models probabilistic environments discrete bounded state spaces however animals commonly unbounded environments address challenging situation study information based learning strategies agents unbounded state spaces using non parametric bayesian models specifically demonstrate chinese restaurant process model able solve problem empirical bayes version able efficiently explore bounded unbounded worlds relying little prior information 
exponential concentration density functional estimator plug estimator large class integral continuous probability densities class includes important families entropy divergence mutual information conditional versions densities dimensional unit cube lie beta smoothness class prove estimator converges rate beta furthermore prove estimator exponential concentration inequality its mean whereas previous related results bounded only expected error estimators finally demonstrate bounds case conditional renyi mutual information 
scalable kernel methods doubly stochastic gradients general perception kernel methods scalable neural nets become choice large scale nonlinear learning problems hard kernel paper propose approach scales kernel methods using novel concept called stochastic functional gradients based fact many kernel methods expressed convex optimization problems approach solves optimization problems making unbiased stochastic approximations functional gradient using random training points another using random features associated kernel performing descent steps noisy functional gradient algorithm simple need commit number random features allows flexibility function class grow see data streaming setting demonstrate function learned procedure iterations converges optimal function reproducing kernel hilbert space rate achieves generalization bound approach readily scale kernel methods regimes dominated neural nets show competitive performances approach compared neural nets datasets energy handwritten digits mnist imagenet using convolution features 
fast training pose detectors fourier domain many datasets samples related known image transformation non rigid deformation applies datasets objects different viewpoints datasets augmented samples datasets high degree redundancy because induced transformations preserve intrinsic properties objects ensembles classifiers used pose estimation share many characteristics since related geometric transformation assuming transformation norm preserving cyclic propose closed form solution fourier domain eliminate leverage shelf solvers train several pose classifiers simultaneously extra cost experiments show training sliding window object detector pose estimator orders magnitude transformations diverse motion plane rotations 
accelerated proximal coordinate gradient method develop accelerated randomized proximal coordinate gradient method solving broad class composite convex optimization problems particular method achieves faster linear convergence rates minimizing strongly convex functions existing randomized proximal coordinate gradient methods show apply method solve dual regularized empirical risk minimization erm problem devise efficient implementations avoid full dimensional vector operations ill conditioned erm problems method obtains improved convergence rates state art stochastic dual coordinate ascent sdca method 
communication efficient distributed dual coordinate ascent communication remains significant bottleneck performance distributed optimization algorithms large scale machine learning paper propose communication efficient framework uses local computation primal dual setting dramatically reduce amount necessary communication provide strong convergence rate analysis class algorithms experiments real world distributed datasets implementations experiments compared state art mini batch versions sgd sdca algorithms converges accurate solution quality average quickly 
simple map inference low rank relaxations focus problem maximum posteriori map inference markov random fields binary variables pairwise interactions common inference tasks consider low rank relaxations interpolate discrete problem its full rank semidefinite relaxation followed randomized rounding develop theoretical bounds studying effect rank showing rank grows relaxed objective increases fraction objective value rounded discrete solution decreases practice show algorithms optimizing low rank objectives simple implement enjoy underlying theory outperform existing approaches benchmark map inference tasks 
sampling problem drawing samples discrete distribution converted discrete optimization problem work show sampling continuous distribution converted optimization problem continuous space central method stochastic process recently described mathematical statistics call process present construction process sampling practical generic sampling algorithm searches maximum process using search analyze correctness convergence time sampling demonstrate empirically makes efficient bound likelihood evaluations closely related adaptive sampling based algorithms 
bayesian model identifying states neural population activity neural population activity cortical circuits solely driven external inputs endogenous states multiple time scales understand information processing cortical circuits need understand statistical structure internal states interaction sensory inputs present statistical model extracting neural population states multi channel recordings neural spiking activity population states modelled using hidden markov decision tree state dependent tuning parameters linear observation model present variational bayesian inference algorithm estimating posterior distribution parameters neural population recordings simulated data show identify underlying sequence population states reconstruct ground truth parameters using population recordings visual cortex model levels population states outperforms state state linear model finally modelling state dependence improves accuracy sensory stimuli population response 
sequence sequence learning neural networks deep neural networks powerful models achieved excellent performance difficult learning tasks work whenever large labeled training sets available used map sequences sequences paper present general approach sequence learning makes minimal assumptions sequence structure method uses long short term memory map input sequence vector fixed dimensionality another deep decode target sequence vector main result english translation task dataset translations produced achieve score entire set score penalized vocabulary words additionally difficulty long sentences comparison phrase based system achieves score dataset used hypotheses produced system its score increases close previous state art learned phrase sentence representations sensitive word order relatively invariant active passive finally order words source sentences target sentences improved performance because doing introduced many short term dependencies source target sentence optimization problem easier 
improved distributed principal component analysis study distributed computing setting multiple set points wish compute functions union point sets key task setting principal component analysis pca compute low dimensional subspace capturing variance union point sets possible given procedure approximate pca approximately solve problems means clustering low rank approximation essential properties approximate distributed pca algorithm its communication cost computational efficiency given desired accuracy applications give algorithms analyses distributed pca lead improved communication computational costs means clustering related problems empirical study real world data shows speedup orders magnitude preserving communication only negligible degradation solution quality techniques develop input sparsity subspace embeddings high correctness probability dimension sparsity independent error probability may independent interest 
sparse polynomial learning graph sketching polynomial non zero real coefficients give algorithm exactly reconstructing given random examples uniform distribution n$ runs time polynomial succeeds function satisfies unique sign property output value corresponds unique set values parities sufficient condition satisfied every coefficient perturbed small random noise satisfied high probability functions chosen randomly coefficients positive learning sparse polynomials boolean domain time polynomial considered notoriously hard worst case result shows problem tractable sparse polynomials show application result sketching problem learning sparse number size uniformly drawn random provide experimental results real world dataset 
tight continuous relaxation balanced cut problem spectral clustering relaxation normalized ratio cut become standard graph based clustering methods existing methods computation multiple clusters corresponding balanced cut graph based greedy techniques heuristics weak connection original minimizing normalized cut paper propose tight continuous relaxation any balanced cut problem show related recently proposed relaxation cases loose leading poor performance practice optimization tight continuous relaxation propose algorithm hard sum ratios minimization problem achieves descent extensive comparisons show method beats existing approaches ratio cut balanced cut criteria 
efficient online random ensembles randomized decision trees usually referred random widely used classification regression tasks machine learning statistics random achieve competitive predictive performance computationally efficient train making excellent candidates real world prediction tasks popular random forest variants random forest extremely randomized trees operate training data online methods greater demand existing online random however require training data batch achieve comparable predictive performance work processes construct ensembles random decision trees call grown incremental online fashion distribution online batch achieve competitive predictive performance comparable existing online random trained batch random being order magnitude faster representing better computation vs accuracy tradeoff 
expectation maximization learning determinantal point processes determinantal point process dpp probabilistic model set diversity compactly parameterized positive semi definite kernel matrix fit dpp given task learn entries its kernel matrix maximizing log likelihood available data however log likelihood non convex entries kernel matrix learning problem hard previous work focused restricted convex learning settings learning only single weight row kernel matrix learning weights linear combination fixed kernel matrices work propose novel algorithm learning full kernel matrix changing kernel matrix entries eigenvalues eigenvectors lower bounding likelihood manner expectation maximization algorithms obtain effective optimization procedure method real world product recommendation task achieve relative gains log likelihood compared naive approach maximizing likelihood projected gradient ascent entries kernel matrix 
capturing semantically meaningful word dependencies admixture poisson mrfs develop fast algorithm admixture poisson mrfs topic model propose novel metric directly evaluate model topic model recently introduced topic model allows word dependencies topic unlike previous topic models lda assume independence words topic research semantic coherence topic models measures model provide strong support explicitly modeling word dependencies semantically meaningful essential appropriately modeling real data shows significant promise providing better topic model high computational complexity because parameters estimated number words only provide results datasets light develop parallel alternating newton algorithm training model handle important step towards scaling large datasets addition only provided results utility motivated simple previous evaluations topic models propose novel evaluation metric based human scores word pairs word brings mind another word provide quantitative qualitative results corpus demonstrate superiority previous topic models identifying semantically meaningful word dependencies code available software 
streaming memory limited algorithms community detection paper consider sparse networks consisting finite number non overlapping communities disjoint clusters higher density clusters clusters intra inter cluster edge densities size graph grows large making cluster reconstruction problem hence difficult solve interested scenarios network size very large adjacency matrix graph hard store data stream model columns adjacency matrix revealed sequentially natural framework setting model develop novel clustering algorithms extract clusters asymptotically accurately algorithm offline needs store keep assignments nodes clusters requires memory scales linearly network size second algorithm online may classify node corresponding column revealed information algorithm requires memory growing sub linearly network size construct efficient streaming memory limited clustering algorithms address problem clustering partial information only small proportion columns adjacency matrix observed develop setting spectral algorithm independent interest 
content based recommendations poisson factorization develop collaborative topic poisson factorization generative model preferences used build recommender systems learning content recommend personalized interest detail models behavior texts poisson distributions connecting latent topics represent texts latent preferences represent provides better recommendations competing methods gives interpretable latent space understanding patterns exploit stochastic variational inference model massive real world datasets example fit full arxiv usage dataset contains ratings word counts day demonstrate empirically model outperforms several baselines including previous state art approach 
statistical decision theoretic framework social choice paper statistical decision theoretic viewpoint social choice focus decision system agents framework given statistical ranking model decision space loss function defined parameter decision pairs formulate social choice mechanisms decision rules minimize expected loss suggests general framework design analysis social choice mechanisms compare bayesian estimators minimize bayesian expected loss mallows model model respectively rule consider various normative properties addition computational complexity asymptotic behavior particular show bayesian estimator model satisfies desired properties computed polynomial time asymptotically different rules data generated model ground truth parameter 
compressive sensing signals gmm sparse precision matrices paper compressive sensing signals drawn gaussian mixture model gmm sparse precision matrices previous work shown signal drawn given gmm perfectly reconstructed noise measurements dominant rank covariance matrix ii sparse gaussian graphical model efficiently estimated fully observed training signals using graphical lasso paper addresses problem challenging ii assuming gmm unknown signal only partially observed incomplete linear measurements challenging assumptions develop hierarchical bayesian method simultaneously estimate gmm recover signals using solely incomplete measurements bayesian shrinkage prior promotes sparsity gaussian precision matrices addition provide theoretical performance bounds relate reconstruction error number signals measurements available sparsity level precision matrices measurements proposed method demonstrated extensively compressive sensing video results simulated hardware acquired real measurements show significant performance improvement state art methods 
bayesian sampling using stochastic gradient dynamics based sampling methods hybrid monte carlo hmc langevin dynamics commonly used sample target distributions recently approaches combined stochastic gradient techniques increase sampling efficiency dealing large datasets problem approach stochastic gradient introduces unknown amount noise prevent proper sampling discretization problem show leverage small number additional variables order stabilize fluctuations induced unknown noise method inspired idea statistical physics general theory 
sparse gaussian chain graph models paper address problem learning structure gaussian chain graph models high dimensional space chain graph models generalizations undirected directed graphical models contain mixed set directed undirected edges problem sparse structure learning studied extensively gaussian graphical models recently conditional gaussian graphical models little previous work structure recovery gaussian chain graph models consider linear regression models linear regression models using building blocks chain graph models argue goal recover model structures many advantages using chain component models linear regression models including convexity optimization problem computational efficiency recovery structured sparsity ability leverage model structure semi supervised learning demonstrate approach simulated datasets 
regularization propose general framework regularization based group framework group defined parameter space fixed control complexity model parameters lie convex hull common regularizers recovered particular cases connection revealed norm group derive properties group satisfy being amenable optimization conditional projected gradient algorithms finally suggest strategy exploration presenting simulation results symmetric groups 
efficient minimax strategies square loss games consider online prediction problems loss prediction outcome measured squared euclidean distance its generalization squared mahalanobis distance derive minimax solutions case prediction action spaces setup sometimes called game ball setup related gaussian density estimation show cases value sub game quadratic function simple statistic state coefficients efficiently computed using explicit relation resulting deterministic minimax strategy randomized strategy linear functions statistic 
residual bootstrap high dimensional regression near low rank designs study residual bootstrap method context high dimensional linear regression specifically analyze distributional approximation linear ridge regression estimator regression coefficients estimated squares classical results show consistently approximates laws provided n$ design matrix size p$ relatively little work considered additional structure linear model may extend validity setting setting propose version obtained ridge regression main structural assumption design matrix nearly low rank sense its singular values decay according power law extra technical assumptions derive simple criterion ensuring consistently approximates law given contrast result study confidence intervals mean response values row design precisely show conditionally gaussian design near low rank structure simultaneously approximates laws n$ result notable sparsity assumptions furthermore since consistency results formulated terms mallows metric existence distribution required 
large margin convex polytope machine present convex polytope machine novel non linear learning algorithm large scale binary classification tasks finds large margin convex polytope separator class develop stochastic gradient descent based algorithm amenable massive datasets heuristic procedure avoid sub optimal local minima experimental evaluations large scale datasets distinct domains mnist handwritten digit recognition topic security demonstrate trains models faster sometimes several orders magnitude state art similar approaches kernel svm methods achieving comparable better classification performance empirical results suggest unlike prior similar approaches need control number sub classifiers polytope avoid overfitting 
distributed variational inference sparse gaussian process regression latent variable models gaussian processes gps powerful tool probabilistic inference functions applied regression non linear dimensionality reduction offer desirable properties uncertainty estimates robustness fitting principled ways tuning hyper parameters however scalability models big datasets remains active topic research introduce novel variational inference sparse regression latent variable models allows efficient distributed algorithm done exploiting data given inducing points formulate evidence lower bound map reduce setting show inference scales data computational resources preserving balanced distribution nodes demonstrate utility scaling gaussian processes big data show performance improves increasing amounts data regression data latent variable modelling mnist results show gps perform better many common models used big data 
learning distributed representations structured output prediction years distributed representations inputs led performance gains many applications allowing statistical information shared inputs however predicted outputs labels generally structures treated discrete objects outputs discrete units paper present formulation structured prediction represent individual labels structure dense vectors allow semantically similar labels share parameters extend representation larger structures defining using tensor products give natural generalization standard structured prediction approaches define learning objective jointly learning model parameters label vectors propose alternating minimization algorithm learning show formulation outperforms structural svm baselines tasks multiclass document classification part speech 
convex deep learning normalized kernels deep learning long standing pursuit machine learning recently unreliable training methods before discovery improved heuristics embedded layer training complementary research strategy develop alternative modeling architectures admit efficient training methods range structures deep models paper develop architecture nonlinearities allows deep compositions trained global optimality approach admits parametric nonparametric forms normalized kernels represent latent layer outcome fully convex formulation able capture compositions nonlinear layers arbitrary depth 
tight convex relaxations sparse matrix factorization based norm propose convex formulation sparse matrix factorization problems number nonzero elements factors assumed fixed known formulation counts sparse pca multiple factors subspace clustering low rank sparse regression potential applications compute slow rates upper bound statistical dimension suggested norm rank matrices showing its statistical dimension order magnitude smaller usual norm trace norm combinations convex formulation theory hard does lead provably polynomial time algorithmic schemes propose active set algorithm leveraging structure convex problem solve show promising numerical results 
learning search bound algorithms bound widely used method combinatorial optimization including mixed integer programming structured prediction map inference work focused developing problem specific techniques little known design node searching strategy bound tree address key challenge learning adaptive node searching order any class problem bound strategies learned learning apply algorithm linear programming based bound solving mixed integer programs compare method fastest open source solvers very efficient commercial solver gurobi demonstrate approach achieves better solutions faster 
integer polynomial programming based framework lifted map inference paper present approach lifted map inference markov logic networks mlns key idea approach compactly encode map inference problem integer polynomial program applying lifted inference steps mln lifted decomposition lifted conditioning partial grounding encoding lifted sense integer assignment variable may represent truth assignment multiple ground atoms mln show solve converting integer linear program solving using state art techniques experiments several benchmark mlns show algorithm substantially superior ground inference existing methods terms computational efficiency solution quality 
conditional random field autoencoders unsupervised structured prediction introduce framework unsupervised learning structured predictors overlapping global features input latent representation predicted conditional observed data using feature rich conditional random field reconstruction input generated conditional latent structure using generative model similarly autoencoder formulation enables efficient exact inference resorting independence assumptions restricting kinds features used illustrate connections traditional autoencoders posterior regularization multi view learning finally show competitive results framework canonical tasks natural language processing part speech word alignment show training model substantially efficient comparable feature rich baselines 
features deep neural many deep neural networks trained natural images exhibit phenomenon common layer learn features similar filters layer features appear specific particular dataset task general applicable many datasets tasks features transition general specific layer network transition studied extensively paper experimentally quantify generality versus neurons layer deep convolutional neural network report surprising results distinct issues higher layer neurons original task expense performance target task expected optimization difficulties related splitting networks adapted neurons expected example network trained imagenet demonstrate issues may depending features transferred bottom middle top network document features decreases distance base task target task increases transferring features tasks better using random features final surprising result initializing network transferred features any number layers produce boost generalization fine tuning target dataset 
accelerated mini batch randomized block coordinate descent method consider regularized empirical risk minimization problems particular minimize sum smooth empirical risk function nonsmooth regularization function regularization function block separable solve minimization problems randomized block coordinate descent manner existing methods usually decrease objective value exploiting partial gradient randomly selected block iteration need data partial gradient block gradient exactly obtained however setting may computationally expensive practice paper propose mini batch randomized block coordinate descent method estimates partial gradient selected block based mini batch randomly sampled data iteration accelerate method exploiting semi stochastic optimization scheme effectively reduces variance partial gradient estimators theoretically show strongly convex functions method attains lower overall iteration complexity existing methods application method solve regularized sparse learning problems numerical experiments shows method naturally exploits sparsity structure achieves better computational performance existing methods 
deep learning real time game play using offline monte carlo tree search planning combination modern reinforcement learning deep learning approaches holds promise making significant progress challenging applications requiring rich perception policy selection learning environment provides set games represent useful benchmark set applications combining model reinforcement learning deep learning called achieves best real time agents far planning based approaches achieve far higher scores best model approaches exploit information available human players orders magnitude slower needed real time play main goal work build better real time game agent central idea slow planning based agents provide training data deep learning architecture capable real time play proposed agents based idea show outperform 
latent source model online collaborative filtering despite prevalence collaborative filtering recommendation systems little theoretical development why works especially setting items users time address theoretical gap introducing model online recommendation systems cast item recommendation model learning problem analyze performance cosine similarity collaborative filtering method model users items assume types users users given type share common string probabilities determining chance item time step recommend item user key distinction related bandit literature once user item item user goal maximize number items users time main result establishes nearly initial learning time steps simple collaborative filtering algorithm achieves essentially optimal performance knowing algorithm exploitation step uses cosine similarity types exploration steps explore space items standard literature explore similarity users novel work 
distributed bayesian posterior sampling moment sharing propose distributed markov chain monte carlo mcmc inference algorithm large scale bayesian posterior simulation assume dataset partitioned stored nodes cluster procedure involves independent mcmc posterior sampler node based its local partition data moment statistics local posteriors collected sampler propagated cluster using expectation propagation message passing low communication costs moment sharing scheme improves posterior estimation quality enforcing agreement samplers demonstrate speed inference quality method empirical studies bayesian logistic regression sparse linear regression spike slab prior 
learning pseudo ensembles formalize notion pseudo ensemble possibly infinite collection models model according noise process dropout deep neural network trains pseudo ensemble generated randomly nodes network examine relationship pseudo ensembles involve perturbation model space standard ensemble methods existing robustness focus perturbation observation space present novel regularizer based making behavior pseudo ensemble robust respect noise process generating fully supervised setting regularizer matches performance dropout unlike dropout regularizer naturally extends semi supervised setting produces state art results provide case study transform recursive neural tensor network pseudo ensemble significantly improves its performance real world sentiment analysis benchmark 
learning time varying coverage functions coverage functions important class discrete functions capture laws diminishing returns paper propose problem learning time varying coverage functions arise naturally applications social network analysis machine learning algorithmic game theory develop novel parametrization time varying coverage function illustrating connections counting processes present efficient algorithm learn parameters maximum likelihood estimation provide rigorous theoretic analysis its sample complexity empirical experiments information diffusion social network analysis demonstrate assumptions underlying diffusion process method performs significantly better existing approaches synthetic real world data 
relax minimax optimal sparse pca polynomial time provide statistical computational analysis sparse principal component analysis pca high dimensions sparse pca problem highly nonconvex nature consequently its global solution attains optimal statistical rate convergence solution computationally intractable obtain meanwhile its convex relaxations tractable compute yield estimators statistical rates convergence hand existing nonconvex optimization procedures greedy methods lack statistical guarantees paper propose stage sparse pca procedure attains optimal principal subspace estimator polynomial time main stage employs novel algorithm named sparse orthogonal iteration pursuit iteratively solves underlying nonconvex problem however analysis shows algorithm only desired computational statistical guarantees restricted region namely attraction obtain desired initial estimator region solve convex formulation sparse pca early stopping integrated analytic framework simultaneously characterize computational statistical performance stage procedure computationally procedure converges rate initialization stage geometric rate main stage statistically final principal subspace estimator achieves minimax optimal statistical rate convergence respect sparsity level *$ dimension sample size procedure motivates general paradigm nonconvex statistical learning problems provable statistical guarantees 
discriminative metric learning neighborhood formulate problem metric learning nearest neighbor classification large margin structured prediction problem latent variable representing choice neighbors task loss directly corresponding classification error describe efficient algorithm exact loss augmented inference fast gradient descent algorithm learning model objective drives metric establish neighborhood boundaries benefit true class labels training points approach boundaries provide advantage certain direct its handling optimizing classification accuracy previously proposed experiments variety data sets method shown achieve excellent results compared current state art metric learning 
finding sparse vector subspace linear sparsity using alternating directions consider problem recovering vector subspace n$ problem considered homogeneous variant sparse recovery problem finds applications sparse dictionary learning sparse pca problems signal processing machine learning simple convex heuristics problem provably break fraction nonzero entries target sparse vector substantially exceeds contrast exhibit relatively simple nonconvex approach based alternating directions provably succeeds fraction nonzero entries knowledge practical algorithm achieve linear scaling result assumes planted sparse model target sparse vector embedded otherwise random subspace empirically proposed algorithm succeeds challenging data models arising sparse dictionary learning 
asynchronous sequential monte carlo introduce sequential monte carlo algorithm call particle cascade particle cascade asynchronous alternative traditional sequential monte carlo algorithms amenable parallel distributed implementations uses barrier leads improved particle memory efficiency algorithm sense run unbounded number particles keeping fixed memory budget prove particle cascade provides unbiased marginal likelihood estimator existing pseudo marginal methods 
discrete graph hashing hashing emerged popular technique fast nearest neighbor search databases particular learning based hashing received considerable attention due its appealing storage search efficiency however performance unsupervised learning based hashing methods rapidly code length increases argue performance due optimization procedures used achieve discrete binary codes paper presents graph based unsupervised hashing model preserve neighborhood structure massive data discrete code space cast graph hashing problem discrete optimization framework directly learns binary codes tractable alternating maximization algorithm proposed explicitly deal discrete constraints yielding high quality codes capture local neighborhoods extensive experiments performed large datasets samples show discrete optimization based graph hashing method obtains superior search accuracy state art unsupervised hashing methods especially codes 
feedback detection predictors predictor deployed production system may features uses predictions feedback loop occur example model predicts certain type behavior causing behavior predicts self paper analyze predictor feedback detection causal inference problem introduce local randomization scheme used detect non linear feedback real world problems conduct study proposed methodology using predictive system currently deployed part search engine 
rates convergence nearest neighbor classification analyze behavior nearest neighbor classification metric spaces provide finite sample distribution dependent rates convergence minimal assumptions general existing bounds enable product establish universal consistency nearest neighbor broader range data spaces previously known illustrate upper lower bounds introducing smoothness class nearest neighbor classification instance margin condition convergence rate nearest neighbor matches recently established lower bounds nonparametric classification 
consistency weighted majority votes revisit statistical learning perspective classical decision theoretic problem weighted expert voting particular examine consistency asymptotic optimal weighted majority related rules case known expert levels give sharp error estimates optimal rule levels unknown empirically estimated provide bayesian analyses situation proof techniques non standard may independent interest bounds derive nearly optimal several challenging open problems posed experimental results provided illustrate theory 
beta negative binomial process exchangeable partitions mixed membership modeling beta negative binomial process integer valued stochastic process employed partition count vector latent random count matrix marginal probability distribution exchangeable random partitions data developed current inference number atoms beta process paper introduces exchangeable partition probability function explicitly describe clusters data points group random number exchangeable partitions shared groups fully collapsed gibbs sampler developed leading novel nonparametric bayesian topic model distinct existing ones simple implementation fast convergence good mixing state art predictive performance 
zero recognition unreliable attributes principle zero learning makes possible train object recognition model simply specifying category attributes example classifiers generic attributes construct classifier category properties providing training images practice however standard zero paradigm suffers because attribute predictions novel images hard right propose novel random forest approach train zero models explicitly accounts attribute predictions leveraging statistics error method obtains robust discriminative models unseen classes devise extensions handle scenario unreliable attribute datasets demonstrate benefit visual category learning zero training examples critical domain rare categories categories defined fly 
concavity reweighted approximation analyze reweighted version approximation estimating log partition function product distribution defined region graph establish sufficient conditions concavity reweighted objective function terms weight assignments expansion show reweighted version sum product algorithm applied region graph produce global optima approximation whenever algorithm converges region graph layers corresponding bethe approximation show sufficient conditions concavity necessary finally provide explicit polytope concavity terms structure region graph conclude simulations demonstrate advantages reweighted approach 
online decision making general combinatorial spaces study online combinatorial decision problems sequential decisions combinatorial space knowing cost decisions trial goal minimize total regret sequence trials relative best fixed decision hindsight problems studied settings decisions represented boolean vectors costs linear representation study general setting costs may linear any suitable low dimensional vector representation elements decision space give general algorithm problems call low dimensional online mirror descent algorithm generalizes component hedge algorithm algorithm study offers generalization previous work emphasizes role convex polytope arising vector representation decision space boolean representations lead general vector representations lead general study several examples types finally demonstrate benefit having general framework problems application online transportation problem associated transportation generalize birkhoff polytope doubly stochastic matrices resulting algorithm generalizes algorithm 
fast multivariate spatio temporal analysis low rank tensor learning accurate efficient analysis multivariate spatio temporal data critical geology applications existing models usually assume simple inter dependence variables space time computationally expensive propose unified low rank tensor learning framework multivariate spatio temporal analysis conveniently incorporate different properties spatio temporal data spatial clustering shared structure variables demonstrate general framework applied forecasting tasks develop efficient greedy algorithm solve resulting optimization problem convergence guarantee conduct experiments synthetic datasets real application datasets demonstrate method only significantly faster existing methods achieves lower estimation error 
clustered factor analysis spike data high dimensional simultaneous recordings neural spiking activity explored analyzed latent variable factor models models however ill extract structure shared distributed aspects firing activity multiple cells extend factor models proposing model discovers groups cells pool recorded neurons model combines aspects mixture factor models capturing clustering structure aspects latent dynamical system models capturing temporal dependencies resulting model infer latent factors data using variational inference model parameters estimated expectation maximization em address crucial problem initializing parameters em extending sparse subspace clustering algorithm integer valued spike count observations illustrate merits proposed model applying calcium imaging data neurons show meaningful clustering structure data 
algorithms optimization mdps many sequential decision making problems may want risk minimizing measure variability costs addition minimizing standard criterion conditional value risk relatively risk measure addresses shortcomings known variance related risk measures because its computational gained popularity finance operations research paper consider mean optimization problem mdps derive formula computing gradient risk sensitive objective function devise policy gradient algorithms uses specific method estimate gradient updates policy parameters descent direction establish convergence algorithms locally risk sensitive optimal policies finally demonstrate usefulness algorithms optimal stopping problem 
variations natural images deep gaussian mixture models generative models seen machine learning many problems written terms distribution data including prediction reconstruction imputation simulation promising directions unsupervised learning may lie deep learning methods given success supervised learning however current problems deep unsupervised learning methods harder scale result easier scalable shallow methods gaussian mixture model student mixture model remain surprisingly competitive paper propose scalable deep generative model images called deep gaussian mixture model straightforward powerful generalization multiple layers parametrization deep gmm allows efficiently capture products variations natural images propose em based algorithm scales large datasets show expectation maximization steps easily distributed multiple machines density estimation experiments show deeper gmm architectures generalize better shallow ones results state art 
partition wise linear models region specific linear models widely used practical applications because non linear highly interpretable model representations key challenges non convexity simultaneous optimization regions region specific models paper proposes novel convex region specific linear models refer partition wise linear models key ideas linear models regions partitions region representing region specific linear models linear combinations partition specific models optimizing regions partition selection large number given partition candidates means convex structured regularizations addition providing initialization globally optimal solutions convex formulation makes possible derive generalization bound advanced optimization techniques proximal methods decomposition proximal maps sparsity inducing regularizations experimental results demonstrate partition wise linear models perform better competitive state art region specific locally linear models 
large scale detection adaptation major challenge scaling object detection difficulty obtaining labeled images large numbers categories recently deep convolutional neural networks cnns emerged clear object classification benchmarks part due training labeled classification images unfortunately only small fraction labels available detection task cheaper easier large quantities image level labels search detection data label precise bounding paper propose large scale detection adaptation algorithm learns difference tasks knowledge classifiers categories bounding box annotated data detectors method potential enable detection tens thousands categories lack bounding box annotations classification data evaluation imagenet detection challenge demonstrates efficacy approach algorithm enables produce detector using available classification data nodes imagenet tree additionally demonstrate modify architecture produce fast detector running detector models software available 
deep networks internal selective attention feedback connections traditional convolutional neural networks cnn stationary feedforward change parameters during evaluation feedback higher lower layers real however does deep attention selective network architecture feedback structure dynamically its convolutional filter during classification harnesses power sequential processing improve classification performance allowing network iteratively focus its internal attention its convolutional filters feedback trained direct policy search huge dimensional parameter space scalable natural evolution strategies cifar cifar datasets outperforms previous state art model datasets 
parallel feature selection inspired group testing paper presents parallel feature selection method classification scales very high dimensions large data sizes original method inspired group testing theory feature selection procedure consists collection randomized tests performed parallel corresponds subset features scoring function may applied measure relevance features classification task develop general theory providing sufficient conditions true features guaranteed correctly identified superior performance method demonstrated challenging relation extraction task very large data set redundant features sample size order millions present comparisons state art feature selection methods range data sets method exhibits competitive performance terms running time accuracy moreover yields substantial speedup used pre processing step existing methods 
low rank time frequency synthesis many single channel signal decomposition techniques rely low rank factorization time frequency transform particular nonnegative matrix factorization nmf power magnitude short time fourier transform considered many audio applications setting nmf divergence shown generative gaussian composite model step forward empirical approaches based hoc transform divergence generative model raw signal only its work presented paper ultimate gap proposing novel signal synthesis model low rank time frequency structure particular approach multi resolution representations possible traditional nmf setting describe expectation maximization algorithms estimation model report audio signal processing results music decomposition speech 
pre training recurrent neural networks linear autoencoders propose pre training technique recurrent neural networks based linear autoencoder networks sequences linear dynamical systems modelling target sequences closed form solution definition optimal weights linear autoencoder given training set sequences solution however computationally very demanding suggest procedure approximate solution given number hidden units weights obtained linear autoencoder used initial weights input hidden connections recurrent neural network trained desired task using known datasets sequences music show proposed pre training approach highly effective since allows largely improve state art results considered datasets 
semi supervised learning deep generative models increasing size modern data sets combined difficulty obtaining label information semi supervised learning problems significant practical importance modern data analysis revisit approach semi supervised learning generative models develop models allow effective generalisation small labelled data sets large ones generative approaches far inefficient non scalable show deep generative models approximate bayesian inference exploiting advances variational methods used provide significant improvements making generative approaches highly competitive semi supervised learning 
signal aggregate constraints additive factorial hmms application energy blind source separation problems difficult because inherently entire goal identify meaningful sources introduce way incorporating domain knowledge problem called signal aggregate constraints encourage total signal unknown sources close specified value based observation total signal varies widely unknown sources good idea total values expect incorporate additive factorial hidden markov model formulate energy problems only mixture signal assumed observed convex quadratic program approximate inference employed recovering source signals real world energy data set show dramatically improves original significantly improves state art approach 
stochastic variational inference hidden markov models variational inference algorithms proven successful bayesian analysis large data settings advances using stochastic variational inference svi however methods largely studied independent exchangeable data settings develop svi algorithm learn parameters hidden markov models hmms time dependent data setting challenge applying stochastic optimization setting arises dependencies chain consider observations propose algorithm harnesses memory decay chain adaptively bound errors arising edge effects demonstrate effectiveness algorithm synthetic experiments large dataset batch algorithm computationally infeasible 
bootstrap kernel tests bootstrap method nonparametric hypothesis tests based kernel distribution embeddings proposed bootstrap method used construct provably consistent tests apply random processes naive permutation based bootstrap fails applies large group kernel tests based statistics hypothesis non illustrate approach construct sample independence multiple lag independence time series experiments bootstrap gives strong performance synthetic examples audio data performance gibbs sampler code available 
using message passing analog clustering bipartite graph methods infer local search strategies cluster time common technique update row based current column vice versa propose algorithm maximizes global objective function using message passing objective function closely approximates general likelihood function separating cluster size term row column count penalties because global optimization framework approach important features practice moreover expectation maximization used learn model parameters unknown simulations method outperforms best existing algorithms planted clusters overlap applied gene expression datasets method finds gene clusters high quality terms cluster size density 
fast kernel learning pattern ability automatically discover patterns perform essential quality intelligent systems kernel methods gaussian processes great potential pattern since kernel controls generalisation properties methods however automatically large scale patterns general difficult developing gaussian process models purpose involves several challenges vast majority kernels kernel learning methods currently only succeed smoothing interpolation difficulty fact gaussian processes typically only tractable small datasets scaling expressive kernel learning approach poses different challenges scaling standard gaussian process model faces additional computational constraints need retain significant model structure rich information available large dataset paper propose gaussian process approach large scale pattern recover sophisticated class kernels perform video long range forecasting large datasets including problem training points proposed method significantly outperforms alternative scalable flexible gaussian process methods speed accuracy moreover show distinct combination expressive kernels fully non parametric representation scalable inference exploits existing model structure critical large scale pattern 
learning graphs using representation statistically consistent existing research suggests embedding graphs unit sphere beneficial learning labels vertices graph however choice optimal embedding remains open issue representation graphs class embeddings unit sphere introduced paper show exists representations statistically consistent large class graphs including power law random graphs result achieved extending notion consistency designed setting graph part analysis explicitly derive relationships rademacher complexity measure structural properties graphs number show fraction vertices graph nodes need labelled learning algorithm consistent known labelled sample complexity function graph time relates labelled sample complexity graph connectivity properties density graphs multiview setting whenever individual views expressed graph known heuristic convex combination tend improve accuracy analysis presented easily extends multiple graph helps develop sound statistical understanding heuristic previously 
spectral support norm regularization support norm successfully applied sparse vector prediction problems observe wider class norms call box norms framework derive efficient algorithm compute operator squared norm improving original method support norm extend norms vector matrix setting introduce spectral support norm study its properties show closely related multitask learning cluster norm apply norms real synthetic matrix completion datasets findings indicate spectral support norm regularization gives state art performance consistently improving trace norm regularization matrix elastic 
unsupervised learning efficient short term memory network learning recurrent neural networks topic difficulties problems report substantial progress unsupervised learning recurrent networks keep track input signal specifically show networks learn efficiently represent present past inputs based local learning rules only results based several key insights develop local learning rule recurrent weights main aim drive network regime average feedforward signal inputs recurrent inputs show learning rule minimizes cost function second develop local learning rule feedforward weights based networks recurrent inputs already predict feedforward inputs minimizes cost third show learning rules modified network directly encode non inputs show learning rules applied network time version network output back consequence network starts efficiently represent its signal inputs history develop main theory linear networks learning rules transferred balanced spiking networks 
quantized estimation gaussian sequence models euclidean central result statistical theory theorem characterizes minimax rate normal means model nonparametric estimation paper present extension theorem estimation carried storage communication constraints particular place limits number bits used encode estimator analyze excess risk terms constraint signal size noise level give sharp upper lower bounds case euclidean ball establishes optimal minimax tradeoff storage risk setting 
learning concept hierarchy multi labeled documents topic models discover patterns word usage large corpora difficult unsupervised structure noisy human provided labels especially label space large paper present model label hierarchy induce hierarchy user generated labels topics associated labels set multi labeled documents model robust account missing labels provide interpretable summary otherwise label set show empirically effectiveness predicting held words labels unseen documents 
variational gaussian process state space models state space models successfully used years different areas science engineering present procedure efficient variational bayesian learning nonlinear state space models based sparse gaussian processes result learning tractable posterior nonlinear dynamical systems comparison conventional parametric models offer trade model capacity computational cost avoiding overfitting main algorithm uses hybrid inference approach combining variational bayes sequential monte carlo present stochastic variational inference online learning approaches fast learning long time series 
fast prediction large scale kernel machines kernel machines kernel svm kernel ridge regression usually construct high quality models however real world applications remains limited due high prediction cost paper present novel insights improving prediction efficiency kernel machines show adding classical kernel approximation elegant way significantly reduce prediction error additional prediction cost second provide theoretical analysis bounding error solution computed using kernel approximation method show error related weighted objective function weights given model computed original kernel theoretical insight suggests point selection technique situation knowledge original model based insights provide divide conquer framework improving prediction speed divide problem smaller local reduce problem size second phase develop kernel approximation based fast prediction approach subproblem apply algorithm real world large scale classification regression datasets show proposed algorithm consistently significantly better competitors example classification problem terms prediction time algorithm achieves times speedup full kernel svm fold speedup state art approach obtaining higher prediction accuracy vs 
