randomized dependence coefficient introduce randomized dependence coefficient measure non linear dependence random variables arbitrary dimension based maximum correlation coefficient defined terms correlation random non linear copula projections invariant respect marginal distribution transformations low computational cost easy implement just lines code paper 
documents multiple overlapping windows grids counts analysis documents represented words models count features typically based mixing small number topics sam recently observed many corpora documents another smooth way features dropping ones being introduced counting models spatial word distributions learned way document distribution features modeled sum histograms major drawback method essentially mixture content generated single area may problematic especially lower dimensional grids paper overcome issue counting nature topic models basic counting introduce generative kernel based document usage strategy useful understanding large corpora evaluate approach document classification multimodal retrieval obtaining state art results standard benchmarks 
coupled local estimators implement bayesian information integration experiments demonstrated brain integrates information multiple sensory cues near bayesian optimal manner present study proposes novel mechanism achieve consider connected networks integration direction information dorsal superior temporal ventral areas network serves local estimator receives independent cue visual direct input external stimulus positive interactions improve decoding accuracy individual network bayesian inference cues model successfully explains experimental finding achieve bayesian multisensory integration only receives single cue direct external input result suggests brain may implement optimal information integration local estimator connections cortical regions 
latent maximum margin clustering present maximum margin framework clusters data using latent variables using latent representations enables framework model unobserved information embedded data implement idea large margin learning develop alternating descent algorithm effectively solve non convex optimization problem latent maximum margin clustering framework based video clustering tasks video represented latent model presence absence video experimental results obtained standard datasets show proposed method outperforms non latent maximum margin clustering conventional clustering approaches 
data driven robust polynomial optimization consider robust optimization polynomial optimization problems uncertainty set set candidate probability density functions set ball density function estimated data samples data driven random polynomial optimization problems inherently hard due nonconvex objectives constraints however show employing polynomial histogram density estimates introduce robustness respect distributional uncertainty sets making problem show solution robust problem limit sequence tractable programming relaxations give finite sample consistency guarantees data driven uncertainty sets finally apply model solution method network problem 
transfer learning transductive setting category models objects activities typically rely supervised learning requiring sufficiently large training sets transferring knowledge known categories novel classes only labels however far common scenario work extend transfer learning semi supervised learning exploit unlabeled instances novel categories only labeled instances proposed approach semantic transfer combines main transfer information known novel categories incorporating external knowledge expert specified information mid level layer semantic attributes second exploit manifold structure novel classes specifically adapt graph based learning algorithm far only used semi supervised learning zero shot shot learning third improve local neighborhood graph structures feature based representation mid level object attribute based representation evaluate approach challenging datasets different applications namely animals attributes imagenet image classification activity recognition approach consistently outperforms state art transfer semi supervised approaches datasets 
bayesian optimization explains human active search many real world problems complicated objective functions optimize functions humans utilize sophisticated sequential decision making strategies many optimization algorithms developed purpose compare humans terms performance try general underlying algorithm people may using searching maximum function subjects shown function location task maximum possible subjects win close maximum location analysis non optimizing functions different families shows humans outperform known optimization algorithms bayesian optimization based gaussian processes exploit values values obtained far predicts human performance searched locations better follow controlled experiments subjects optimization tasks confirm gaussian processes provide general unified theoretical account explain passive active function learning search humans 
provable subspace clustering meets sparse subspace clustering low rank representation considered state art methods subspace clustering methods similar convex optimizations exploiting intuition self main difference minimizes vector norm representation matrix induce sparsity minimizes nuclear norm trace norm low rank structure because representation matrix simultaneously sparse low rank propose algorithm termed low rank sparse subspace clustering combining develops theoretical guarantees algorithm results reveal interesting insights strength demonstrate advantages methods preserving self property graph connectivity time 
generalized random utility models multiple types propose model demand estimation multi agent product settings present estimation algorithm uses mcmc techniques classify agents types model extends popular setup allow data driven classification agents types using agent level data focus applications involving data agents ranking alternatives present theoretical conditions establish identifiability model likelihood posterior results real simulated data provide support scalability approach 
polar operators structured sparse estimation structured sparse estimation become important technique many areas data analysis unfortunately estimators create computational difficulties entail sophisticated algorithms contribution uncover rich class structured sparse regularizers polar operator evaluated efficiently operator simple conditional gradient method developed combined smoothing local optimization significantly reduces training time state art demonstrate reduction polar proximal maps enables efficient latent fused lasso 
decomposing proximal map proximal map key step gradient type algorithms become prevalent large scale high dimensional problems simple functions proximal map available closed form complicated functions become highly motivated need combining regularizers simultaneously induce different types structures paper systematic proximal map sum functions proximal maps individual only known results scattered literature discover several decompositions obtained theory 
point based value iteration optimal belief compression pomdps paper presents major results towards solving partially observable markov decision problems algorithm outperforms existing algorithms standard infinite horizon benchmark problems give integer program solves collaborative bayesian games program because its linear relaxation very integral show bounded belief converted pomdp actions exponential number actions correspond strategies present method transform any bounded number parameter using optimal belief compression show combination results opens door classes algorithms based previous pomdp algorithms choose algorithm point based valued iteration modify produce tractable value iteration method outperforms existing algorithms 
pac bayes empirical inequality present pac bayes empirical inequality inequality based combination pac bayesian bounding technique empirical bound allows advantage small empirical variance especially useful regression show empirical variance significantly smaller empirical loss pac bayes empirical inequality significantly tighter pac bayes kl inequality otherwise comparable pac bayes empirical inequality interesting example application pac bayesian bounding technique self bounding functions provide empirical comparison pac bayes empirical inequality pac bayes kl inequality synthetic example several datasets 
modeling clutter perception using parametric object partitioning visual clutter perception image being ranging object detection relatively little effort model important ubiquitous approach models clutter number objects image objects defined similar intensity color gradient features introduce novel parametric method merging modeling mixture distributions similarity distance statistics taking normalized number objects following partitioning estimate clutter perception model using dataset realistic scenes rank human clutter showed method only predicted clutter extremely existing clutter perception models behavioral object segmentation ground truth number objects image clutter perception number objects features 
robust multimodal graph matching sparse coding meets graph matching graph matching challenging problem very important applications wide range fields image video analysis biological biomedical problems propose robust graph matching algorithm inspired sparsity related techniques cast problem resembling group collaborative sparsity formulations non smooth convex optimization problem efficiently solved using augmented lagrangian techniques method deal weighted unweighted graphs multimodal data different graphs represent different types data proposed approach naturally integrated collaborative graph inference techniques solving general network inference problems observed variables possibly different modalities correspondence algorithm tested compared state art graph matching techniques synthetic real graphs present results multimodal graphs applications collaborative inference brain connectivity alignment functional imaging fmri data 
multiple environments limited experiments paper considers problem transferring experimental findings learned multiple heterogeneous domains target environment only limited experiments performed reduce questions multiple domains limited scope symbolic extending full experiments introduced provide different graphical algorithmic conditions computing transport formula setting way observational experimental information scattered different domains synthesize consistent estimate desired effects 
data speeds training time learning sparse vectors increased data years led several possible data computational resource data available sample complexity limit possible extra examples speed computation time required perform learning give positive answer question natural supervised learning problem consider agnostic pac learning sparse vectors n$ class learnable using examples main contribution novel non cryptographic methodology establishing computational statistical allows show widely believed assumption random hard efficiently learning class using examples impossible show stronger hardness assumptions examples suffice hand show algorithm learns class efficiently using examples formally tradeoff sample computational complexity natural supervised learning problem 
causal inference time series using restricted structural equation models causal inference uses observational data infer causal structure data generating system study class restricted structural equation models time series call time series models independent noise models require independent residual time series whereas traditional methods causality exploit variance residuals work contains main contributions theoretical restricting model class additive noise provide general identifiability results existing ones results cover instantaneous effects nonlinear non instantaneous time series practical feedback time series propose algorithm based non linear independence tests time series data insufficient data generating process does satisfy model assumptions algorithm may give partial results mostly avoids answers structural equation model point view allows extend theoretical algorithmic part situations time series measured different time delays may fmri data example outperforms existing methods artificial real data code provided 
deep fisher networks large scale image classification massively parallel computations become broadly available modern deep architectures trained very large datasets popularity trained convolutional neural networks particular recently shown yield state art performance challenging image classification benchmarks imagenet however elements architectures similar standard hand crafted representations used computer vision paper explore proposing version state art fisher vector image encoding stacked multiple layers architecture significantly improves standard fisher vectors obtains competitive results deep convolutional networks significantly smaller computational cost hybrid architecture allows measure performance improvement deeper image classification conventional features encodings 
sparse additive models low rank background sparse additive model modeling involves sum exp computing consuming costs large scales moreover assumption equal background classes topics may strong paper extends propose sparse additive model low rank background sam simple efficient estimation particularly employing double majorization bound approximate log likelihood quadratic lower bound sum exp terms constraints low rank sparsity simply nuclear norm norm regularizers interestingly optimization task manner transformed form robust pca consequently parameters supervised sam efficiently learned using existing algorithm robust pca based accelerated proximal gradient besides supervised case extend sam unsupervised scenarios experiments real world data demonstrate effectiveness efficiency sam showing state art performances 
variance reduction stochastic gradient optimization stochastic gradient optimization class widely used algorithms training machine learning models optimize objective uses noisy gradient computed random data samples true gradient computed entire dataset however variance noisy gradient large algorithm spend time leading convergence worse performance paper develop general approach using control variance reduction stochastic gradient data statistics low order moments pre computed estimated online used form control demonstrate construct control practical problems using stochastic gradient optimization convex map estimation logistic regression non convex stochastic variational inference latent dirichlet allocation problems approach shows faster convergence better performance classical approach 
training deep recurrent neural networks time series temporal hierarchy information spread multiple time scales common recurrent neural networks however explicitly hierarchy research focusing training algorithms basic architecture study effect hierarchy recurrent neural networks processing time series layer recurrent network receives hidden state previous layer input architecture allows perform processing difficult temporal tasks naturally capture structure time series show reach state art performance recurrent networks character level language modelling trained stochastic gradient descent analysis different time scales 
simple example dirichlet process mixture number components data assumed come finite mixture unknown number components become common dirichlet process mixtures only density estimation inferences number components typical approach posterior distribution number components occurring far posterior number clusters observed data however turns posterior consistent does converge true number components note give demonstration perhaps simplest possible setting normal components unit variance applied data mixture standard normal component example exhibits going posterior probability cluster goes 
variational policy search trajectory optimization order learn effective control policies dynamical systems policy search methods able discover successful desired task random exploration work simple domains complex high dimensional tasks present challenge particularly combined high dimensional policies parameter space exploration infeasible present method uses trajectory optimization powerful exploration strategy policy search variational decomposition maximum likelihood policy objective allows standard trajectory optimization algorithms differential dynamic programming standard supervised learning policy demonstrate resulting algorithm outperform prior methods challenging tasks 
scalable kernels graphs continuous attributes graphs continuous node attributes arise many applications state art graph kernels comparing continuous graphs suffer high runtime complexity instance popular shortest path kernel scales number nodes paper present class path kernels computational complexity graph number edges due sparsity small real world graphs kernels scale large graphs experiments presented kernels outperform state art kernels terms speed accuracy classification benchmark datasets 
density estimation unweighted nearest neighbor graphs consider unweighted nearest neighbor graph points sampled unknown density prove estimate density just unweighted matrix graph knowing points themselves distance similarity scores key insights local differences link numbers used estimate local function integrating function shortest paths leads estimate underlying density 
decision compact rich models classification randomized decision trees forests rich history machine learning seen considerable success application perhaps particularly computer vision however face fundamental limitation given data number nodes decision trees grow exponentially depth certain applications example embedded processors memory limited resource exponential growth trees limits depth potential accuracy paper proposes decision idea ensembles decision directed acyclic graphs shows compact powerful discriminative models classification unlike conventional decision trees only allow path every node dag decision allows multiple paths root present compare node merging algorithms jointly optimize features structure efficiently during training node node merging driven minimization exactly objective function weighted sum entropies leaves results varied datasets show compared decision forests several decision require dramatically memory considerably improving generalization 
invariant components image probabilistic generative approach study optimal image encoding based generative approach non linear feature combinations explicit encoding far approaches unsupervised learning learning visual features sparse coding ica account translations representing features different earlier models used separate encoding features facilitate invariant data encoding recognition probabilistic generative models explicit encoding far assumed linear superposition components encode image time apply model non linear feature superposition explicit encoding avoiding linear studied model represents match component ubiquitous natural images order account non linear model qualitatively very different linear models using component representations separated feature parameters investigated encodings learned model using artificial data components model extracts components correctly identify components hidden variables model natural image model learns component masks features typical image components using reverse correlation estimate receptive fields associated model hidden units many receptive fields fields sensitive complex structures results show probabilistic models capture invariances trained efficiently image resulting encoding represents alternative model neural encoding images visual cortex 
actor critic algorithms risk sensitive mdps many sequential decision making problems may want risk minimizing measure variability rewards addition maximizing standard criterion variance related risk measures common risk sensitive criteria finance operations research however optimizing many criteria known hard problem paper consider discounted average reward markov decision processes formulation define measure variability policy turn gives set risk sensitive criteria optimize criteria derive formula computing its gradient devise actor critic algorithms estimating gradient policy parameters ascent direction establish convergence algorithms locally risk sensitive optimal policies finally demonstrate usefulness algorithms traffic signal control application 
summary statistics feature infinite mixture models commonly used clustering sample posterior mixture assignments monte carlo methods its maximum posteriori solution optimization however problems posterior hard interpret sampled paper introduce novel statistics based block sizes representing sample sets feature develop element based definition entropy quantify segmentation elements propose simple algorithm called entropy visualize information experiments various infinite mixture posteriors feature allocation dataset demonstrate proposed statistics useful practice 
shot learning big data model shot learning situation very scalar observations n$ available associated observation very high dimensional vector provides context enables predict subsequent observations given context features analysis problems studied easier dimension large words prediction easier context provided proposed methodology variant principal component regression rigorous analysis sheds light instance show classical estimators may inconsistent specified setting scalar classical estimator expansion phenomenon appears somewhat novel contrasts shrinkage methods far common big data 
variational inference distance metrics gaussian process regression introduce novel variational method allows approximately integrate kernel hyperparameters length scales gaussian process regression approach consists novel variant variational framework recently developed gaussian process latent variable model additionally makes representation gaussian process consider technique learning distance metrics gaussian process regression setting provide experimental evaluations comparisons existing methods considering datasets high dimensional inputs 
correlations back case associative memory retrieval long statistical dependencies neuronal activity need taken account decoding stimuli encoded neural population studied equally need account dependencies synaptic weights decoding patterns previously encoded auto associative memory show activity dependent learning produces correlations account dynamics memory retrieval leads poor recall derive optimal network dynamics recall face synaptic correlations range synaptic plasticity rules dynamics involve studied circuit motifs forms feedback inhibition experimentally observed nonlinearities therefore show addressing problem synaptic correlations leads novel functional account key biophysical features neural 
optimal neural population codes high dimensional stimulus variables does neural population process sensory optimal coding theories assume neural tuning curves adapted prior distribution stimulus variable previous work discussed optimal solutions only dimensional stimulus variables expand ideas present solutions define optimal tuning curves high dimensional stimulus variables consider solutions minimal case number neurons population equal number stimulus dimensions case dimensional stimulus variables analytically derive optimal solutions different optimal criteria minimal reconstruction error maximal mutual information higher dimensional case learning rule improve population code provided 
online variational approximations non exponential family change point models application tracking bayesian online change point detection algorithm provides efficient way exact inference parameters underlying model may change time requires computation underlying model posterior only computed online time memory exponential family models develop variational approximations posterior change point times formulated run efficient inference underlying model exponential family does tractable posterior predictive distributions doing develop improvements online variational inference apply methodology tracking problem using data signal noise feature distributed develop variational method inferring parameters non exponential family distribution 
stochastic gradient descent using predictive variance reduction stochastic gradient descent popular large scale optimization slow convergence asymptotically due inherent variance remedy problem introduce explicit variance reduction method stochastic gradient descent call stochastic variance reduced gradient smooth strongly convex functions prove method fast convergence rate stochastic dual coordinate ascent sdca stochastic average gradient however analysis significantly simpler intuitive moreover unlike sdca method does require storage gradients easily applicable complex problems structured prediction problems neural network learning 
using multiple samples learn mixture models mixture models problem assumed distributions observe sample mixture distributions unknown coefficients goal instances generating distributions identify parameters hidden distributions work assumption access several samples drawn underlying distributions different mixing weights topic modeling having multiple samples reasonable assumption data sample prove possible differences samples better recover underlying structure present algorithms recover underlying structure milder assumptions current state art dimensionality separation high methods applied topic modeling allow generalization words present training data 
learning hidden markov models non sequence data tensor decomposition learning dynamic models observed data central issue many scientific studies engineering tasks usual setting data collected sequentially trajectories dynamical system quite modern scientific modeling tasks however turns reliable sequential data difficult whereas order easier obtain examples include modeling certain biological processes existing methods learning dynamic model non sequence data mostly based expectation maximization involves non convex optimization hard analyze inspired advances spectral learning methods propose study problem different perspective moment matching spectral decomposition framework identify reasonable assumptions generative process non sequence data propose learning algorithms based tensor decomposition method tensor provably recover order markov models hidden markov models best knowledge formal guarantee learning non sequence data simulation results confirm theoretical findings 
model selection consistency penalized estimators geometric theory penalized estimators used diverse areas science engineering fit high dimensional models low dimensional structure penalties geometrically decomposable expressed sum convex support functions generalize notion geometrically decomposable penalties develop general framework establishing consistency model selection consistency estimators penalties framework derive results special cases interest statistical learning 
dropout training adaptive regularization dropout feature schemes control artificially training data generalized linear models dropout performs form adaptive regularization using show dropout regularizer order equivalent regularizer applied scaling features estimate inverse diagonal fisher information matrix establish connection online learner close relative operates repeatedly solving linear dropout regularized problems dropout regularization develop natural semi supervised algorithm uses unlabeled data create better adaptive regularizer apply idea document classification tasks show consistently performance dropout training improving state art results reviews dataset 
subsampling algorithms fast squares regression address problem fast estimation ordinary squares large amounts data p$ propose methods solve big data problem subsampling covariance matrix using single stage estimation run order size input best method gives error bound independent amount subsampling long above threshold provide theoretical bounds algorithms fixed design randomized hadamard preconditioning sub gaussian random design setting compare performance methods synthetic real world datasets show observations sub gaussian directly expensive randomized hadamard preconditioning loss accuracy 
faster regression randomized hadamard transform propose fast algorithm regression number features larger number observations n$ standard way solve regression setting works dual space gives running time algorithm runs time works preconditioning design matrix randomized hadamard transform subsequent subsampling features provide risk bounds algorithm fixed design setting show experimental results synthetic real datasets 
accelerated batch stochastic dual coordinate ascent stochastic dual coordinate ascent sdca effective technique solving regularized loss minimization problems machine learning paper considers extension sdca batch setting used practice main contribution introduce accelerated batch version sdca prove fast convergence rate method discuss implementation method parallel computing system compare results stochastic dual coordinate ascent accelerated deterministic gradient descent method 
improved generalized upper bounds complexity policy iteration given markov decision process mdp states actions state study number iterations needed policy iteration pi algorithms converge optimal discounted optimal policy consider variations pi pi changes actions states positive advantage simplex pi only changes action state maximal advantage show pi iterations improving factor result simplex pi iterations improving factor result structural assumptions mdp consider bounds independent given measure maximal time maximal time states recurrent classes policies show simplex pi iterations generalizes result deterministic mdps post n$ n$ explain why similar results hard derive pi finally additional restrictive assumption state space sets respectively states recurrent policies show simplex pi pi iterations 
online learning nonparametric mixture models sequential variational approximation computationally expensive algorithms inference limiting bayesian nonparametric models large scale applications tackle problem propose bayesian learning algorithm dp mixture models following conventional paradigm random initialization plus iterative update approach starting given prior method recursively approximate posterior sequential variational approximation process components incorporated needed algorithm estimate dp mixture model pass making particularly suited applications massive data experiments synthetic data real datasets demonstrate remarkable improvement efficiency orders magnitude speed compared state art 
online robust pca stochastic optimization robust pca methods typically based batch optimization load samples memory efficiently processing big data paper develop online robust principal component analysis pca processes sample time instance hence its memory cost independent data size significantly computation storage efficiency proposed method based stochastic optimization equivalent batch rpca method indeed show pca provides sequence subspace optimum its batch counterpart hence provably robust sparse corruption moreover pca naturally applied tracking dynamic subspace simulations subspace recovering tracking demonstrate robustness efficiency advantages pca online pca batch rpca methods 
informative dimensions present novel non parametric method finding subspace stimulus features contains information response system method generalizes similar approaches problem spike triggered average spike triggered covariance maximally informative dimensions maximizing mutual information features responses directly integral probability metrics kernel hilbert spaces minimize information features combination informative features responses since estimators metrics access data kernels easy compute exhibit good theoretical convergence properties method easily generalized populations neurons spike patterns using particular expansion mutual information show informative features information features independent rest 
scalable approach probabilistic latent space inference large scale networks propose scalable approach making inference latent spaces large networks succinct representation networks bag motifs statistical model efficient stochastic variational inference algorithm able analyze real networks vertices hundreds latent single machine setting reach many existing methods compared state art probabilistic approaches method several orders magnitude faster competitive improved accuracy latent space recovery link prediction 
understanding variable forests randomized trees despite growing interest practical various scientific areas variable derived tree based ensemble methods understood theoretical point view work characterize mean decrease variable measured ensemble randomized trees asymptotic sample ensemble size conditions derive level decomposition information jointly provided input variables output terms importance input variable ii degree interaction given input variable input variables iii different interaction terms given degree show importance variable equal zero only variable irrelevant importance relevant variable invariant respect removal addition irrelevant variables illustrate properties simple example discuss may change case non randomized trees random forests extra trees 
correlated random features fast semi supervised learning paper presents correlated nystrom fast semi supervised algorithm regression classification algorithm draws main ideas generates consisting computationally random features second regression using canonical correlation analysis unlabeled data biases regression towards useful features shown regression substantially reduce variance minimal increase bias contains accurate estimators theoretical empirical work shows regression random features closely approximates kernel regression accuracy requirement holds random show consistently outperforms state art algorithm semi supervised learning substantially improving predictive performance reducing variability performance wide variety real world datasets reducing runtime orders magnitude 
dynamic clustering asymptotics dependent dirichlet process mixture paper presents novel algorithm based dependent dirichlet process mixture model clustering batch sequential data containing unknown number evolving clusters algorithm derived low variance asymptotic analysis gibbs sampling algorithm provides hard clustering convergence guarantees similar means algorithm empirical results synthetic moving gaussian clusters real trajectory data demonstrate algorithm requires orders magnitude computational time probabilistic hard clustering algorithms providing higher accuracy examined datasets 
better approximation faster algorithm using proximal average common practice approximate complicated functions friendly ones large scale machine learning applications losses regularizers entail great computational challenges usually approximated smooth functions examine powerful methodology point approximation simply proximal map approximation using convex analysis tool proximal average yields novel proximal gradient algorithm strictly better based smoothing any extra overhead numerical experiments conducted important applications overlapping group lasso graph guided fused lasso corroborate theoretical claims 
rapid distance based detection sampling distance based approaches detection popular data mining require model underlying probability distribution particularly challenging high dimensional data present empirical comparison various approaches distance based detection large number datasets report surprising observation simple sampling based scheme outperforms state art techniques terms efficiency effectiveness better understand phenomenon provide theoretical analysis why sampling based approach outperforms alternative methods based nearest neighbor search 
regularized estimators statistical algorithmic theory local optima establish theoretical results local optima various regularized estimators loss penalty functions allowed nonconvex results show long loss function satisfies restricted strong convexity penalty function satisfies suitable conditions any local optimum objective function lies statistical precision true parameter vector theory covers broad class nonconvex objective functions including corrected versions lasso errors variables linear models regression generalized linear models using nonconvex regularizers graph inverse covariance matrix estimation optimization side show simple adaptation gradient descent may used compute global optimum statistical precision epsilon log epsilon iterations fastest possible rate any order method provide variety simulations illustrate theoretical predictions 
non linear domain adaptation boosting common assumption machine vision training samples drawn distribution however many problems assumption violated applications different generate variations appearance data due changing experimental conditions problem data very time consuming limiting amount data labeled training paper present multi task learning algorithm domain adaptation based boosting unlike previous approaches learn task specific decision boundaries method learns single decision shared feature space common tasks boosting learn non linear mapping observations task need specific priori knowledge its global analytical form yields parameter domain adaptation approach successfully leverages learning tasks labeled data evaluate approach challenging datasets achieve significant improvement state art 
mid level visual element discovery discriminative mode work mid level visual representations capture information level complexity higher typical visual words lower full semantic objects several approaches proposed discover mid level visual elements representative frequently occurring visual dataset visually discriminative however current approaches hoc difficult analyze evaluate work pose visual element discovery discriminative mode drawing connections known studied mean shift algorithm given weakly labeled image collection method discovers visually coherent clusters maximally discriminative respect labels advantage formulation requires only single pass data propose coverage principled way experimentally analyzing evaluating different visual discovery approaches compare method prior work view dataset evaluate method task scene classification demonstrating state art performance scene dataset 
quantile estimator high dimensional distributions paper introduce novel method efficiently estimate family hierarchical sets high dimensional distributions method natural extension class svm algorithm finds multiple parallel reproducing kernel hilbert space call method used estimate high dimensional distribution purpose introduce global convex optimization program finds estimated sets once show solved efficiently prove correctness method present empirical results demonstrate its superiority existing methods 
active learning outcome dependent query costs propose learning setting unlabeled data cost label depends its value known advance study binary classification extreme case algorithm only pays negative labels motivation applications detection possible term setting consider complexity algorithm number negative points labels learn hypothesis low relative error design algorithms thresholds line axis aligned rectangles show algorithms complexity significantly lower active label complexity discuss general approach general hypothesis class describe several interesting directions future work 
message passing algorithm multi agent trajectory planning describe novel approach computing collision global trajectories agents specified initial final based improved version alternating direction method multipliers admm algorithm compared existing methods approach naturally parallelizable allows incorporating different cost functionals only apply method classical challenging instances observe its computational requirements scale several cost functionals show algorithm used local motion planning solving problem joint optimization space 
learning stochastic neural networks multilayer neural networks popular models used nonlinear regression classification tasks regressors model conditional distribution predictor variables given input variables however predictive distribution assumed gaussian tasks structured prediction problems conditional distribution multimodal forming many mappings using stochastic hidden variables deterministic ones belief nets induce rich multimodal distribution output space however previously proposed learning algorithms very slow work real valued data paper propose stochastic network hidden layers having deterministic stochastic variables generalized em training procedure using importance sampling allows efficiently learn complicated conditional distributions demonstrate superiority model conditional restricted boltzmann machines mixture density networks synthetic datasets modeling expressions moreover show latent features model improves classification provide additional qualitative results color images 
inferring neural population dynamics multiple partial recordings neural circuit simultaneous recordings activity large neural populations extremely used infer dynamics interactions neurons local circuit light computations performed possible measure activity hundreds neurons using photon calcium imaging however many computations thought involve circuits consisting thousands neurons cortical cortex contribute statistical method together sequentially sets neurons model phrasing problem fitting latent dynamical system missing observations method allows substantially expand population sizes population dynamics characterized number simultaneously neurons particular demonstrate using recordings cortex method makes possible predict noise correlations non simultaneously recorded neuron pairs 
multi prediction deep boltzmann machines introduce multi prediction deep boltzmann machine seen single probabilistic model trained maximize variational approximation generalized family recurrent nets share parameters approximately solve different inference problems prior methods training perform classification tasks require initial learning pass trains layer time does require greedy outperforms standard classification classification missing inputs mean field prediction tasks 
higher order priors joint intrinsic image objects attributes estimation many methods proposed recover intrinsic scene properties shape illumination single image however models applied datasets work explore effects intrinsic scene properties recovered image objects attributes present scene cast problem joint energy minimization framework model able encode strong correlations intrinsic properties shape illumination objects given scene tested approach pascal datasets observe qualitative improvements overall accuracy 
blind calibration compressed sensing using message passing algorithms compressed sensing concept allows acquire compressible signals small number measurements very implementations therefore correct calibration central issue paper study called blind calibration training signals available perform calibration sparse unknown extend approximate message passing algorithm used case blind calibration calibration gains elements signals algorithm applicable settings measurements ways multiplication gain unlike previously blind calibration algorithms based convex relaxations study phase blind calibration problem show cases convex relaxation possible algorithm requires smaller number measurements signals order perform 
learning trajectory preferences iterative improvement consider problem learning good trajectories tasks challenging because criterion defining good trajectory varies users tasks environments paper propose active online learning framework teaching preferences its users object tasks key novelty approach lies type feedback expected user human user does need demonstrate optimal trajectories training data needs iteratively provide trajectories improve trajectory currently proposed system argue active preference feedback easily user demonstrations optimal trajectories challenging non intuitive provide high degrees theoretical regret bounds algorithm match asymptotic rates optimal trajectory algorithms formulate function capture contextual information demonstrate algorithm variety tasks preferences only influenced object being environment 
large scale distributed sparse precision estimation consider problem sparse precision matrix estimation high dimensions using estimator several desirable theoretical properties present inexact alternating direction method multiplier admm algorithm establish rates convergence objective optimality conditions develop large scale distributed framework computations scales millions dimensions parameters using hundreds proposed framework solves column blocks only involves operations parallel matrix multiplications evaluate algorithm shared memory distributed memory architectures block distribution data parameters achieve load balance improve efficiency memory hierarchies experimental results show algorithm substantially scalable state art methods scales linearly number 
neural representation action sequences far simple matching model macaque superior temporal brain area receives integrates inputs ventral dorsal visual processing thought form motion processing respectively processing articulated actions prior work shown small population neurons contains sufficient information decoding actor invariant action action invariant actor specific conjunction actor action paper addresses questions invariance properties individual neural representations population representation second neural encoding mechanisms produce individual neural representations pixel baseline model simply computes linear weighted sum ventral dorsal responses short action produces surprisingly good fits neural data interestingly using inputs single stream actor invariance action invariance produced simply having different linear weights 
algorithms sparse multi factor nmf nonnegative matrix factorization nmf popular data analysis method objective matrix nonnegative components product nonnegative matrices work describe simple efficient algorithm multi factor nonnegative matrix factorization problem generalizes original nmf problem factors furthermore extend algorithm incorporate regularizer based dirichlet distribution normalized columns encourage sparsity obtained factors sparse nmf algorithm closed form intuitive interpretation efficient comparison previous works point iterations demonstrate effectiveness efficiency algorithms synthetic real data sets 
statistical models provide unified framework high dimensional analysis statistical models model parameters constrained parameters allow any number types structures any statistical model consider general class estimators minimize sum any loss function instance call regularization convolution weighted regularization functions structural component provide unified framework varied statistical models linear regression multiple regression principal component analysis varied superposition structures 
parallel sampling dp mixture models using sub cluster present novel mcmc sampler dirichlet process mixture models used conjugate non conjugate prior distributions proposed sampler massively achieve significant computational gains non ergodic restricted gibbs iteration mixed merge proposals produce valid sampler regular cluster augmented sub clusters construct likely moves unlike many previous parallel samplers proposed sampler accurately correct stationary distribution markov chain need approximate models empirical results illustrate sampler exhibits better convergence properties current methods 
trading computation communication distributed stochastic dual coordinate ascent present study distributed optimization algorithm employing stochastic dual coordinate ascent method stochastic dual coordinate ascent methods strong theoretical guarantees better performances stochastic gradient descent methods optimizing regularized loss minimization problems efforts studying distributed framework progress line distributed stochastic dual coordinate ascent algorithm star network analysis tradeoff computation communication verify analysis experiments real data sets moreover compare proposed algorithm distributed stochastic gradient descent methods distributed alternating direction methods multipliers optimizing distributed framework observe competitive performances 
prior prior dependent regret bounds thompson sampling consider stochastic multi armed bandit problem prior distribution reward distributions interested studying prior prior dependent regret bounds very spirit usual distribution distribution dependent bounds non bayesian stochastic bandit show thompson sampling attains optimal prior bound sense any prior distribution its bayesian regret bounded above result sense exists prior distribution any algorithm bayesian regret bounded study case priors setting optimal mean known lower bound smallest gap show case regret thompson sampling fact uniformly bounded time showing thompson sampling greatly advantage properties priors 
structured learning logistic regression successful approach structured learning write learning objective joint function linear parameters inference messages updates paper observes inference problem addition entropy terms fixed messages learning objective reduces traditional non structured logistic regression problem respect parameters logistic regression problems training example bias term determined current set messages based insight structured energy function extended linear factors any function class exists minimize logistic loss 
space partitioning tree consider task nearest neighbor search class binary space partitioning trees includes trees principal axis trees random projection trees try rigorously answer question tree nearest neighbor present theoretical results imply trees better vector quantization performance better search performance guarantees explore another factor search performance margins trees demonstrate theoretically empirically large margin improve search performance space partitioning tree 
ising model parameters fast mixing inference general ising models difficult due high making tree based algorithms intractable moreover interactions strong gibbs sampling may exponential time converge stationary distribution present algorithm project ising model parameters parameter set guaranteed fast mixing several gibbs sampling using projected parameters accurate original parameters interaction strengths strong limited time available sampling 
mixed optimization smooth functions known optimal convergence rate stochastic optimization smooth functions stochastic optimization lipschitz continuous convex functions contrast optimizing smooth functions using full gradients yields convergence rate work consider setup optimizing smooth functions termed mixed optimization allows access stochastic oracle full gradient oracle goal significantly improve convergence rate stochastic optimization smooth functions having additional small number full gradient oracle show full gradient oracle stochastic oracle proposed mixed optimization algorithm able achieve optimization error 
conditional random fields univariate exponential families conditional random fields model distribution multivariate response conditioned set covariates using undirected graphs widely used variety multivariate prediction applications popular instances class models categorical discrete ising conditional gaussian based however best suited varied types response variables many applications including count valued responses introduce derived imposing node wise conditional distributions response variables conditioned rest responses covariates arising univariate exponential families allows derive novel multivariate given any univariate exponential distribution including poisson negative exponential distributions particular addresses common problem specifying feature functions determining interactions response variables covariates develop class tractable penalized estimators learn distributions data unified analysis general class showing exact structure recovery achieved high probability 
stochastic blockmodel approximation theory consistent estimation given convergent sequence graphs exists limit object called random graphs generated nonparametric perspective random graphs opens door study graphs traditional parametric models time poses challenging question estimate underlying observed graphs paper propose computationally efficient algorithm estimate set observed graphs generated show approximating stochastic block models consistently estimated estimation error size graph approaches 
reinforcement learning robust markov decision processes important challenge markov decision processes ensure robustness respect adversarial system behavior taking advantage parts system consider problem setting unknown parts state space arbitrary transitions parts stochastic devise algorithm adaptive potentially adversarial behavior show achieves similar regret bounds stochastic case 
linear convergence proximal gradient method trace norm regularization motivated various applications machine learning problem minimizing convex smooth loss function trace norm regularization received attention currently popular method solving problem proximal gradient method known sublinear rate convergence paper show large class loss functions convergence rate fact linear result established any strong convexity assumption loss function key proof error bound aforementioned trace norm regularized problem may independent interest 
recurrent networks coupled solving constraint satisfaction problems present recurrent neuronal network modeled continuous time dynamical system solve constraint satisfaction problems discrete variables represented coupled networks values encoded localized patterns oscillations learned recurrent weights networks constraints variables encoded network connectivity sources noise network escape local optima its search solutions satisfy constraints effective network connectivity oscillations solution satisfies constraints network state changes pseudo random manner its trajectory approximates sampling procedure selects variable assignment probability increases fraction constraints assignment external evidence input network force variables specific values inputs applied network evaluates entire set variables its search states satisfy maximum number constraints being consistent external input results demonstrate proposed network architecture perform deterministic search optimal solution problems non convex cost functions network inspired canonical models cortex suggests possible dynamical mechanisms solve constraint satisfaction problems present biological networks implemented circuits 
latent structured active learning paper present active learning algorithms context structured prediction problems reduce amount labeling necessary learn good models algorithms only label subsets output query examples using entropies local marginals good surrogate uncertainty demonstrate effectiveness approach task prediction single images show good models learned labeling only random variables particular performance using full training set obtained only labeling random variables 
bandits multi armed bandit problems great deal attention because formalize exploration exploitation trade offs arising several relevant applications online advertisement generally recommendation systems many cases however applications strong social component integration bandit algorithm lead performance increase instance may want serve content group users taking advantage underlying network social relationships paper introduce novel algorithmic approaches solution bandit problems specifically design analyze global strategy bandit algorithm network node user allows signals contexts nodes derive scalable variants strategy based different ways clustering graph nodes experimentally compare algorithm its variants state art methods contextual bandits relational information experiments synthetic real world datasets show increase prediction performance obtained exploiting network structure 
learning feature selection dependencies multi task learning probabilistic model based prior proposed learning dependencies process identifying relevant features prediction exact inference intractable model however expectation propagation offers approximate alternative because process estimating feature selection dependencies may suffer fitting model proposed additional data multi task learning scenario considered model used setting modifications furthermore assumptions restrictive multi task methods different tasks share feature selection dependencies different relevant features model coefficients experiments real synthetic data show model performs better multi task alternatives literature experiments show model able induce suitable feature selection dependencies problems considered only training data 
non parametric low variance kernel sample propose family maximum mean kernel sample tests low sample complexity consistent hyperparameter allows control tradeoff sample complexity computational time family tests tests computationally statistically efficient combining favorable properties previously proposed sample tests does better leveraging samples produce low variance estimates finite sample case avoiding quadratic number kernel evaluations complex null hypothesis approximation required tests sample statistics uses smaller quadratic number kernel evaluations avoids completely computational burden complex null hypothesis approximation maintaining consistency conservative thresholds type error finally results combining multiple kernels transfer hypothesis allowing increase discriminative power decrease sample complexity 
online pca contaminated data consider online principal component analysis pca contaminated samples containing outliers revealed sequentially principal components estimator due outliers previous online pca algorithms fail case results arbitrarily propose online robust pca algorithm able improve estimation initial constant fraction outliers show final result proposed online rpca optimum actually mild conditions online rpca achieves maximal robustness breakdown point moreover online rpca shown efficient storage computation since need explore previous samples traditional robust pca algorithms online rpca scalability large scale data 
non strongly convex smooth stochastic approximation convergence rate consider stochastic approximation problem convex function minimized given only knowledge unbiased estimates its gradients certain points framework includes machine learning methods based minimization empirical risk focus problems strong convexity previously known algorithms achieve convergence rate function values consider analyze algorithms achieve rate classical supervised learning problems squares regression show stochastic gradient descent constant step size achieves desired rate logistic regression achieved simple novel stochastic gradient algorithm successive local quadratic approximations loss functions preserving running time complexity stochastic gradient descent algorithms provide non asymptotic analysis generalization error expectation high probability squares run extensive experiments showing outperform existing approaches 
efficient algorithm smooth queries study differentially private mechanisms smooth queries databases consisting data points d$ smooth query specified function partial derivatives order bounded develop differentially private mechanism class smooth queries accuracy mechanism outputs summary obtain answer query user runs evaluation algorithm contains information summary runs time evaluation algorithm query runs time mechanism based approximation transformed smooth functions low degree small efficiently computable coefficients 
pairwise provably fast algorithms approximate way similarity search notion pairwise similarity look search problems way similarity functions paper focus problems related way similarity way size collection sets binary vectors show approximate way similarity search problems fast algorithms provable guarantees analogous pairwise case analysis speedup guarantees naturally extend way process extend traditional framework locality sensitive hashing lsh handle higher order independent theoretical interest applicability way search shown sets application addition demonstrate advantage way pairwise case improving retrieval quality 
unsupervised spectral learning finite state finite state standard tool modeling input output sequences used numerous applications ranging computational biology natural language processing recently presented spectral algorithm learning samples aligned input output sequences paper address realistic challenging setting unknown learning algorithm learning finding low rank matrix constraints derived observable statistics formulation provide identifiability results distributions following previous work rank minimization propose regularized convex relaxation objective based minimizing nuclear norm penalty subject linear constraints solved efficiently 
learning deep compact image representation visual tracking paper study challenging problem tracking trajectory moving object video possibly very complex background contrast existing only learn appearance object online different approach inspired advances deep learning architectures emphasis unsupervised feature learning problem specifically using auxiliary natural images train stacked denoising autoencoder offline learn generic image features robust variations followed knowledge transfer offline training online tracking process online tracking involves classification neural network constructed encoder part trained autoencoder feature additional classification layer feature classifier tuned adapt appearance changes moving object comparison state art challenging benchmark video sequences shows deep learning very efficient accurate 
learning multi level sparse representations bilinear approximation matrix powerful paradigm unsupervised learning applications however natural hierarchy concepts reflected unsupervised analysis example image sequence considered semantic concepts pixel neuron counterpart unsupervised analysis driven concrete problem propose decomposition matrix observations product sparse matrices rank decreasing lower higher levels contrast prior work allow hierarchical relations lower level higher level concepts addition learn nature relations imposing finally describe optimization scheme allows optimize decomposition levels jointly greedy level level fashion proposed bilevel sparse matrix factorization formalism allows simultaneously interpret calcium imaging sequence terms neurons membership time neurons experiments show proposed model fully recovers structure difficult synthetic data designed experimental data importantly bilevel yields real world calcium imaging data 
robust data driven dynamic programming stochastic optimal control distribution noise typically unknown limited data before dynamic programming dp based solution schemes applied conditional expectations dp estimated kernel regression however sample paths solution procedure directly determine evaluation points cost functions resulting data driven dp scheme asymptotically consistent admits efficient computational solution combined parametric value function approximations training data sparse however estimated cost functions display high variability optimistic bias corresponding control policies perform poorly sample tests small sample effects propose robust data driven dp scheme expectations dp worst case expectations set distributions close best estimate show arising max problems dp reduce tractable conic programs demonstrate robust algorithm state art benchmark algorithms sample tests several application domains 
low rank matrix tensor completion adaptive sampling study low rank matrix tensor completion propose novel algorithms employ adaptive sampling schemes obtain strong performance guarantees problems algorithms exploit adaptivity identify entries highly informative identifying column space matrix tensor consequently results hold row space highly coherent contrast previous analysis matrix completion absence noise show exactly recover n$ matrix rank using observations better best known bound random sampling show recover order tensor using noisy recovery show consistently estimate low rank matrix corrupted noise using observations complement study simulations verify theoretical guarantees demonstrate scalability algorithms 
probabilistic low rank matrix completion adaptive spectral regularization algorithms propose novel class algorithms low rank matrix completion approach builds novel penalty functions values low rank matrix exploiting mixture model representation penalty show suitably chosen set latent variables enables derive expectation maximization algorithm obtain maximum posteriori estimate completed low rank matrix resulting algorithm iterative soft thresholded algorithm iteratively adapts shrinkage coefficients associated values algorithm simple implement scale large matrices provide numerical comparisons approach alternatives showing interest proposed approach low rank matrix completion 
distributed exploration multi armed bandits study exploration multi armed bandits mab setting players order identify optimal arm motivation comes mab algorithms computationally large scale applications results demonstrate non trivial tradeoff number arm required players amount communication particular main result shows allowing players only once able learn times faster single player learning players gives rise parallel speed complement result lower bound showing general best possible extreme present algorithm achieves factor speed learning performance communication only logarithmic 
regret frontier performance guarantees online learning algorithms typically form regret bounds express cumulative loss overhead compared best expert hindsight small common case large structured expert sets typically regret especially small compared simple experts cost modest additional overhead compared complex others study regret trade offs achieved regret individual expert multi objective criterion simple fundamental case loss characterise achievable optimal trade offs corresponding optimal strategies sample size exactly finite horizon asymptotically 
direct loss minimization margin maximization boosting propose boosting method greedy coordinate descent algorithm builds ensemble classifier weak classifiers directly minimizing empirical classification error labeled training examples once training classification error reduced local minimum runs greedy coordinate ascent algorithm continuously weak classifiers maximize any arbitrarily defined margins reaching local maximum margins certain sense experimental results collection machine learning benchmark datasets show gives consistently better results column generation noise tolerant maximizes order bottom sample margin 
regret based robust solutions uncertain markov decision processes paper seek robust policies uncertain markov decision processes mdps robust optimization approaches problems computation policies maximize value corresponding worst uncertainty work proposed minimax regret suitable alternative objective robust optimization however existing algorithms handling minimax regret restricted models uncertainty rewards only provide algorithms employ sampling improve multiple dimensions handle transition reward models dependence model state action pairs decision scalability quality bounds finally demonstrate empirical effectiveness sampling approaches provide comparisons benchmark algorithms domains literature provide sample average approximation analysis compute posteriori error bounds 
permutation testing multiple hypothesis testing significant problem nearly studies order correct phenomena require reliable estimate family wise error rate known correction method being simple implement quite conservative substantially power study because dependencies statistics permutation testing hand exact non parametric method estimating given threshold low thresholds computational burden prohibitive paper observe permutation testing fact amounts columns very large matrix analyzing spectrum matrix certain conditions see low rank plus low variance residual decomposition makes suitable highly order matrix completion methods propose novel permutation testing methodology offers large speedup fidelity estimated different datasets show computational speedup factor roughly achieved recovering distribution very high accuracy show estimated threshold recovered faithfully stable 
generalized denoising auto encoders generative models work shown denoising implicitly capture structure data generating density case corruption noise gaussian reconstruction error squared error data continuous valued led various proposals sampling implicitly learned density function using langevin metropolis hastings mcmc however training procedure regularized auto encoders implicit estimation underlying data generating distribution data discrete using forms corruption process reconstruction errors another issue mathematical justification only valid limit small corruption noise propose different problem issues arbitrary noisy corruption arbitrary reconstruction loss seen log likelihood handling discrete continuous valued variables removing bias due non corruption noise non penalty 
supervised sparse analysis operators paper propose computationally efficient framework learning sparse models formulate unified approach contains particular cases models sparse analysis type priors mixtures supervised training proposed model formulated bilevel optimization problem operators optimized achieve best possible performance specific task reconstruction classification restricting operators shift invariant approach thought way learning analysis sparsity convolutional operators leveraging ideas fast regressors designed approximate exact sparse codes propose way constructing forward neural networks approximating learned models fraction computational cost exact solvers shift invariant case leads principled way constructing task specific convolutional networks illustrate proposed models several experiments music analysis image processing applications 
low rank matrix reconstruction clustering approximate message passing study problem reconstructing low rank matrices noisy observations formulate problem bayesian framework allows exploit structural properties matrices addition low sparsity propose efficient approximate message passing algorithm derived belief propagation algorithm perform bayesian inference matrix reconstruction successfully applied proposed algorithm clustering problem formulating problem clustering low rank matrix reconstruction problem additional structural property numerical experiments show proposed algorithm outperforms lloyd means algorithm 
reasoning neural tensor networks knowledge base completion common problem knowledge representation related fields reasoning large joint knowledge graph represented relation entities goal paper develop powerful neural network model suitable inference relationships previous models suffer weak interaction entities simple linear projection vector space address problems introducing neural tensor network model allow entities relations interact additionally observe knowledge base models improved representing entity average vectors words entity additional dimension similarity entities share statistical strength assess model considering problem predicting additional true relations entities given partial knowledge base model outperforms previous models classify unseen relationships accuracy respectively 
zero shot learning cross transfer work introduces model recognize objects images training data available object class only necessary knowledge unseen categories comes unsupervised corpora unlike previous zero shot learning models only differentiate unseen classes model mixture objects simultaneously obtaining state art performance classes thousands training images reasonable performance unseen classes achieved seeing distributions words texts semantic space understanding objects look deep learning model does require any manually defined semantic visual features words images images close semantic word vectors corresponding classes resulting image embeddings used image seen unseen class separate recognition model employed type demonstrate strategies gives high accuracy unseen classes second conservative its prediction novelty seen classes accuracy high 
estimating lasso risk noise level study fundamental problems variance risk estimation high dimensional statistical modeling particular consider problem learning coefficient vector p$ noisy linear observation n$ popular estimation procedure solving penalized squares objective known lasso basis pursuit denoising context develop estimators estimation risk variance noise used select regularization parameter optimally approach combines unbiased risk estimate results analysis approximate message passing risk lasso establish high dimensional consistency estimators sequences matrices increasing dimensions independent gaussian entries establish broader class gaussian designs conditional certain conjecture statistical approach provides asymptotically consistent risk estimator addition demonstrate simulation variance estimation outperforms several existing methods literature 
learning adaptive value information structured prediction discriminative methods learning structured models wide spread very rich feature representations however computational cost feature extraction prohibitive large scale time sensitive applications dominating cost inference models significant efforts sparsity based model selection decrease cost feature selection methods control computation tune feature extraction input run time address key challenge learning control feature extraction adaptively exploiting non data propose architecture uses rich feedback loop extraction prediction run time control policy learned using efficient value function approximation adaptively value information features level individual variables input demonstrate significant state art methods challenging datasets articulated pose estimation video achieve accurate state art model simultaneously faster using only small fraction possible features similar results task 
efficient online inference bayesian nonparametric relational models stochastic block models characterize observed network relationships latent community memberships large social networks expect entities multiple communities number communities grow network size introduce model phenomena hierarchical dirichlet process relational model allows nodes mixed membership unbounded set communities allow scalable learning derive online stochastic variational inference algorithm focusing models undirected networks propose efficient structured mean field variational bound online methods automatically communities compared state art online learning methods parametric relational models show significantly improved link prediction accuracy sparse networks tens thousands nodes analysis large network knows 
approximate inference latent gaussian markov models continuous time observations propose approximate inference algorithm continuous time gaussian markov process models discrete continuous time likelihoods show continuous time limit expectation propagation algorithm exists results hybrid fixed point iteration consisting expectation propagation updates discrete time terms variational updates continuous time term introduce methods improve marginals approximation approach extends classical kalman smoothing procedure non gaussian observations enabling continuous time inference variety models including spiking neuronal models state space models point process observations box likelihood models experimental results real simulated data demonstrate high distributional accuracy significant computational savings compared discrete time approaches neural application 
linear convergence condition number independent access full gradients smooth strongly convex optimization optimal iteration complexity gradient based algorithm conditional number case optimization problem ill conditioned need evaluate larger number full gradients computationally expensive paper propose reduce number full gradient required allowing algorithm access stochastic gradients objective function present novel algorithm named mixed gradient descent able utilize kinds gradients distinctive step mixed gradient descent combination gradient stochastic gradient update intermediate solutions performing fixed number mixed gradient able improve sub optimality solution constant factor achieve linear convergence rate theoretical analysis shows able optimal solution computing full gradients stochastic gradients 
high dimensional sparse recovery correlated measurements consider problem accurately estimating high dimensional sparse vector using small number linear measurements contaminated noise known standard computationally tractable sparse recovery algorithms lasso various extensions perform poorly measurement matrix contains highly correlated columns develop simple greedy algorithm called iteratively variables desired loss function any surprisingly effective handling measurement matrices high correlations prove easily used standard sparse recovery algorithms improved performance theoretically quantify statistical guarantees complement analysis numerical results synthetic real data 
graphs deep learning increasing number applications require processing signals defined weighted graphs provide flexible tool signal processing classical setting regular domains existing graph flexible guided solely structure underlying graph directly particular class signals paper introduces machine learning framework constructing graph sparsely represent given class signals construction uses lifting scheme based observation recurrent nature lifting scheme gives rise structure resembling deep auto encoder network particular properties resulting satisfy determine training objective structure neural networks training unsupervised conducted greedy pre training auto encoders training completed obtain linear transform applied any graph signal time memory linear size graph improved sparsity transform signals experiments synthetic real data 
robust spatial filtering beta divergence efficiency brain computer interfaces bci largely depends reliable extraction informative features high dimensional signal crucial step protocol computation spatial filters common spatial patterns algorithm computes filters maximize difference power conditions tailored extract relevant information motor experiments however highly sensitive artifacts data outliers may alter estimate decrease classification performance inspired concepts field information geometry propose novel approach precisely formulate divergence maximization problem utilize property particular type divergence namely beta divergence estimation spatial filters presence artifacts data demonstrate usefulness method toy data recordings subjects 
convex relaxations permutation problems seeks reconstruct linear order variables using similarity information direct applications sequencing example prove equivalence combinatorial sum problem quadratic minimization problem class similarity matrices problem solved exactly spectral algorithm noiseless case produce convex relaxation sum problem improve robustness solutions noisy setting relaxation allows impose additional structural constraints solution solve semi supervised problems present numerical experiments data markov chains sequences 
high dimensional gaussian process bandits many applications machine learning require optimizing unknown functions defined high dimensional space noisy samples expensive obtain address notoriously hard challenge assumptions function varies only low dimensional subspace smooth low norm kernel hilbert space particular present algorithm leverages low rank matrix recovery techniques learn underlying subspace unknown function applies gaussian process upper confidence sampling optimization function carefully tradeoff sampling budget subspace estimation function optimization obtain cumulative regret bounds convergence rates bayesian optimization high dimensions noisy observations numerical results demonstrate effectiveness approach difficult scenarios 
memory frontier complex synapses theoretical models synapses described solely single scalar value size postsynaptic potential complexity molecular underlying real synapses understand functional contribution molecular complexity learning memory essential expand theoretical single scalar entire dynamical system many internal molecular functional states moreover theoretical considerations alone demand expansion network models scalar synapses assuming finite numbers distinguishable synaptic strengths limited memory capacity raises fundamental question does synaptic complexity give rise address develop mathematical relationship structural memory properties complex synapses themselves molecular networks moreover proving uncover framework based time theory impose order internal states complex synaptic models thereby relationship synaptic structure function 
marginals models consider number classical computational problems regarding marginal distributions inference models specifying full joint distribution prove general efficient number problems demonstrate algorithmic progress inference automatically yields progress problems main technique involves formulating problems linear programs proving dual separation oracle method provided target problem technique may independent interest probabilistic inference 
order decomposition trees lifting attempts speedup probabilistic inference exploiting symmetries model exact lifted inference methods propositional counterparts work recursively decomposing model problem propositional case exist formal structures decomposition trees represent decomposition allow determine complexity inference priori however currently equivalent structure analogous complexity results lifted inference paper introduce propositional order level show trees characterize lifted inference solution probabilistic model terms sequence lifted operations theoretical analysis complexity lifted inference terms novel notion lifted tree 
comparative framework lasso algorithms lasso modern multivariate data analysis its performance suffers common situation covariates correlated limitation led growing number lasso algorithms pre multiply matrices prior running standard lasso direct comparison similar lasso style algorithms original lasso difficult because performance methods depends auxiliary penalty parameter paper propose agnostic theoretical framework comparing lasso algorithms lasso having choose apply framework lasso instances highlight outperform lasso additionally theory offers insights algorithms provide partial solutions 
lasso rules dual projection lasso widely used regression technique sparse representations dimension feature space number samples extremely large solving lasso problem remains challenging improve efficiency solving large scale lasso problems proposed safe rules able quickly identify predictors predictors components solution vector predictors features optimization problem reduce its scale transforming standard lasso its dual form shown predictors include set constraints optimal dual solution paper propose efficient effective rule dual projections dpp mainly based uniqueness optimal dual solution due fact feasible set dual space convex closed moreover show rule extended identify groups group lasso best knowledge currently exact rule group lasso evaluated rule using many real data sets results show rule effective identify predictors existing state art rules lasso 
binary bayesian hierarchical clustering beta discovering hierarchical regularities data key problem interacting large datasets modeling cognition encoding knowledge previous bayesian solution provides convenient probabilistic model data represented binary tree unfortunately data better described trees generalize existing belief propagation framework beta models range tree structures because complex combinatorial search possible structures develop sampling schemes using sequential monte carlo dirichlet process mixture models inference efficient tractable present results synthetic real data show beta outperforms real datasets qualitatively better capturing data hierarchies 
latent source model nonparametric time series classification classifying time series nearest neighbor approach widely used practice performance competitive better methods neural networks decision trees support vector machines develop theoretical justification effectiveness nearest neighbor classification time series guiding hypothesis many applications forecasting topics become trends actually many time series relative number time series access topics become trends only distinct whereas collect massive amounts data hypothesis propose latent source model time series naturally leads weighted majority voting classification rule approximated nearest neighbor classifier establish performance guarantees weighted majority voting nearest neighbor classification model accounting time series observe model complexity experimental results synthetic data show weighted majority voting achieving rate nearest neighbor classification observing time series weighted majority news topics become trends able detect topics advance time mean advantage true positive rate false positive rate 
efficient optimization sparse gaussian process regression propose efficient discrete optimization algorithm selecting subset training data induce sparsity gaussian process regression algorithm estimates set hyperparameters using single objective marginal likelihood variational energy space time complexity linear training set size algorithm applied large regression problems discrete continuous domains empirical evaluation shows state art performance discrete case competitive results continuous case 
hierarchical topic regression inspired level theory unifies setting propose supervised hierarchical latent dirichlet allocation jointly captures documents multi level topic structure polar response variables model extends nested process discover tree structured topic hierarchy uses topic hierarchical word regression parameters model response variables experiments domain sentiment analysis tasks show improves predictive accuracy dimension insight topics 
stochastic convex optimization multiple objectives paper interested development efficient algorithms convex optimization problems simultaneous presence multiple objectives order information cast stochastic multiple objective optimization problem constrained optimization problem choosing function objective try bound objectives appropriate thresholds examine stages exploration exploitation based algorithm approximates stochastic objectives sampling solves constrained stochastic optimization problem projected gradient method method attains convergence rate strong assumption objectives second approach efficient dual stochastic algorithm leverages theory lagrangian method constrained optimization attains optimal convergence rate high probability general lipschitz continuous objectives 
kernel variable interactions introduce kernel nonparametric tests variable interaction total independence using embeddings measures reproducing kernel hilbert space resulting statistics straightforward compute used powerful variable interaction tests consistent alternatives large family reproducing kernels show sensitive cases independent individually weak influence third dependent variable combined effect strong influence makes especially suited finding structure directed graphical models outperforms competing nonparametric tests structures 
online variational inference dirichlet process mixture models variational inference algorithms provide effective framework large scale training bayesian nonparametric models stochastic online approaches promising sensitive chosen learning rate converge poor local optima present algorithm online variational inference scales very large finite datasets avoiding complexities stochastic gradient algorithm maintains finite dimensional sufficient statistics batches full dataset requiring additional memory scaling millions examples exploiting nested families variational bounds infinite nonparametric models develop principled birth merge moves allowing non local optimization adaptively add components model escape local optima merges redundancy improve speed using dirichlet process mixture models image clustering denoising demonstrate major improvements robustness accuracy 
designed measurements vector count data consider design linear projection measurements vector poisson signal model projections performed vector poisson rate n$ observed data vector counts m$ projection matrix designed maximizing mutual information latent class label associated consider mutual information respect analytic expressions gradient presented gradient performed respect measurement matrix connections widely studied gaussian measurement model example results presented compressive topic modeling document corpora word counting compressive sensing chemical classification photon counting 
robust transfer principal component analysis rank constraints principal component analysis pca established technique data analysis processing provides convenient form dimensionality reduction effective small gaussian presented data however applicability standard principal component analysis real scenarios limited its sensitivity large errors paper tackle challenge problem recovering data corrupted errors high magnitude developing novel robust transfer principal component analysis method method based assumption useful information recovery corrupted data matrix gained uncorrupted related data matrix speciﬁcally formulate data recovery problem joint robust principal component analysis problem data matrices shared common principal components matrices individual principal components data matrix formulated optimization problem minimization problem convex objective function non convex rank constraints develop efﬁcient proximal projected gradient descent algorithm solve proposed optimization problem convergence guarantees empirical results image denoising tasks show proposed method effectively recover images random large errors signiﬁcantly outperform standard pca robust pca 
online learning costs adaptive study power different types adaptive setting prediction expert full information bandit feedback measure player performance using notion regret known policy regret better captures adversary player behavior setting losses allowed drift characterize nearly complete manner power adaptive bounded memories costs particular show costs rate bandit feedback interestingly rate significantly worse rate costs full information case novel reduction experts bandits show bounded memory adversary force regret full information case proving costs easier control bounded memory lower bounds rely stochastic adversary strategy generates loss processes strong dependencies 
learning prices repeated inspired real time online display advertising consider problem inferring value distribution good repeatedly interacting price mechanism model agent goal maximize long term interested mechanisms maximize long term revenue present algorithms regret future showing advertisements users give lower bound regret increases shows particular any algorithm suffer linear regret 
probabilistic principal geodesic analysis principal geodesic analysis generalization principal component analysis pca dimensionality reduction data riemannian manifold currently defined geometric fit data probabilistic model inspired probabilistic pca present latent variable model provides probabilistic framework factor analysis manifolds compute maximum likelihood estimates parameters model develop monte carlo expectation maximization algorithm expectation approximated hamiltonian monte carlo sampling latent variables demonstrate ability method recover ground truth parameters simulated data its effectiveness analyzing shape variability corpus data set human brain images 
confidence intervals hypothesis testing high dimensional statistical models fitting high dimensional statistical models requires non linear parameter estimation procedures consequence generally impossible obtain exact probability distribution parameter estimates turn implies extremely challenging quantify associated certain parameter estimate concretely commonly procedure exists computing classical measures uncertainty statistical confidence intervals values consider broad class regression problems propose efficient algorithm constructing confidence intervals values resulting confidence intervals nearly optimal size testing null hypothesis certain parameter method nearly optimal power approach based constructing biased version regularized estimators construction improves work field does assume special structure design matrix furthermore proofs remarkably simple method prediction problem 
learning noisy labels paper theoretically study problem binary classification presence random classification noise learner seeing true labels labels independently small probability moreover random label noise class conditional probability depends class provide approaches suitably modify any given surrogate loss function provide simple unbiased estimator any loss obtain performance bounds empirical risk minimization presence iid data noisy labels loss function satisfies simple condition show method leads efficient algorithm empirical minimization second leveraging reduction risk minimization noisy labels classification weighted loss suggest simple weighted surrogate loss able obtain strong empirical risk bounds approach very remarkable consequence methods used practice biased svm weighted logistic regression provably noise tolerant synthetic non separable dataset methods achieve accuracy labels corrupted competitive respect recently proposed methods label noise several benchmark datasets 
tracking time varying graphical structure structure learning algorithms graphical models focused exclusively stable environments underlying generative process does change assume generating model globally stationary real world environments however changes occur signal real world data come generating models only locally stationary paper present novel heuristic structure learning algorithm changes graphical model structure parameters dynamic real time manner show simulation algorithm performs batch mode learning generating graphical structure globally stationary significantly better only locally stationary 
factorized asymptotic bayesian inference latent feature models paper extends factorized asymptotic bayesian inference latent feature inference applicable models including specific condition matrix complete log likelihood required derive factorized information criterion asymptotic analysis matrix shows form mixture models several desirable properties automatic hidden states selection parameter identifiability empirically perform better state art indian buffet processes terms model selection prediction computational efficiency 
effective distributed synchronous parallel parameter propose parameter system distributed follows synchronous parallel model computation maximizes time computational workers spend doing useful work algorithms providing correctness guarantees parameter provides easy shared interface read write access model values parameters variables model allows distributed workers read versions values local central storage significantly increases proportion time workers spend computing furthermore model ensures algorithm correctness limiting maximum values provide proof correctness empirical results demonstrating model achieves faster algorithm convergence several different problems compared fully synchronous asynchronous schemes 
bayesian estimation parameters undirected graphical models large scale applications undirected graphical models social networks biological networks similar patterns occur frequently give rise similar parameters situation group parameters efficient learning show grouping unknown infer parameter groups during learning bayesian approach impose dirichlet process prior parameters posterior inference usually involves calculating intractable terms propose approximation algorithms namely metropolis hastings algorithm auxiliary variables gibbs sampling algorithm beta approximation gibbs simulations show algorithms outperform conventional maximum likelihood estimation mle gibbs performance close gibbs sampling exact likelihood calculation models learned gibbs generalize better models learned mle real world voting data 
online learning costly features labels paper introduces online problem learner able purchase values subset feature values learner uses information come prediction given option paying seeing loss evaluated way learner pays predictions chooses observe including cost observing loss function given cost observed features consider variations problem depending learner observe label provide algorithms upper lower bounds regret variants show positive cost observing label significantly increases regret problem 
sparse nonnegative deconvolution compressive calcium imaging algorithms phase transitions propose compressed sensing calcium imaging framework monitoring large neuronal populations image randomized projections spatial calcium concentration measuring concentration individual locations develop scalable nonnegative deconvolution methods extracting neuronal spike time series observations address problem demixing spatial locations neurons using rank penalized matrix factorization methods exploiting sparsity neural spiking demonstrate number measurements needed significantly smaller total number neurons result potentially enable imaging larger populations considerably faster rates compared traditional techniques unlike traditional problem involves block diagonal sensing matrix non orthogonal sparse basis multiple study effect distinctive features noiseless setup using results conic geometry provide tight approximations number measurements needed perfect deconvolution certain classes spiking processes show number phase transition similar phenomena observed standard settings however case required measurement rate depends just mean sparsity level underlying spiking process 
novel step method cross language representation learning cross language important learning task natural language processing critical challenge cross language learning lies words different disjoint feature spaces paper propose step representation learning method feature spaces different exploiting set parallel documents speciﬁcally ﬁrst formulate matrix completion problem produce complete parallel document term matrix documents induce cross document representation applying latent semantic obtained matrix projected gradient descent algorithm solve formulated matrix completion problem convergence guarantees proposed approach evaluated set experiments cross language sentiment tasks product reviews experimental results demonstrate proposed learning approach outperforms number comparison cross language representation learning methods especially number parallel documents small 
sampling gibbs distribution random maximum posteriori perturbations paper describe map inference used sample efficiently gibbs distributions specifically provide means drawing approximate unbiased samples gibbs distributions introducing low dimensional perturbations solving corresponding map assignments approach leads ways derive lower bounds partition functions demonstrate empirically method typical high signal high coupling regime setting results energy challenging alternative approaches sampling lower bounds 
graphical models inference missing data address problem deciding exists consistent estimator given relation data missing random employ formal representation called graphs explicitly causal mechanisms encode dependencies mechanisms variables being measured using representation define notion ensures given graph given query algorithm exists limit large samples produces estimate data missing present conditions graph satisfy order hold devise algorithms detect presence conditions 
visual datasets domain adaptation visual recognition problems common data distribution training testing domain adaptation essential however image data difficult manually discrete domains required adaptation algorithms standard practice datasets domains weak real conditions alter statistics complex ways pose background resolution propose approach automatically discover latent domains image video datasets formulation key properties domains maximum maximum maximum require underlying distributions identified domains different maximum ensure strong discriminative model learned domain devise nonparametric representation efficient optimization procedure coupled constraint successfully discover domains training data extensively evaluate approach object recognition human activity recognition tasks 
statistical active learning algorithms describe framework designing efficient active learning algorithms tolerant random classification noise framework based active learning algorithms statistical sense rely estimates expectations functions random examples builds powerful statistical query framework show any efficient active statistical learning algorithm automatically converted efficient active learning algorithm tolerant random classification noise forms noise complexity resulting algorithms information theoretically optimal quadratic dependence noise rate demonstrate power framework showing commonly studied concept classes including thresholds rectangles linear separators efficiently learned framework results combined generic lead known computationally efficient algorithms learning concept classes presence random classification noise provide exponential improvement dependence error passive counterparts addition show algorithms automatically converted efficient active differentially private algorithms leads differentially private active learning algorithms exponential label savings passive case 
bayesian inference online experimental design mapping neural develop inference optimal design procedure recovering synaptic weights neural base procedure data experiment populations putative neurons recording single postsynaptic neuron present realistic statistical model accounts main sources variability experiment allows large amounts information biological system incorporated available present simpler model facilitate online experimental design entails efficient bayesian inference optimized approach results equal quality posterior estimates synaptic weights roughly number experimental trials experimentally realistic conditions tested synthetic data generated full model 
methods user friendly submodular optimization recently become submodularity naturally captures widely occurring concepts machine learning signal processing computer vision consequence need efficient optimization procedures submodular functions particular minimization problems general submodular minimization challenging propose approach exploits existing submodular functions contrast previous approaches method approximate impractical does need any parameter tuning moreover easy implement key component approach formulation discrete submodular minimization problem continuous best approximation problem solved sequence its solution automatically thresholded obtain optimal discrete solution method solves continuous discrete formulations problem therefore applications learning inference reconstruction experiments show benefits algorithms image segmentation tasks 
unsupervised structure learning stochastic stochastic compactly represent used model different types data images events present unified stochastic agnostic type data being modeled propose unsupervised approach learning structures parameters starting trivial initial approach iteratively unified manner optimizes posterior probability empirical evaluation applied approach learning image achieved comparable better performance previous approaches 
convex tensor decomposition structured norm regularization propose class structured norms tensors includes recently proposed norms latent convex optimization based tensor decomposition based properties structured norms mathematically analyze performance latent approach tensor decomposition empirically perform better approach settings show theoretically indeed case particular unknown true tensor low rank specific mode approach performs knowing mode smallest rank way show novel result structures norms interesting general context structured sparsity confirm numerical simulations theory precisely predict scaling mean squared error 
stochastic ratio matching rbms sparse high dimensional inputs sparse high dimensional data vectors common many application domains very large number rarely non zero features unfortunately computational unsupervised feature learning algorithms based auto encoders rbms because involve reconstruction step input vector predicted current feature values algorithm recently developed successfully handle case auto encoders based importance sampling scheme stochastically selecting input elements actually reconstruct during training particular example generalize idea rbms propose stochastic ratio matching algorithm computational advantages importance sampling scheme show stochastic ratio matching good estimator allowing approach state art bag word classification benchmarks keeping computational cost linear number non zeros 
learning markov networks constraint satisfaction investigate problem learning structure markov network data shown structure networks described terms constraints enables existing solver technology optimization compute optimal networks starting initial scores computed data achieve efficient encodings develop novel markov network structure using balancing condition separators forming network resulting translations propositional its extensions maximum theories answer set programming enable prove optimality networks previously stochastic search 
parametric task learning introduce novel formulation multi task learning called parametric task learning handle infinitely many tasks parameterized continuous parameter key finding certain class problems path optimal task wise solutions represented piecewise linear functions continuous task parameter based fact employ parametric programming technique obtain common shared representation continuously parameterized tasks efficiently show formulation useful various scenarios learning non cost sensitive learning quantile regression demonstrate usefulness proposed method experimentally scenarios 
deep architecture matching short texts many machine learning problems learning matching types objects images users products queries documents matching level objects usually measured inner product certain feature space modeling effort focuses mapping objects original space feature space proven successful range matching tasks insufficient capturing rich structure matching process complicated objects paper propose deep architecture effectively model complicated matching relations objects heterogeneous domains specifically apply model matching tasks natural language finding sensible responses relevant answers given question architecture naturally combines hierarchy intrinsic natural language problems therefore greatly improves state art models 
computing stationary distribution locally computing stationary distribution large finite infinite state space markov chain become central many problems statistical inference network analysis standard methods involve large matrix multiplications power iteration simulations long random walks sample states stationary distribution markov chain monte carlo mcmc however methods computationally costly involve operations every state scale computation time linearly size state space paper provide novel algorithm answers chosen state stationary probability larger estimates stationary probability algorithm uses information local neighborhood state graph induced constant size relative state space provide correctness convergence guarantees depend algorithm parameters mixing properties simulation results show method gives tight estimates 
nonparametric multi group membership model dynamic networks relational graphs networks dynamic relational structure evolves time fundamental problem analysis time varying network data extract summary common structure dynamics underlying relations entities build intuition changes network structure driven dynamics level groups nodes propose nonparametric multi group membership model dynamic networks model contains main components model birth groups respect dynamics network structure distance dependent indian buffet process capture individual node group memberships hidden markov model explain dynamics network structure explicitly modeling connectivity structure demonstrate identifying dynamics latent groups number different types network data experimental results show model achieves higher predictive performance future network forecasting missing link prediction 
adaptive step size policy gradient methods decade policy gradient methods significantly popularity reinforcement learning field particular largely employed motor control robotic applications thanks ability cope continuous state action domains partial observable problems policy gradient mainly focused identification effective gradient directions efficient estimation algorithms performance policy gradient methods determined only gradient direction since convergence properties strongly influenced choice step size small values imply slow convergence rate large values may lead oscillations divergence policy parameters step size value usually chosen hand tuning little attention its automatic selection paper propose determine learning rate maximizing lower bound expected performance gain focusing gaussian policies derive lower bound second order polynomial step size show simplified version lower bound gradient estimated trajectory samples properties proposed approach empirically evaluated linear quadratic problem 
optimistic control distributed unsupervised learning research distributed machine learning algorithms focused algorithms strict constraints algorithms constraints consider intermediate alternative algorithms assume unlikely arise resolution protocol view optimistic control paradigm particularly appropriate large scale machine learning algorithms particularly unsupervised setting demonstrate approach problem areas clustering feature learning online location evaluate methods large scale experiments cluster computing environment 
boosting online offline ensemble learning propose train ensemble learning algorithm limited number samples novel approach lies area offline online ensemble approaches seen restriction identify basic strategies used present main contribution greedy edge expectation maximization maintains content case boosting samples projections weak classifier response space propose efficient algorithmic implementation makes tractable practice demonstrate its efficiency experimentally several compute vision data sets outperforms online offline methods memory constrained setting 
multiclass total variation clustering ideas image processing literature recently motivated set clustering algorithms rely concept total variation algorithms perform partitioning tasks recursive extensions yield results multiclass clustering tasks paper presents general framework multiclass total variation clustering does rely results greatly outperform previous total variation algorithms compare state art nmf approaches 
approximate inference continuous determinantal processes determinantal point processes dpps random point processes suited modeling machine learning focus dpp based models diverse subset selection discrete finite base set discrete setting admits efficient algorithm sampling based defining kernel matrix recently growing interest using dpps defined continuous spaces discrete dpp sampler extends formally continuous case computationally steps required directly extended restricted cases paper present efficient approximate dpp sampling schemes based nystrom random feature approximations apply wide range kernel functions demonstrate utility continuous dpps mixture modeling applications synthesizing human poses activity spaces 
global solver its efficient approximation variational bayesian low rank subspace clustering probabilistic model its prior given bayesian learning offers inference automatic parameter tuning however bayesian learning computational difficulty rigorous bayesian learning intractable many models its variational bayesian vb approximation prone suffer local paper overcome difficulty low rank subspace clustering providing exact global solver its efficient approximation extracts low dimensional structure data embedding samples low dimensional subspaces its variational bayesian variant shown good performance prove key property vb model highly thanks property optimization problem vb separated small only small number unknown variables exact global solver relies another key property stationary condition set polynomial equations method computational efficiency propose efficient approximate variant stationary condition polynomial equation single variable experimental results show usefulness approach 
thompson sampling dimensional exponential family bandits thompson sampling demonstrated many complex bandit models however theoretical guarantees available parametric multi armed bandit limited bernoulli case extend proving asymptotic optimality algorithm using prior dimensional exponential family bandits proof builds previous work makes extensive closed forms kullback leibler divergence fisher information prior available exponential family allow give finite time exponential concentration inequality posterior distributions exponential families may interest its moreover analysis covers distributions optimistic algorithm proposed including heavy tailed exponential families 
active learning probabilistic using maximum gibbs error criterion introduce objective function pool based bayesian active learning probabilistic objective function called policy gibbs error expected error rate random classifier drawn prior distribution examples adaptively selected active learning policy exact maximization policy gibbs error hard propose greedy strategy maximizes gibbs error iteration gibbs error instance expected error random classifier selected posterior label distribution instance apply maximum gibbs error criterion active learning scenarios non adaptive adaptive batch active learning scenario prove criterion achieves near maximal policy gibbs error constrained fixed budget practical implementations provide approximations maximum gibbs error criterion bayesian conditional random fields transductive naive bayes experimental results named entity recognition task classification task show maximum gibbs error criterion effective active learning criterion noisy models 
noise efficient multi task gaussian process inference structured residuals multi task prediction models widely being used regressors classification models information related tasks common models assume output tasks independent conditioned inputs propose multi task gaussian process approach model regressors task correlations residuals order accurately identify true regressors resulting gaussian model covariance term sum products efficient parameter inference sample prediction feasible synthetic examples applications prediction substantial benefits modeling structured noise compared established alternatives 
convex low rank loss matrices applications subset ranking losses design convex surrogate losses minimization entails consistency respect desired target loss important concept theory machine learning years give explicit construction convex squares type surrogate loss designed any multiclass learning problem target loss matrix low rank structure surrogate loss operates surrogate target space dimension rank target loss result design convex variety subset ranking problems target losses including expected rank utility mean average precision pairwise 
inverse density inverse problem equation approach address problem estimating ratio density function another density generally arbitrary function knowing approximating ratio needed various problems inference integration particular needs average function respect probability distribution given sample another referred importance sampling statistical inference closely related problem shift transfer learning various mcmc methods approach based problem estimating ratio inverse problem terms integral operator corresponding kernel reducing integral equation known problem kind formulation combined techniques regularization kernel methods leads principled kernel based framework constructing algorithms analyzing theoretically resulting family algorithms inverse regularized estimator flexible simple easy implement provide detailed theoretical analysis including concentration bounds convergence rates gaussian kernel densities defined d$ smooth dimensional sub manifolds euclidean space model selection unsupervised semi supervised inference generally difficult problem interestingly turns density ratio estimation setting samples distributions available simple completely unsupervised methods choosing parameters call model selection mechanism cross density cross validation finally show experimental results including applications classification shift framework 
adaptive multi column deep neural networks application robust image denoising stacked sparse denoising auto encoders recently shown successful removing noise corrupted images however denoising techniques robust variation noise types seen during training present multi column stacked sparse denoising autoencoder novel technique combining multiple multi column combining outputs eliminate need determine type noise alone its statistics time show good denoising performance achieved single system variety different noise types including ones seen training set additionally experimentally demonstrate efficacy denoising achieving mnist error rates images close uncorrupted images 
learning parameters directed undirected graphical models recently proposed algorithm learning parameters bayesian networks originally derived terms approximate inference meta network bayesian approach parameter estimation initial derivation discover place provided concrete context identifying its properties contrast em formal setting somewhat number concepts paper propose greatly simplified perspective general approach continuous optimization perspective several advantages makes results non trivial prove initially second design algorithms graphical models leading algorithm learning parameters markov networks derive algorithm paper show empirically sometimes learn better estimates complete data several times faster commonly used optimization methods conjugate gradient 
similarity component analysis measuring similarity crucial many learning tasks broader notion metric learning algorithms model example similarity arise process aggregating decisions multiple latent components latent component compares data its way focusing different subset features paper propose similarity component analysis probabilistic graphical model discovers latent components data latent component generates local similarity value computed its metric independently components final similarity measure obtained combining local similarity values noisy derive em based algorithm fitting model parameters similarity data pairwise comparisons validate model synthetic datasets discovers ground truth latent components apply classification task link prediction task tasks attains significantly better prediction competing methods moreover show exploratory analysis data gain insights data patterns hidden its latent components local similarity values 
approximate bayesian image interpretation using generative probabilistic programs idea computer vision bayesian inverse problem computer long history appealing difficult directly implement vision tasks approached complex bottom processing show possible write short simple probabilistic programs define flexible generative models automatically invert interpret real world images generative probabilistic programs consist stochastic scene based software stochastic likelihood model output data latent variables adjust fidelity likelihood model representations algorithms computer originally designed produce high quality images used deterministic highly approximate stochastic generative models formulation combines probabilistic programming computer approximate bayesian computation depends only general purpose automatic inference techniques describe applications sequences adversarially inferring road models camera images probabilistic programs present relies lines probabilistic code accurate approximately bayesian inferences real world images 
local privacy minimax bounds sharp rates probability estimation provide detailed study estimation probability distributions discrete continuous setting data private give sharp minimax rates convergence estimation locally private settings exhibiting fundamental privacy convergence rate providing tools allow movement privacy statistical efficiency continuum results classical work randomized response optimal way perform sampling maintaining privacy 
firing rate predictions optimal networks firing rates spiking network related neural input connectivity network important problem because firing rates important measures network activity study neural computation neural network dynamics however difficult problem because spiking mechanism individual neurons highly non linear individual neurons interact strongly connectivity develop technique calculating firing rates optimal networks particularly interesting networks because provide optimal spike based signal representation cortex spiking activity dynamic balance inhibition firing rates network dynamics algorithm optimizing signal representation identify algorithm firing rates finding solution algorithm firing rate calculation relates network firing rates directly network input connectivity function allows explain function underlying mechanism tuning curves variety systems 
manifold based similarity adaptation label propagation label propagation state art methods semi supervised learning estimates labels label information graph label propagation assumes data points nodes connected graph similar labels consequently label estimation heavily depends edge weights graph represent similarity node propose method graph capture manifold structure input features using edge weights parameterized similarity function approach edge weights represent similarity local reconstruction weight simultaneously being reasonable label propagation justification provide analytical considerations including interpretation cross validation propagation model feature space error analysis based low dimensional manifold model experimental results demonstrated effectiveness approach synthetic real datasets 
non uniform camera removal using adaptive sparse penalty typical camera standard uniform convolutional assumption part because problematic create unknown center point consequently successful blind deconvolution removing artifacts requires estimation varying non uniform operator using ideas bayesian inference convex analysis paper non uniform blind algorithm several desirable previously attributes underlying objective function includes adaptive penalty latent sharp image non uniform operator noise level together coupling allows penalty automatically adjust its shape based estimated degree local image structure regions large prominent edges discounted remaining regions modest edges therefore overall estimation process explicitly incorporating structure selection heuristics algorithm implemented using optimization strategy parameter simpler existing methods detailed theoretical analysis empirical validation real images serve validate proposed method 
near optimal sampling data matrices consider problem independently sampling non zero entries matrix order produce sparse minimizes large n$ matrices m$ example representing observations attributes give distributions exhibiting important properties closed forms probability sampling item computable minimal information regarding second allow sketching matrices non zeros presented algorithm arbitrary order stream computation non zero third resulting matrices only sparse non zero entries highly compressible lastly importantly mild assumptions distributions provably competitive optimal offline distribution note probabilities optimal offline distribution may complex functions entries matrix therefore regardless computational complexity optimal distribution impossible compute streaming model 
learning prune metric non metric spaces focus approximate nearest neighbor retrieval metric non metric spaces employ tree explore simple effective learning prune approaches density estimation sampling inequality methods evaluated using data sets metric euclidean non metric kl divergence distance functions conditions spaces tree applicable discussed tree learned compared recently proposed state art approaches multi probe locality sensitive hashing lsh permutation methods method competitive state art methods cases efficient rank approximation quality 
online learning episodic markovian decision processes relative entropy policy search study problem online learning finite episodic markov decision processes loss function allowed change episodes natural performance measure learning problem regret defined difference total loss best stationary policy total loss learner assume learner given access finite action space state space structure layers state transitions only possible layers describe variant recently proposed relative entropy policy search algorithm show its regret episodes t\log bandit setting t\log full information setting guarantees largely improve previously known results milder assumptions significantly improved general assumptions 
optimistic policy iteration natural actor critic view non optimality result approximate dynamic programming approaches reinforcement learning problem greedy value function methods value based policy gradient methods main result show important subset methodology fact limiting special case general formulation methodology optimistic policy iteration only greedy value function methods natural actor critic methods permits directly interpolate resulting continuum strength markov assumption policy improvement seen dual spirit continuum style algorithms policy evaluation second main result show substantial subset soft greedy value function approaches having potential avoid policy policy subset converge any optimal policy certain case consequently context approximations majority greedy value function methods suffer risk presence systematic sub optimality 
bayesian hierarchical community discovery propose efficient bayesian nonparametric model discovering hierarchical community structure social networks model tree structured mixture potentially exponentially many stochastic describe family greedy model selection algorithms worst case scales quadratically number vertices network independent number communities algorithms orders magnitude faster infinite relational model achieving comparable better accuracy 
bandits experts independence consider partial model multi armed bandits introduced main result regret directed model terms dominating independence numbers graph show undirected case learner achieve optimal regret graph before selecting action results shown using variants exp algorithm operating graph time efficient manner 
predictive pac learning process decompositions call stochastic process learnable admits generalization error zero probability any concept class finite dimension iid processes simplest example mixture learnable processes need learnable its generalization error need rate paper argue natural predictive pac condition past observations mixture component sample path definition only matches realistic learner demand allows several otherwise problems learning dependent data particular give novel pac generalization bound mixtures learnable processes generalization error worse mixture component provide mixtures regular mixing processes independent interest 
pass efficient unsupervised feature selection goal unsupervised feature selection identify small number important features represent data propose algorithm modification classical algorithm requires small number data improvements based ideas keeping track multiple features pass shown affect final selection algorithm selects exact features classical algorithm favorable numerical stability describe experiments real world datasets sometimes show improvements several orders magnitude classical algorithm results appear competitive recently proposed randomized algorithms terms pass efficiency run time hand randomized algorithms may produce better features cost small probability failure 
simultaneous alignment robust recovery low rank tensors work propose general method recovering low rank order tensors data unknown transformation corrupted arbitrary sparse errors since matrices tensor introduce auxiliary variables hard constraints augmented multiplier method improve computational efficiency introduce proximal gradient step alternating direction minimization method provided proof convergence linearized version problem inner loop overall algorithm simulations experiments show methods efficient effective previous work proposed method easily applied simultaneously multiple images videos context state art algorithms viewed special cases work only performs part function method 
bayesian mixture modelling inference based thompson sampling monte carlo tree search monte carlo tree search drawing great interest domain planning uncertainty particularly little domain knowledge available central problems trade exploration exploitation paper present novel bayesian mixture modelling inference based thompson sampling approach addressing proposed dirichlet mcts mcts algorithm represents uncertainty reward actions mcts search tree mixture normal distributions inferences bayesian settings choosing conjugate priors form combinations dirichlet distributions thompson sampling used select best action decision node experimental results show proposed algorithm achieved state art comparing popular algorithm context online planning general markov decision processes 
solving inverse problem markov chain partial observations markov chain convenient tool represent dynamics complex systems traffic social systems probabilistic transition takes place internal states markov chain characterized initial state probabilities state transition probability matrix traditional setting major goal properties markov chain probabilities known paper inverse version problem probabilities partial observations limited number states observations include frequency state rate reaching state another practical examples task include traffic monitoring systems need infer traffic every single link road network very limited number observation points formulate task regularized optimization problem probability functions efficiently solved using notion natural gradient using synthetic real world data sets including traffic monitoring data demonstrate effectiveness method 
locally adaptive bayesian multivariate time series modeling multivariate time series important allow time varying smoothness mean covariance process particular may certain time intervals exhibiting rapid changes others changes slow locally adaptive smoothness obtain inferences predictions smoothing time intervals smoothing times exhibiting slow variation lead predictive intervals substantially wide depending time propose continuous multivariate stochastic process time series having locally varying smoothness mean covariance matrix process constructed utilizing latent dictionary functions time given nested gaussian process priors linearly related observed data sparse mapping using differential equation representation usual computational obtaining mcmc online algorithms approximate bayesian inference performance simulations illustrated financial application 
mapping paradigm brain imaging neuroscience links brain activation maps behavior cognition studies due nature individual experiments based neural response small number stimuli link incomplete causal point view come function implied activation brain regions necessary combine wide exploration various brain functions statistical inference introduce methodology knowledge towards link observed brain activity corresponding function rely large corpus imaging studies predictive engine challenges studies corpus key elements contribute labeling tasks performed cognitive modeling long tail corpus knowledge approach demonstration predicting cognitive content completely brain images propose method predicts experimental different studies 
noise associative memories advances associative memory design structured pattern sets graph based inference algorithms allowed reliable learning recall exponential number patterns designs correct external errors recall assume neurons compute contrast highly variable neurons hippocampus olfactory cortex consider associative memories noisy internal computations analytically characterize performance long internal noise level specified threshold error probability recall phase small surprisingly show internal noise actually improves performance recall phase computational experiments additional support theoretical analysis work suggests functional benefit noisy neurons biological neuronal networks 
exact stable recovery pairwise interaction tensors tensor completion incomplete observations problem significant practical interest however unlikely exists efficient algorithm provable guarantee recover general tensor limited number observations paper study recovery algorithm pairwise interaction tensors recently gained considerable attention modeling multiple attribute data due its simplicity effectiveness specifically absence noise show exactly recover pairwise interaction tensor solving constrained convex program minimizes weighted sum nuclear norms matrices observations noisy cases prove error bounds constrained convex program recovering tensors experiments synthetic dataset demonstrate recovery performance algorithm agrees theory addition apply algorithm temporal collaborative filtering task obtain state art results 
bayesian entropy estimation binary spike train data using parametric prior knowledge entropy basic quantity information theory fundamental building block analysis neural codes estimating entropy discrete distribution samples important difficult problem received considerable attention statistics theoretical neuroscience however neural responses statistical structure generic entropy estimators fail exploit example existing bayesian entropy estimators naive assumption spike words equally likely priori makes allocation prior probability mass cases spikes sparse develop bayesian estimators entropy binary spike trains using priors designed flexibly exploit statistical structure simultaneously recorded spike responses define prior distributions spike words using mixtures dirichlet distributions centered simple parametric models parametric model captures high level statistical features data average spike count spike word allows posterior entropy rapidly standard estimators cases probability spiking strongly dirichlet distributions prior mass distributions far parametric model consistent estimates arbitrary distributions devise compact representation data prior allow computationally efficient implementations bayesian squares empirical bayes entropy estimators large numbers neurons apply estimators simulated real neural data show substantially outperform traditional methods 
perfect associative learning spike dependent plasticity extensions suggest theoretical concept highly relevant understanding networks spiking neurons brain known however computational power its variants plasticity mechanisms real synapses prove spike dependent plasticity having form synapses spike dependent plasticity shape inhibitory synapses sufficient original learning rule respective plasticity mechanisms post synaptic neurons show simple realistic dynamics efficiently learned proposed mechanism acquisition mappings spatio temporal activity patterns area brain spatio temporal spike patterns another region long term memories cortex results learning processes realistic networks spiking neurons depend interactions synaptic plasticity mechanisms dynamics neurons 
poisson graphical models undirected graphical models gaussian graphical models ising multinomial categorical graphical models widely used variety applications modeling distributions large number variables standard instances however ill suited modeling count data increasingly ubiquitous big data settings genomic sequencing data user data spatial data studies existing classes poisson graphical models arise joint distributions correspond poisson distributed node conditional distributions major drawback only model negative conditional dependencies given its infinite domain paper objective modify poisson graphical model distribution capture rich dependence structure count valued variables strategies poisson distribution show only leads valid joint distribution model however limitations types variables dependencies may modeled address propose novel variants poisson distribution corresponding joint graphical model distributions models provide class poisson graphical models capture positive negative conditional dependencies count valued variables learn graph structure model penalized neighborhood selection demonstrate performance methods learning simulated networks network sequencing data 
streaming variational bayes present bayes framework synchronous computation bayesian posterior framework makes streaming updates estimated posterior according user specified approximation function demonstrate usefulness framework variational bayes vb fitting latent dirichlet allocation model large scale document demonstrate advantages algorithm stochastic variational inference single pass setting designed streaming setting does apply 
gaussian process conditional copulas applications financial time series estimation dependencies multiple variables central problem analysis financial time series common approach express dependencies terms copula function typically copula function assumed constant may covariates large influence dependence structure data account bayesian framework estimation conditional copulas proposed framework parameters copula non linearly related arbitrary conditioning variables evaluate ability method predict time varying dependencies several observe consistent performance gains compared static copula models time varying copula methods 
extracting regions interest biological images convolutional sparse block coding biological cells similar replicated large many biological applications rely accurate identification cells locations image data develop generative model captures regularities present images elements different types formally model described convolutional sparse block coding inference variant convolutional matching pursuit adapted block based representations extend learning algorithm subspaces several principal vectors decomposition just good models little cross subspaces obtained learning blocks incrementally perform extensive experiments simulated images inference algorithm consistently recovers large proportion cells small number false positives fit convolutional model noisy photon images spiking neurons cortical show recovers locations flexibility block based representation reflected variability recovered 
approximate dynamic programming finally performs game popular video game widely used benchmark various optimization techniques including approximate dynamic programming algorithms close look literature game shows algorithms based approximating value function value function based performed poorly methods search directly space policies learning policy parameters using optimization black box cross entropy method achieved best reported results makes conjecture game good policies easier represent learn corresponding value functions order obtain good performance algorithms search policy space traditional ones search value function space paper conjecture applying algorithm called classification based modified policy iteration game extensive experimental results show time algorithm namely obtains best results reported literature small large results similar achieved method large uses considerably fewer samples call generative model game 
third order edge statistics curvature cortical connections field models used explain human grouping performance explain mean frequency long range connections cortical columns however fields essentially depend pairwise statistics edges natural scenes develop spectral pairwise statistics show significant higher order structure analysis using probabilistic spectral embedding curvature dependent components field challenge biological learning algorithms 
online pomdp planning regularization pomdps provide principled framework planning uncertainty computationally intractable due paper presents online search algorithm difficulties limiting search set sampled scenarios policies sampled scenarios using sparse partially observable tree sparsely sampled belief tree algorithm named regularized searches policy optimally size policy accuracy its value estimate obtained sampling give output sensitive performance bound policies derived show works small optimal policy exists give approximation experiments show strong results compared fastest online pomdp algorithms 
matrix completion any given set observations matrix completion problem aim recover unknown real matrix subset its entries problem comes many application areas received great deal attention context central approach problem output matrix possible complexity rank trace norm agrees partially specified matrix performance approach assumption revealed entries sampled randomly received considerable attention practice set revealed entries chosen random results apply therefore guarantees performance algorithm using present means obtain performance guarantees respect any set initial observations step remains matrix possible complexity agrees partially specified matrix give way interpret output algorithm finding probability distribution non revealed entries respect bound generalization error proven complex set revealed entries according certain measure better bound generalization error 
regression tree tuning streaming setting consider problem maintaining data structures partition based regression procedure setting training data sequentially time prove possible structure time any time step achieving nearly optimal regression rate terms unknown metric dimension finally prove regression lower bound independent given data size hence appropriate streaming setting 
dictionary learning estimating conditional distributions nonparametric estimation conditional distribution response given high dimensional features challenging problem important allow only mean variance shape response density change flexibly features massive dimensional propose dictionary learning model conditional response density convex combination dictionary densities densities used weights dependent path tree decomposition feature space fast graph partitioning algorithm applied obtain tree decomposition bayesian methods used adaptively prune average different sub trees soft probabilistic manner algorithm scales efficiently approximately features state art predictive performance demonstrated toy examples neuroscience applications including features 
dimension gradient present online learning algorithm extends gradient infinite dimensional spaces analysis shows algorithm implicitly able estimate norm unknown achieving regret bound order standard achievable knowing analysis introduce novel tools algorithms time varying regularizers local smoothness lower bound show algorithm optimal term linear lipschitz losses 
stochastic optimization pca study pca stochastic optimization problem propose novel stochastic approximation algorithm refer matrix stochastic gradient practical variant study method theoretically empirically 
hierarchical classification large scale study paper hierarchical classification strategies context large scale propose multiclass hierarchical data dependent bound generalization error classifiers large scale bound provides explanation several empirical results reported literature related performance hierarchical classifiers introduce another type bounds approximation error family classifiers derive features used meta classifier nodes prune large scale finally illustrate theoretical several experiments conducted widely used 
learning gaussian graphical models observed latent gaussian graphical models gauss markov random fields widely used many applications trade modeling capacity efficiency learning inference important research problem paper study family small feedback vertex sets set nodes removal breaks cycles exact inference computing marginal distributions partition function complexity using message passing algorithms size total number nodes propose efficient structure learning algorithms cases nodes observed useful modeling social networks nodes correspond small number high degree nodes rest networks modeled tree regardless maximum degree knowing full graph structure exactly compute maximum likelihood estimate known polynomial time unknown bounded size nodes latent variables structure learning equivalent decomposing inverse covariance matrix exactly approximately sum tree structured matrix low rank matrix incorporating efficient inference learning steps obtain learning algorithm using alternating low rank correction complexity iteration perform experiments using synthetic data real data delays demonstrate modeling capacity various sizes show empirically family size good balance modeling capacity efficiency 
visual concept learning combining machine vision bayesian generalization concept hierarchies learning visual concept small number positive examples significant challenge machine learning algorithms current methods typically fail appropriate level generalization concept hierarchy given set visual examples work cognitive science bayesian models generalization addresses challenge prior results assumed objects perfectly present algorithm learning visual concepts directly images using probabilistic predictions generated visual classifiers input bayesian generalization model existing challenge data tests paradigm collect available large scale dataset visual concept learning using imagenet hierarchy source possible concepts human provide ground truth labels image instance concept using paradigm similar used experiments studying word learning children compare performance system several baseline algorithms show significant advantage results combining visual classifiers ability identify appropriate level using bayesian generalization 
robust filters large classification tasks paper presents approach classification large number labels approach reduction binary classification label sets represented low dimensional binary vectors representation follows principle filters space efficient data structure originally designed approximate membership testing show naive application filters robust individual binary classifiers errors present approach exploits specific feature real world datasets number labels large many labels appear together provably robust sublinear training inference complexity respect number labels compares state art algorithms large scale datasets 
solving multi way matching problem permutation problem matching just different sets objects arises variety contexts including finding correspondence feature points multiple images computer vision present usually solved matching sets pairwise series contrast propose method permutation finds matchings jointly shot relaxation decomposition resulting algorithm computationally efficient demonstrate theoretical experimental results stable noise previous methods 
generalizing analytic shrinkage arbitrary covariance structures analytic shrinkage statistical technique offers fast alternative cross validation regularization covariance matrices appealing consistency properties show proof consistency implies bounds growth rates violated data prove consistency assumptions covariance structure therefore better match real world data addition propose extension analytic shrinkage orthogonal complement shrinkage adapts covariance structure finally demonstrate superior performance novel approach data domains finance optical character recognition neuroscience 
top regularization deep belief networks designing principled effective algorithm learning deep architectures challenging problem current approach involves training fully unsupervised learning followed strongly discriminative optimization suggest deep learning strategy gap resulting phase learning procedure propose implement scheme using method deep belief networks top information network constructed building blocks restricted boltzmann machines learned combining bottom top sampled signals global optimization procedure merges samples forward bottom pass top pass used experiments mnist dataset show improvements existing algorithms deep belief networks object recognition results dataset yield competitive results 
learning efficient random maximum posteriori predictors non decomposable loss functions work develop efficient methods learning random map predictors structured label problems particular construct posterior distributions perturbations stochastic gradient methods show every smooth posterior distribution suffice define smooth pac bayesian risk bound suitable gradient methods addition relate posterior distributions computational properties map predictors suggest multiplicative posteriors learn super modular potential functions specialized map predictors graph describe label augmented posterior models efficient map approximations arising linear program relaxations 
heterogeneous neighborhood based multi task local learning algorithms existing multi task local learning methods defined neighborhood consists data points only task paper different existing methods propose local learning methods multi task classification regression problems based heterogeneous neighborhood defined data points tasks specifically extend nearest neighbor classifier formulating decision function data point weighted voting neighbors tasks weights task specific defining regularizer task specific weight matrix approach symmetric regularized objective function proposed efficient coordinate descent method developed solve regression problems extend kernel regression multi task setting similar way classification case experiments toy data real world datasets demonstrate effectiveness proposed methods 
machine teaching bayesian learners exponential family knows learning goal wants design good training data machine propose optimal teaching framework learners employ bayesian models framework expressed optimization problem teaching examples balance future loss learner effort optimization problem general hard case learner employs conjugate exponential family models present approximate algorithm finding optimal teaching set algorithm optimizes sufficient statistics teaching examples give several examples illustrate framework 
scoring workers many control questions study problem estimating continuous quantities prices probabilities point using approach challenging aspect combining answers workers biases usually unknown highly diverse control items known answers used evaluate workers performance hence improve combined results target items unknown answers raises problem many control items total number items workers answer limited control items evaluates workers better leaves fewer resources target items direct interest give theoretical results problem different scenarios provide simple rule provide theoretical analysis accuracy different methods 
action image dataset inverse optimal control learn task specific visual human eye movements provide rich source information human visual processing complex task visual stimulus believed determine human eye movements fully understood development reliable dynamic eye movement prediction systems work makes contributions towards addressing problem complement largest challenging static computer vision datasets voc actions human eye movement annotations collected task constraints action context recognition dataset unique datasets images terms its large scale images task control action single image emphasis second introduce models automatically discover areas interest introduce novel dynamic consistency metrics based method automatically determine number spatial support addition locations based encodings show unconstrained read world stimuli task significant influence visual behavior finally leverage large scale dataset conjunction powerful machine learning techniques computer vision features introduce novel dynamic eye movement prediction methods learn task sensitive reward functions eye movement data efficiently integrate rewards plan future based inverse optimal control show propose methodology achieves state art modeling results 
determinantal point process latent variable model inhibition neural spiking data point processes popular models neural spiking behavior provide statistical distribution temporal sequences spikes reveal complexities underlying series recorded action however common neural point process models poisson process gamma process capture interactions correlations critical modeling populations neurons develop novel model based determinantal point process latent embeddings neurons effectively captures visualize complex inhibitory competitive interaction show model natural extension popular generalized linear model sets interacting neurons model extended incorporate gain control divisive normalization neural spiking based periodic phenomena applied neural spike recordings hippocampus see model captures inhibitory relationships classes neurons periodic known present data 
robust sparse principal component regression high dimensional elliptical model paper focus principal component regression its application high dimension non gaussian data major contributions low dimensions double asymptotic framework dimension sample size increase strength development minimax optimal principal component estimation time characterize potential advantage classical principal component regression square estimation gaussian model propose analyze robust sparse principal component regression high dimensional elliptically distributed data elliptical distribution semiparametric generalization gaussian including many known distributions multivariate gaussian rank gaussian cauchy logistic allows random vector heavy tailed tail dependence extra very suitable modeling finance biomedical imaging data elliptical model prove method estimate regression coefficients optimal parametric rate therefore good alternative gaussian based methods experiments synthetic real world data conducted illustrate empirical usefulness proposed method 
global map optimality combinatorial search area convex relaxation consider energy minimization undirected graphical models known map inference problem markov random fields combinatorial methods provably optimal integral solution problem big progress past decade typically cope large scale datasets hand large scale datasets typically defined sparse graphs convex relaxation methods linear programming relaxations provide good approximations integral solutions propose novel method combining combinatorial convex programming techniques obtain global solution initial combinatorial problem based information obtained solution convex relaxation method application combinatorial solver small fraction initial graphical model allows optimally solve big problems demonstrate power approach computer vision energy minimization benchmark 
near optimal detection graphs using extended detection activity graphs statistical problem arises many applications network detection activity monitoring social networks its wide applicability graph structured detection serves case study difficulty balancing computational complexity statistical power work develop generalized likelihood ratio determining connected region activation vertices graph gaussian noise because computationally infeasible provide relaxation called extended uses submodularity approximate intractable generalized likelihood ratio demonstrate connection maximum posteriori inference markov random fields provides time algorithm using network theory able control type error prove conditions risk consistent finally consider specific graph models nearest neighbor graphs random graphs show graphs results provide near optimal performance matching results known lower bounds 
demixing fast inference olfactory system faces difficult inference problem determine present based distributed activation its neurons derive neural implementations approximate inference algorithms used brain variational algorithm builds work based sampling importantly realistic prior distribution used past spike slab prior zero concentration mapping algorithms neural dynamics infer correct takes eliminate false positives behavioral level algorithms very similar predictions however different assumptions connectivity neural computations different predictions neural activity distinguishable experimentally provide insight mechanisms employed olfactory system because algorithms very different coding strategies provide insight networks represent probabilities 
learning multiple models regularized consider general problem multiple model learning data statistical algorithmic problem includes clustering multiple regression subspace clustering special cases common approach solving problems generalize lloyd algorithm clustering expectation maximization soft clustering however approach unfortunately sensitive outliers large noise single point may models propose different general formulation seeks model distribution data points weights regularized sufficiently spread robustness making assumptions class balance provide generalization bounds explain iterations may computed efficiently demonstrate robustness benefits approach experimental results prove important case clustering approach non trivial breakdown point guaranteed robust fixed adversarial unbounded outliers 
topic models uniqueness tensor decompositions structured sparsity latent representations very popular unsupervised feature learning years paper specify models identified given observable moments certain order consider probabilistic topic models regime number latent topics greatly size observed word general topic models establish generic identifiability constraint referred topic sufficient conditions identifiability involve novel set higher order expansion conditions topic word matrix population structure model set higher order expansion conditions allow models require existence perfect matching latent topics higher order observed words establish random structured topic models regime identifiability results allow general non distributions modeling topic proportions handle arbitrarily correlated topics framework identifiability results imply uniqueness class tensor decompositions structured sparsity class decompositions general decomposition 
distributed means clustering general paper provides algorithms distributed clustering popular center based objectives means algorithms provable guarantees improve communication complexity existing approaches following approach clustering reduce problem finding clustering low cost problem finding small size provide distributed method constructing global improves previous methods reducing communication complexity works general communication provide experimental evidence approach synthetic real data sets 
multi task bayesian optimization bayesian optimization recently proposed framework automatically tuning hyperparameters machine learning models shown yield state art performance impressive efficiency paper explore possible transfer knowledge gained previous optimizations tasks order optimal hyperparameter settings efficiently approach based extending multi task gaussian processes framework bayesian optimization show method significantly speeds optimization process compared standard single task approach propose straightforward extension algorithm order jointly minimize average error multiple tasks demonstrate used greatly speed cross validation lastly significant contribution adaptation recently proposed acquisition function entropy search cost sensitive multi task settings demonstrate utility acquisition function utilizing small dataset order explore hyperparameter settings large dataset algorithm dynamically chooses dataset query order yield information unit cost 
online learning dynamic parameters social networks paper addresses problem online learning dynamic setting consider social network individual observes private signal underlying state world neighbors time unlike many existing approaches underlying state dynamic evolves according geometric random view scenario optimization problem agents aim learn true state smallest possible loss based decomposition global loss function introduce update mechanisms generates estimate true state establish tight bound rate change underlying state track parameter bounded variance characterize explicit expressions steady state mean square deviation estimates truth individual observe only estimators recovers optimal objective function decomposition learning quality finally provide upper bound regret proposed methods measured average errors estimating parameter finite time 
graphical transformation belief propagation maximum weight matchings sized cycles max product popular distributed heuristic finding maximum posteriori map assignment joint probability distribution represented graphical model recently shown converges correct map assignment class loopy following common feature linear programming lp relaxation map problem tight gap unfortunately lp relaxation does general guarantee convergence correctness algorithm failure cases reverse engineering solution namely given tight lp design algorithm paper design algorithm maximum weight matching problem general graphs prove algorithm converges correct optimum respective lp relaxation may include associated non sized cycles tight significant part approach novel graph transformation designed force convergence theoretical result suggests efficient based heuristic problem consists making sequential modifications underlying experiments show heuristic performs traditional algorithms using lp solvers problems 
learning invariance linear functionals reproducing kernel hilbert space incorporating invariance information important many learning problems exploit invariances existing methods resort approximations lead expensive optimization problems semi definite programming rely separation tractability methods limit space functions non convex models paper propose framework learning reproducing kernel hilbert spaces using local invariances explicitly characterize behavior target function data instances invariances compactly encoded linear functionals value penalized loss function based establish formulation efficiently optimized convex program hold linear functionals required bounded show true variety commonly used invariances experiments learning unlabeled data transform invariances show proposed method yields better similar results compared state art 
approximate gaussian process inference drift function stochastic differential equations introduce nonparametric approach estimating drift functions systems stochastic differential equations incomplete observations state vector using gaussian process prior drift function state vector develop approximate em algorithm deal unobserved latent dynamics observations posterior states approximated piecewise linearized process map estimation drift sparse gaussian process regression 
distributed submodular maximization identifying representative elements massive data many large scale machine learning problems clustering non parametric learning kernel machines require selecting massive data set representative subset problems reduced maximizing submodular set function subject cardinality constraints classical approaches require access full data set large scale problems data impractical paper consider problem submodular function maximization distributed fashion develop simple stage protocol easily implemented using style computations theoretically analyze approach show certain natural conditions performance close impractical approach achieved extensive experiments demonstrate effectiveness approach several applications including sparse gaussian process inference tens millions examples using 
adaptive making online learning consider design strategies making order obtain guarantees typically requires very particular stochastic assumptions sequence price fluctuations asset question propose class spread based making strategies performance controlled worst case adversarial settings prove structural properties strategies allows design algorithm obtains low regret relative best strategy hindsight run set experiments showing favorable performance real world price data 
sample complexity subspace learning large number algorithms machine learning principal component analysis pca its non linear kernel extensions spectral embedding support estimation methods rely estimating linear subspace samples paper introduce general formulation problem derive novel learning error estimates results rely natural assumptions spectral properties covariance operator associated data distribution hold wide class metrics subspaces special cases discuss sharp error estimates reconstruction properties pca spectral support estimation key analysis operator theoretic approach broad applicability spectral learning methods 
spike train entropy rate estimation using hierarchical dirichlet process priors entropy rate amount stochastic process spiking neurons entropy rate upper bound rate spike train stimulus information large literature focused problem estimating entropy rate spike train data present bayes squares empirical bayesian entropy rate estimators binary spike trains using hierarchical dirichlet process priors estimator leverages fact entropy rate ergodic markov chain known transition probabilities analytically many stochastic processes non markovian approximated markov processes sufficient depth choosing appropriate depth markov model presents challenges due possibly long time dependencies short data sequences deeper model better account long time dependencies difficult infer limited data approach difficulty using hierarchical prior share statistical power markov chains different present fully bayesian empirical bayes entropy rate estimator based model demonstrate performance simulated real neural spike train data 
project discrete sampling universal hashing consider problem sampling probability distribution defined high dimensional discrete set specified instance graphical model propose sampling algorithm called based embedding set higher dimensional space randomly projected using universal functions lower dimensional subspace explored using combinatorial search methods scheme leverage fast combinatorial optimization tools unlike mcmc methods samples produced guaranteed arbitrarily small constant factor true probability distribution demonstrate using state art combinatorial search tools efficiently sample ising grids strong interactions software verification instances mcmc variational methods fail cases 
discriminative transfer learning tree based priors paper proposes way improving classification performance classes very training examples key idea discover classes similar transfer knowledge method classes tree hierarchy tree structure used impose generative prior classification parameters show priors combined discriminative models deep neural networks method benefits power discriminative training deep neural networks time using tree based generative priors classification parameters propose algorithm learning underlying tree structure gives model flexibility tune tree tree task being solved show model transfer knowledge related classes using fixed semantic trees moreover learn meaningful trees usually leading improved performance method achieves state art classification results image data set multimodal data set 
small variance asymptotics hidden markov models small variance asymptotics provide technique obtaining scalable combinatorial algorithms rich probabilistic models present small variance asymptotic analysis hidden markov model its infinite state bayesian nonparametric extension starting standard derive inference algorithm analogous means arises particular model tend zero analysis extended bayesian nonparametric case simple scalable flexible algorithm discrete state sequence data non fixed number states derive corresponding combinatorial objective functions arising analysis involve means term penalties based state transitions number states key property algorithms particularly nonparametric setting standard probabilistic inference algorithms lack scalability heavily dependent good initialization number results synthetic real data sets demonstrate advantages proposed framework 
convergence monte carlo tree search simultaneous games paper study monte carlo tree search mcts zero sum extensive form games perfect information simultaneous moves present general template mcts algorithms games various selection methods formally prove selection method consistent matrix game satisfies additional requirements exploration mcts algorithm converges approximate equilibrium extensive form game empirically evaluate using regret matching exp selection methods randomly generated worst case games confirm formal result show additional mcts variants converge approximate evaluated games 
devise deep visual semantic embedding model modern visual recognition systems limited ability scale large numbers object categories limitation part due increasing difficulty acquiring sufficient training data form labeled images number object categories grows remedy leverage data sources data train visual models predictions paper present deep visual semantic embedding model trained identify visual objects using labeled image data semantic information demonstrate model matches state art performance class imagenet object recognition challenge making reasonable errors show semantic information exploited predictions tens thousands image labels observed during training semantic knowledge improves zero shot predictions achieving rates thousands novel labels seen visual model 
reward mapping transfer long agents consider transfer knowledge previous tasks current task long bounded agents solve sequence mdps finite novel aspect transfer approach reward functions may build insight work optimal rewards problem guiding agent behavior reward functions task specifying reward function overcome computational bounds agent specifically good guidance reward functions learned previous tasks sequence incrementally train reward mapping function maps task specifying reward functions good initial guidance reward functions subsequent tasks demonstrate approach substantially improve agent performance relative approaches including approach policies 
minimax theory high dimensional gaussian mixtures sparse mean separation several investigated computationally statistically efficient methods learning gaussian mixtures precise minimax bounds statistical performance fundamental limits high dimensional settings understood paper provide precise information theoretic bounds clustering accuracy sample complexity learning mixture high dimensions small mean separation sparse subset relevant dimensions determine mean separation sample complexity only depends number relevant dimensions mean separation achieved simple computationally efficient procedure results provide step theoretical basis methods combine feature selection clustering 
predicting parameters deep learning demonstrate significant redundancy several deep learning models given only weight values feature possible accurately predict remaining values moreover show only parameter values predicted many need learned train several different architectures learning only small number weights predicting rest best case able predict weights network any accuracy 
estimating unseen improved estimators entropy properties recently showed class distributional properties includes practically relevant properties entropy number distinct elements distance metrics pairs distributions estimated given sublinear sized sample specifically given sample consisting independent draws any distribution distinct elements properties estimated accurately using sample size log propose novel modification approach show theoretically estimator optimal constant factors worst case instances practice performs variety estimation tasks variety natural distributions wide range parameters perhaps key step approach sample characterize unseen portion distribution goes tools good turing frequency estimation scheme estimates total probability mass unobserved portion distribution seek estimate shape unobserved portion distribution approach robust general theoretically principled expect may used component larger machine learning data analysis systems 
row column marginals reveal numerous datasets ranging group memberships social networks purchase represented binary matrices data sensitive data notably row column marginals viewed sensitive may analysis investigate data exploited inferences underlying matrix assuming generative model view input marginals constraints possible compute probability density function particular entries interest cells simultaneously generating implicitly sampling datasets satisfy input marginals result efficient algorithm running time equal time required standard sampling techniques generate single dataset experimental evaluation efficiency efficacy framework multiple settings 
real valued neural density estimator introduce model joint density estimation real valued vectors model density product dimensional modeled using mixture density networks shared parameters learns distributed representation data having tractable calculation densities tractable likelihood allows direct comparison methods training standard gradient based compare performance several datasets heterogeneous perceptual data finding outperforms mixture models case 
target algorithms infinite armed bandits bernoulli rewards consider infinite armed bandit problem bernoulli rewards mean rewards independent uniformly distributed rewards referred success failure respectively propose novel algorithm decision exploit any arm based successive namely total number successes failure respectively fixed parameter target algorithm achieves long term average regret large parameter known time horizon regret optimal strictly regret achieved best known algorithms results extended any mean reward distribution support contains unknown time numerical experiments show performance algorithm finite time 
priors priors major address linear inverse problems whereas regularization based approaches build estimators solutions penalized regression optimization problems bayesian estimators rely posterior distribution unknown given assumed family priors may radically different approaches results shown context additive white gaussian denoising bayesian conditional mean estimator always solution penalized regression problem contribution paper extend additive white gaussian denoising results general linear inverse problems colored gaussian noise second characterize conditions penalty function associated conditional mean estimator satisfy certain popular properties convexity smoothness sheds light tradeoff computational efficiency estimation accuracy sparse regularization draws connections bayesian estimation proximal optimization 
sparse overlapping sets lasso learning its application fmri analysis learning effective features useful task useful tasks group lasso standard method selecting common subset features paper interested restrictive form learning available features subsets according notion similarity features useful task similar necessarily features best suited tasks main contribution paper procedure called sparse overlapping sets lasso convex optimization automatically selects similar features related learning tasks error bounds derived its consistency established squared error loss particular motivated multi subject fmri studies functional activity using brain features experiments real synthetic data demonstrate advantages compared lasso group lasso 
sensor selection high dimensional gaussian trees consider sensor selection problem multivariate gaussian distributions only subset latent variables interest pairs vertices connected unique path graph show exist decompositions mutual information local information measures computed efficiently output message passing algorithms integrate decompositions computationally efficient greedy selector computational distributed nodes network experimental results demonstrate comparative efficiency algorithms sensor selection high dimensional distributions additionally derive online computable performance bound based relevant latent variable set valid augmentation exists applicable any distribution 
sequential transfer multi armed bandit finite set models learning prior tasks transferring improve future performance critical building learning agents results supervised reinforcement learning show transfer may significantly improve learning performance literature transfer focused batch learning tasks paper study problem sequential transfer online learning notably multi arm bandit framework objective minimize cumulative regret sequence tasks incrementally transferring knowledge prior tasks introduce novel bandit algorithm based method moments approach estimation possible tasks derive regret bounds 
active learning many practical applications active learning cost effective labels large batches time because cost labeling large batch examples once sublinear number examples batch work study label complexity active learning algorithms labels given number batches tradeoff total number queries number allowed additionally study total cost sufficient learning abstract notion cost labels given number examples once particular sublinear cost functions desirable labels large batches may increase total number labels reduces total cost required learning 
contrastive learning using spectral methods many natural settings analysis goal characterize single data set understand difference set observations another example given background corpus news together particular may want topic model explains word patterns specific another example comes biological signals may collected different regions wants model captures differential statistics observed regions paper notion contrastive learning mixture models develops spectral algorithms inferring mixture components specific data set background data set method builds moment based estimators tensor decompositions latent variable models intuitive feature using background data statistics modify moments estimated data key advantage method background data need only modeled important background complex noisy interest method demonstrated applications contrastive topic modeling genomic sequence analysis 
message passing inference chemical networks work molecular programming explored computational including neural networks linear systems future enable sense control world molecular scale just critical learn environment reason uncertainty small scale systems typically modeled chemical networks work develop procedure arbitrary probabilistic graphical models represented factor graphs discrete random variables chemical networks implement inference particular show based sum product message passing implemented terms chemical represent probabilities show steady state concentration correspond marginal distributions random variables graph validate results simulations standard sum product inference procedure yields exact results tree structured graphs approximate solutions loopy graphs 
eluder dimension sample complexity optimistic exploration paper considers sample complexity multi armed bandit dependencies successful algorithms problem principle optimism face uncertainty guide exploration example class upper confidence bound algorithms work shown simple posterior sampling algorithm sometimes called thompson sampling shares close theoretical connection optimistic approaches paper develop regret bound holds classes algorithms bound applies broadly specialized many model classes depends notion refer eluder dimension measures degree dependence action rewards compared algorithm regret bounds specific model classes general bound matches best available linear models stronger best available generalized linear models 
learning word embeddings efficiently noise contrastive estimation continuous valued word embeddings learned neural language models recently shown capture semantic syntactic information words very setting performance several word similarity tasks best results obtained learning high dimensional embeddings very large quantities data makes scalability training method critical factor propose simple scalable approach learning word embeddings based training log bilinear models noise contrastive estimation approach simpler faster produces better results current state art method achieve results comparable best ones reported obtained cluster using times data order magnitude computing time investigate several model types embeddings learned simpler models perform learned complex ones 
sparse inverse covariance estimation calibration propose semiparametric procedure estimating high dimensional sparse inverse covariance matrix method named applicable elliptical family computationally develop efficient dual inexact iterative projection algorithm based alternating direction method multipliers admm theoretically prove estimator achieves parametric rate convergence parameter estimation model selection moreover estimating column inverse covariance matrix only asymptotically tuning achieves improved finite sample performance present numerical simulations support theory real data example illustrate effectiveness proposed estimator 
stochastic majorization minimization algorithms large scale optimization majorization minimization algorithms consist iteratively minimizing surrogate objective function because its simplicity its wide applicability principle very popular statistics signal processing paper principle scalable introduce stochastic majorization minimization scheme able deal large scale possibly infinite data sets applied convex optimization problems suitable assumptions show achieves expected convergence rate iterations strongly convex functions equally important scheme converges stationary points large class non convex problems develop several efficient algorithms based framework propose stochastic proximal gradient method experimentally matches state art solvers large scale logistic regression second develop online dc programming algorithm non convex sparse estimation finally demonstrate effectiveness technique solving large scale structured matrix factorization problems 
distances computation optimal transport optimal distances fundamental family parameterized distances histograms probability simplex despite appealing theoretical properties performance intuitive formulation computation involves resolution linear program cost prohibitive histograms dimension exceeds hundreds propose work family optimal distances look problems maximum entropy perspective smooth classical optimal problem regularization term show resulting optimum distance computed matrix scaling algorithm speed several orders magnitude faster solvers report improved performance mnist benchmark problem competing distances 
speedup matrix completion side information application multi label learning standard matrix completion theory required observed entries perfectly recover low rank matrix size n$ leading large number observations large many real tasks side information addition observed entries available work develop novel theory matrix completion explicitly explore side information reduce requirement number observed entries show appropriate conditions side information matrices number observed entries needed perfect recovery matrix dramatically reduced demonstrate effectiveness proposed approach matrix completion transductive incomplete multi label learning 
compute local neurons common biological neural networks apply concept gradient based trained artificial multilayer competing linear units tend outperform non competing nonlinear units avoid training sets change time 
fast determinantal point process sampling application clustering determinantal point process dpp gained popularity modeling sets diverse items dpp probability choosing particular set items determinant positive definite matrix similarity items however computing determinant requires time number items hence impractical large sets paper address problem constructing rapidly mixing markov chain acquire sample given dpp sub time addition show framework extended sampling cardinality constrained dpps application show sampling algorithm used provide fast heuristic determining number clusters resulting better clustering 
information theoretic lower bounds distributed statistical estimation communication constraints establish minimax risk lower bounds distributed statistical estimation given budget total number may lower bounds turn reveal minimum amount communication required any procedure achieve classical optimal rate statistical estimation study classes machines messages independently lower bounds established variety problems estimating mean population estimating parameters linear regression binary classification 
projected natural actor critic natural actor popular class policy search algorithms finding locally optimal policies markov decision processes paper address drawback natural actor limits real world applicability lack guarantees present principled algorithm performing natural gradient descent constrained domain context reinforcement learning allows natural actor critic algorithms guaranteed known safe region policy space class constrained natural actor critic algorithms call projected natural actor elucidate relationship natural gradient descent mirror descent 
option adversary black minimax optimal consider popular problem finance option online learning game nature black option model continuously risk option trading underlying asset assuming asset price according geometric motion consider worst case model nature chooses sequence price fluctuations cumulative quadratic constraint sequence hedging decisions main result show value proposed game regret hedging strategy converges black option price significantly weaker assumptions previous work instance allow large asset price show black hedging strategy near optimal non stochastic framework 
discovering hidden variables noisy networks using tests give polynomial time algorithm provably learning structure parameters bipartite noisy bayesian networks binary variables top layer completely hidden unsupervised learning models form discrete factor analysis enabling discovery hidden variables causal relationships observed data obtain efficient learning algorithm family bayesian networks call learnable meaning every latent variable children any common show existence allows identify latent variable learn parameters involving latent variable underlying algorithm techniques structure learning determine set binary variables coupled conditional mutual information learn parameters show learned latent variables model create coupled substantially class structures learn finally give proof polynomial sample complexity learning algorithm experimentally compare variational em 
error minimizing estimates universal wise error bounds low rank matrix completion propose general framework reconstructing denoising single entries incomplete noisy entries describe effective algorithms deciding reconstructing denoising priori bounds error individually noiseless case algorithm exact rank matrices algorithm fast admits highly parallel implementation produces error minimizing estimate qualitatively close theoretical state art nuclear norm methods 
learning local statistics optical flow motivated progress natural image statistics available datasets ground truth optical flow learn local statistics optical flow rigorously compare learned model prior models assumed computer vision optical flow algorithms gaussian mixture model components provides significantly better model local flow statistics compared commonly used models investigate source success show related explicit representation flow boundaries learn model jointly models local intensity pattern local optical flow assumptions computer vision model learns flow boundaries likely intensity boundaries however evaluated large dataset dependency very weak benefit conditioning flow estimation local intensity pattern marginal 
aggregating optimistic planning trees solving markov decision processes paper addresses problem online planning markov decision processes using only generative model propose algorithm based construction forest single state planning trees every explored state action tree contains exactly state drawn generative model trees using planning algorithm follows optimism face uncertainty principle assuming favorable outcome absence information decision making step algorithm individual trees combined discuss approach prove proposed algorithm consistent empirically show performs better related algorithm additionally assumes knowledge transition distributions 
robust learning low dimensional dynamics large neural ensembles recordings large populations neurons possible search low dimensional dynamics finding dynamics requires models account biophysical constraints fit efficiently present approach dimensionality reduction neural data convex does strong assumptions dynamics does require averaging many trials complex statistical models combine local global results combined spectral methods learn dynamical systems models basic method seen extension pca exponential family using nuclear norm minimization evaluate effectiveness method using exact decomposition divergence analogous variance explained pca show model data parameters latent linear dynamical systems recovered dynamics stationary recover true latent subspace demonstrate extension nuclear norm minimization separate sparse local connections global latent dynamics finally demonstrate improved prediction real neural data motor cortex compared fitting linear dynamical models nuclear norm smoothing 
estimation bias multi armed bandit algorithms search advertising search advertising search engine needs select advertisements display formulated instance online learning partial feedback known stochastic multi armed bandit mab problem paper show naive application mab algorithms search advertising advertisement selection produce sample selection bias search engine decreasing expected revenue largest bias increasing game theoretic player regret propose simple bias correction methods benefits search engine 
action eye eye driven model spatio temporal action propose weakly supervised structured learning approach recognition spatio temporal actions video part proposed approach develop generalization max path search algorithm allows efficiently search structured space multiple spatio temporal paths allowing incorporate context information model using spatial annotations form bounding guide latent model during training utilize human data form weak signal achieved incorporating classification structured loss latent svm learning framework experiments challenging benchmark dataset show model accurate terms classification achieves state art results addition show model produce top maps conditioned classification label localized latent paths 
lasso learning sparse bayesian network structure continuous variables address problem learning sparse bayesian network structure continuous variables high dimensional space constraint estimated bayesian network structure directed acyclic graph dag makes problem challenging because search space network structures previous methods based stage approach search space stage searches network structure satisfies dag constraint second stage approach effective low dimensional setting difficult ensure correct network structure stage high dimensional setting paper propose single stage method called lasso recovers optimal sparse bayesian network structure solving single optimization problem search algorithm uses lasso its scoring system approach substantially improves computational efficiency known exact methods based dynamic programming present heuristic scheme improves efficiency lasso significantly quality solutions demonstrate benchmark bayesian networks real data 
total variation learning allow encode higher order relationships data very flexible modeling tool current learning methods based approximations graphs tensor methods only applicable special conditions paper present learning framework fully uses structure key element family regularization functionals based total variation 
submodular optimization submodular cover submodular constraints investigate optimization problems minimizing submodular function subject submodular lower bound constraint submodular cover maximizing submodular function subject submodular upper bound constraint submodular motivated number real world applications machine learning including sensor data subset selection require maximizing certain submodular function coverage simultaneously minimizing another cost problems minimizing difference submodular functions worst case show however phrasing problems constrained optimization natural many applications achieve number bounded approximation guarantees show problems closely related approximation algorithm solving used obtain approximation guarantee provide hardness results problems showing approximation factors tight log factors finally empirically demonstrate performance good scalability properties algorithms 
scalable inference logistic normal topic models logistic normal topic models effectively discover correlation structures latent topics however inference remains challenge because non logistic normal prior multinomial topic mixing proportions existing algorithms restricting mean field assumptions scalable large scale applications paper presents partially gibbs sampling algorithm approaches provably correct distribution ideas data augmentation improve time efficiency present parallel implementation deal large scale applications learn correlation structures thousands topics millions documents extensive empirical results demonstrate promise 
spectral methods neural using generalized quadratic models describe set fast tractable methods characterizing neural responses high dimensional sensory stimuli using model refer generalized quadratic model consists low rank quadratic form followed point exponential family noise quadratic form characterizes neuron stimulus terms set linear receptive fields followed quadratic combination rule maps output desired response range special cases include nd order model elliptical linear nonlinear poisson model show canonical form spectral decomposition response weighted moments yields approximate maximum likelihood estimators quantity called expected log likelihood resulting theory generalizes moment based estimators spike triggered covariance gaussian noise case provides closed form estimators large class non gaussian stimulus distributions show estimators fast provide highly accurate estimates far lower computational cost full maximum likelihood moreover provides natural framework combining multi dimensional stimulus sensitivity spike history dependencies single model show applications spiking data using recordings potential recordings spike trains 
universal models binary spike patterns using centered dirichlet processes probabilistic models binary spike patterns provide powerful tool understanding statistical dependencies large scale neural recordings maximum entropy models seek explain dependencies terms low order interactions neurons remarkable success modeling patterns particularly small groups neurons however models computationally intractable large populations low order models shown datasets overcome limitations propose family universal models binary spike patterns ability model arbitrary distributions m$ binary patterns construct universal models using dirichlet process centered parametric base measure naturally combines flexibility histogram parametric model derive computationally efficient inference methods using bernoulli cascade logistic base measures scale large populations establish condition equivalence cascade logistic nd order ising model making cascade logistic reasonable choice base measure universal model illustrate performance models using neural data 
synthesizing robust incomplete domain models current assume complete domain models focus generating correct unfortunately domain modeling error prone task real world agents plan incomplete domain models domain experts guarantee able model providing annotations parts domain model may incomplete cases goal synthesize robust respect any known domain paper introduce annotations knowledge domain formalize notion plan robustness respect incomplete domain model show approach problem finding robust probabilistic planning problem present experimental results probabilistic 
integrated non factorized variational inference present non factorized variational method full posterior inference bayesian hierarchical models goal capturing posterior variable dependencies efficient possibly parallel computation approach unifies integrated nested approximation variational framework proposed method applicable challenging scenarios typically assumed bayesian lasso characterized non norm arising independent priors derive upper bound kullback leibler divergence yields fast closed form solution optimization method reliable analytic alternative markov chain monte carlo mcmc results tighter evidence lower bound mean field variational bayes vb method 
auxiliary variable exact hamiltonian monte carlo samplers binary distributions present approach sample generic binary distributions based exact hamiltonian monte carlo algorithm applied piecewise continuous augmentation binary distribution interest extension idea distributions mixtures binary continuous variables allows sample posteriors linear regression models spike slab priors parameters illustrate advantages algorithms several examples outperform metropolis gibbs samplers 
symbolic policy iteration factored action mdps address scalability symbolic planning uncertainty factored states actions prior work focused exclusively factored states factored actions value iteration compared policy iteration pi ﬁrst contribution novel method symbolic policy application constraints used yield efﬁcient symbolic pi factored action spaces approach improves scalability cases naive handling policy constraints comes its scalability issues leads second main contribution symbolic policy iteration novel convergent core idea symbolic procedure applies policy constraints only reduce space time complexity update otherwise performs full bellman automatically state give memory bounded version algorithm allowing space time tradeoff empirical results show signiﬁcantly improved scalability state art 
online learning markov decision processes adversarially chosen transition probability distributions study problem online learning markov decision processes mdps transition distributions loss functions chosen adversary present algorithm mixing assumption achieves t\log regret respect comparison set policies regret independent size state action spaces expectations sample paths computed efficiently comparison set polynomial size algorithm efficient consider episodic adversarial online shortest path problem episode adversary may choose weighted directed acyclic graph identified start node goal learning algorithm choose path minimizes loss start node episode loss function given weights edges revealed learning algorithm goal minimize regret respect fixed policy selecting paths problem special case online mdp problem randomly chosen graphs adversarial losses problem efficiently solved show efficiently solved adversarial graphs randomly chosen losses graphs losses adversarially chosen present efficient algorithm regret scales linearly number distinct graphs finally show designing efficient algorithms adversarial online shortest path problem hence adversarial mdp problem hard learning parity noise notoriously difficult problem used design efficient cryptographic schemes 
flexible sampling discrete data correlations marginal distributions learning joint dependence discrete variables fundamental problem machine learning many applications including prediction clustering dimensionality reduction recently framework copula modeling gained popularity due its modular joint distributions properties copulas provide combining flexible models univariate marginal distributions parametric families suitable potentially high dimensional dependence structures radically extended rank likelihood approach learning marginal models completely information learning task hand standard dimensionality reduction problems copula parameter estimation main idea represent data observable rank statistics any information marginals inference typically bayesian framework gaussian copulas complicated fact implies sampling space number constraints increase quadratically number data points result slow mixing using gibbs sampling present efficient algorithm based advances constrained hamiltonian markov chain monte carlo simple implement does require paying quadratic cost sample size 
shot learning causal process people learn visual class just example machine learning algorithms typically require hundreds thousands examples tackle problems present hierarchical bayesian model based causality learn wide range natural simple visual concepts generalizing human ways just image evaluated performance challenging shot classification task model achieved human level error rate substantially outperforming deep learning models used visual turing show model produces human performance tasks including generating examples 
statistical analysis coupled time series kernel cross spectral density operators many applications require analysis complex interactions time series interactions non linear involve vector valued complex data structures graphs provide general framework statistical analysis interactions random variables sampled stationary time series arbitrary objects achieve goal analyze properties kernel cross spectral density operator induced positive definite kernels arbitrary input domains framework enables develop independence time series similarity measure compare different types coupling performance compared using assumptions showing improvement terms detection errors approach testing dependency complex dynamical systems finally approach characterize complex interactions neural time series 
fast algorithms gaussian noise invariant independent component analysis performance standard algorithms independent component analysis quickly addition gaussian noise partially due common step typically consists applying principal component analysis pca components identity covariance invariant gaussian noise paper develop practical algorithm independent component analysis provably invariant gaussian noise main contributions work follows develop implement efficient version gaussian noise invariant algorithm using functions propose very simple efficient fixed point ica gradient iteration ica algorithm usual pca based noiseless case algorithm based special form gradient iteration different gradient descent provide analysis algorithm demonstrating fast convergence following basic properties present number experimental comparisons existing methods showing superior results noisy data very competitive performance noiseless case 
deep neural networks object detection deep neural networks recently shown performance task image classification paper step address problem object detection only classifying precisely objects various classes using present simple powerful formulation object detection regression object masks define multi scale inference procedure able produce high resolution object detection low cost network applications approach achieves state art performance pascal voc 
geometric positive definite matrices elliptically distributions positive definite matrices statistics machine learning paper develop geometric globally certain nonconvex loss functions arising modelling data elliptically distributions exploit remarkable structure convex positive definite matrices allows uncover hidden geodesic convexity objective functions nonconvex ordinary euclidean sense going manifold convexity show metric properties matrices exploited globally several log likelihoods geodesic convex present key results geometric structure obtain efficient fixed point algorithms corresponding objective functions knowledge general results geometric matrices known far experiments reveal benefits approach avoids any computations makes very competitive 
sign cauchy projections square kernel method cauchy random projections popular computing distance high dimension paper propose only projected data show probability collision accurately approximated function square similarity popular measure nonnegative data features generated histograms common vision applications experiments confirm method sign cauchy random projections promising large scale learning applications furthermore extend idea sign stable random projections derive bound collision probability 
relevance topic model social group activity recognition social group activity recognition videos challenging task due semantic gap class labels low level visual features lack labeled training data tackle problem propose relevance topic model jointly learning meaningful mid level representations bag words video representations classifier sparse weights approach sparse bayesian learning incorporated undirected topic model replicated softmax discover topics relevant video classes suitable prediction linear units increase expressive power topics explain better video data containing complex variational inference tractable proposed model efficient variational em algorithm presented model parameter estimation inference experimental results social activity attribute dataset show model achieves state art performance outperforms supervised topic model terms classification accuracy particularly case very small number labeled training videos 
learning structures paper study following variant learning called learning problem structures given set structures set structures cluster given structures total cost dissimilarity minimized learning core problem machine learning wide range applications many areas existing results problem mainly focused graph domain paper present algorithm learning multiple prototypes structures result based number insights structures alignment clustering reconstruction practically efficient quality guarantee validate approach using type data sets random data biological data experiments suggest approach effectively learn prototypes types data 
restricting nonparametric distributions distributions matrices infinitely many columns useful constructing nonparametric latent variable models however distribution implied models number features data point may poorly suited many modeling tasks paper propose class nonparametric priors obtained restricting domain existing models models allow specify distribution number features data point achieve better performance data sets number features modeled original distribution 
bayes myopic planning human learning decision making bandit setting humans achieve long term goals uncertain environment repeated trials noisy observations important problem cognitive science investigate behavior context multi armed bandit task compare human behavior variety models computational complexity result shows subjects basis best bayesian iterative learning model combination partially myopic decision policy known knowledge gradient model accounts subjects choice better number previously proposed models including optimal bayesian learning risk minimization epsilon greedy win shift benefit being performance optimal bayesian model heuristic models computational complexity significantly complex optimal model results constitute theoretical understanding humans exploration exploitation noisy known environment 
probabilistic movement movement established approach representing modular movement generators many state art learning successes based due compact representation inherently continuous high dimensional movements major goal learning combine multiple building blocks modular control architecture solve complex tasks effect representation allow adapting task variables multiple parallel present probabilistic formulation concept maintains distribution trajectories probabilistic approach allows derivation operations essential aforementioned properties framework order trajectory distribution movement control analytically derive stochastic feedback controller given trajectory distribution evaluate compare approach existing methods several simulated real scenarios 
policy integrating human feedback reinforcement learning long term goal reinforcement learning incorporate non expert human feedback solve complex tasks state art methods approached problem mapping human information reward value signals indicate preferences compute necessary control policy paper argue effective human feedback policy introduce bayesian approach attempts maximize information gained human feedback utilizing direct labels policy compare state art approaches highlight scenarios outperforms importantly robust inconsistent human feedback 
dynamical systems tensor time series many scientific data occur sequences called tensors hidden evolving trends data extracted preserving tensor model traditionally used linear dynamical system lds treats observation time vector paper propose dynamical system modeling tensor time series expectation maximization em algorithm estimate parameters models time tensor time series projection corresponding sequence latent low dimensional tensors compared lds equal number parameters achieves higher prediction accuracy marginal likelihood simulated real datasets 
deep content based music recommendation automatic music recommendation become increasingly relevant problem years since music systems rely collaborative filtering however approach suffers start problem usage data available effective paper propose latent factor model recommendation predict latent factors music audio obtained usage data compare traditional approach using bag words representation audio signals deep convolutional neural networks evaluate predictions qualitatively dataset show using predicted latent factors produces sensible despite fact large semantic gap characteristics affect user preference corresponding audio signal show advances deep learning very music recommendation setting deep convolutional neural networks significantly outperforming traditional approach 
stability based validation procedure differentially private machine learning differential privacy motivated definition privacy gained considerable attention algorithms machine learning data mining communities work differentially private machine learning algorithms major barrier achieving differential privacy practical machine learning applications lack effective procedure differentially private parameter tuning determining parameter value size histogram regularization parameter suitable particular application paper introduce generic validation procedure differentially private machine learning algorithms apply certain stability condition holds training algorithm validation performance metric training data size privacy budget used training procedure independent number parameter values searched apply generic procedure fundamental tasks statistics machine learning training regularized linear classifier building histogram density estimator result differentially private solutions problems 
capacity strong patterns model cognitive prototypes solve mean field equations stochastic network noise presence strong multiply patterns solution obtain storage capacity network result provides time rigorous solution mean field equations standard model contrast mathematically technique used derivation show critical stability strong pattern equal its degree sum degrees patterns compared network size case single strong pattern presence simple patterns ratio number patterns network size positive constant obtain distribution patterns mean field storage capacity strong pattern exceeds simple pattern multiplicative factor equal square degree strong pattern square property provides justification using strong patterns model types prototypes 
projection selection near optimal convex relaxation sparse pca propose novel convex relaxation sparse principal subspace estimation based convex rank projection matrices convex problem solved efficiently using alternating direction method multipliers admm establish near optimal convergence rate terms sparsity ambient dimension sample size estimation principal subspace general covariance matrix assuming spiked covariance model special case result implies near optimality solution rank provide general theoretical framework analyzing statistical properties method arbitrary input matrices extends applicability provable guarantees wide settings demonstrate application correlation matrices component analysis 
cluster trees manifolds investigate problem estimating cluster tree density near smooth dimensional manifold embedded d$ study nearest neighbor based algorithm recently proposed chaudhuri mild assumptions obtain rates convergence depend only ambient dimension provide sample complexity lower bound natural class clustering algorithms dimensional 
bayesian inference low rank spatiotemporal neural receptive fields receptive field sensory neuron describes neuron integrates sensory stimuli time space typical experiments spatiotemporal stimuli very high dimensional due large number coefficients needed specify integration time space estimating coefficients small amounts data poses variety challenging statistical computational problems address challenges developing bayesian reduced rank regression methods estimation corresponds modeling sum several space time separable rank filters proves accurate neurons strongly space time approach substantially reduces number parameters needed specify examples consider substantial benefits statistical power computational efficiency particular introduce novel prior low rank using restriction matrix normal prior manifold low rank matrices localized prior row column obtain sparse smooth localized estimates spatial temporal components develop methods inference resulting hierarchical model fully bayesian method using gibbs sampling fast approximate method employs alternating coordinate ascent conditional marginal likelihood develop methods gaussian poisson noise models show low rank estimates substantially outperform full rank estimates accuracy speed using neural data 
adaptive submodular maximization bandit setting maximization submodular functions wide applications machine learning artificial adaptive submodular maximization traditionally studied assumption model world expected gain choosing item given previously selected items states known paper study scenario expected gain initially unknown learned interacting repeatedly optimized function propose efficient algorithm solving problem prove its expected cumulative regret increases time regret bound captures inherent property submodular maximization earlier costly ones refer approach optimistic adaptive submodular maximization because exploration exploitation based optimism face uncertainty principle evaluate method preference problem show non trivial step policies learned just interactions problem 
generalized method moments rank paper propose class efficient generalized method moments algorithms computing parameters model data consists full alternatives technique based full pairwise comparisons computing parameters satisfy set generalized moment conditions identify conditions output unique identify general class consistent inconsistent show theory experiments algorithms run significantly faster classical maximization algorithm achieving competitive statistical efficiency 
analyzing parallel gaussian gibbs sampling sampling inference methods computationally difficult scale many models part because global dependencies reduce parallel computation strict conditional independence structure variables standard gibbs sampling theory requires sample updates performed sequentially dependence variables strong empirical work shown models sampled effectively going simply running gibbs updates parallel only periodic global communication successes limitations strategy understood step towards understanding study gibbs sampling strategy context gaussian distributions develop framework provides convergence conditions error bounds simple proofs connections methods numerical linear particular show gaussian precision matrix generalized any gibbs sampler any update schedule allocation variables processors yields stable sampling process correct sample mean 
minimax optimal algorithms unconstrained linear optimization design analyze minimax optimal algorithms online linear optimization games player choice unconstrained player minimize regret difference loss loss post hoc benchmark strategy standard benchmark loss best strategy chosen bounded set whereas consider broad range benchmark functions consider problem sequential multi stage zero sum game give analysis minimax behavior game providing value game player adversary optimal strategy show objects computed efficiently certain selecting appropriate benchmark construct novel hedging strategy unconstrained game 
nearly optimal algorithms private online learning full information bandit settings provide general technique making online learning algorithms differentially private full information bandit settings technique applies algorithms aim minimize convex loss function sum smaller convex loss terms data point modify popular mirror descent approach variant called follow approximate technique leads algorithms private online learning bandit setting full information setting algorithms improve regret bounds previous work many cases algorithms settings matching dependence input length optimal regret bounds logarithmic factors algorithms require logarithmic space update time 
curvature optimal algorithms learning minimizing submodular functions investigate related important problems connected machine learning namely approximating submodular function learning submodular function pac setting constrained minimization submodular functions problems provide improved bounds depend submodular function improve previously known best results problems function property true many real world submodular functions problems obtain bounds generic black box transformation potentially work any algorithm case submodular minimization propose framework algorithms depend choosing appropriate surrogate submodular function cases provide matching lower bounds improved curvature dependent bounds shown submodular maximization existence similar improved bounds aforementioned problems open question paper showing notion curvature provides improved results empirical experiments add support claims 
optimality active learning gaussian random fields common classifier unlabeled nodes undirected graphs uses label propagation labeled nodes equivalent harmonic predictor gaussian random fields active learning commonly used optimality criterion queries nodes reduce regression loss optimality satisfies submodularity property showing greedy reduction produces globally optimal solution however loss may characterise true nature loss classification problems may best choice active learning consider criterion call optimality queries node minimizes sum elements predictive covariance optimality directly optimizes risk problem determine proportion nodes class paper extend submodularity guarantees optimality optimality using properties specific show satisfy condition addition conditional independence markov random fields optimality real world graphs synthetic real data show outperforms optimality related methods classification 
learning kernels using local complexity notion local complexity design algorithms learning kernels algorithms thereby benefit learning bounds based notion certain general conditions guarantee faster convergence rate devise learning kernel algorithms based convex optimization problem give efficient solution using existing learning kernel techniques another formulated dc programming problem describe solution report results experiments algorithms binary multi class classification tasks 
distributions averaging moments many powerful monte carlo techniques estimating partition functions importance sampling based sampling sequence intermediate distributions interpolate tractable initial distribution intractable target distribution near universal practice geometric initial target distributions alternative paths perform substantially better present novel sequence intermediate distributions exponential families averaging moments initial target distributions derive asymptotically optimal piecewise linear schedule moments path show performs geometric linear schedule moment averaging performs empirically estimating partition functions restricted boltzmann machines rbms form building blocks many deep learning models including deep belief networks deep boltzmann machines 
optimizing policies interested developing policies learning policy manner content example domain concept learning policy specify nature chosen training sequence traditional studies compare several hand selected policies policy selects only difficult classify policy training sequence easy difficult known propose alternative traditional methodology define parameterized space policies search space identify optimum policy example concept learning policies described function exemplar difficulty time propose experimental technique searching policy spaces using gaussian process surrogate based optimization generative model performance evaluating experimental conditions many human subjects traditional methodology does technique evaluates many experimental conditions subjects individual subjects provide only noisy estimate population mean optimization method allows determine shape policy space identify global optimum efficient its subject budget traditional comparison evaluate method behavioral studies suggest method broad applicability optimization problems involving humans domains 
embeddings modeling multi relational data consider problem embedding entities relationships multi relational data low dimensional vector spaces objective propose canonical model easy train contains reduced number parameters scale very large databases hence propose method models relationships translations operating low dimensional embeddings entities despite its simplicity assumption proves powerful since extensive experiments show significantly outperforms state art methods link prediction knowledge besides successfully trained large scale data set entities relationships training samples 
phase retrieval using alternating minimization phase retrieval problems involve solving linear equations missing sign phase complex numbers information popular generic empirical approach many variants problem alternating minimization alternating estimating missing phase information candidate solution paper show simple alternating minimization algorithm geometrically converges solution problem finding vector vector element wise assumption gaussian empirically algorithm performs similar recently proposed convex techniques variant based lifting convex matrix problem sample complexity robustness noise however algorithm efficient scale large problems analytically show geometric convergence solution sample complexity log factors lower bounds establish close optimal scaling case unknown vector sparse work represents only known proof alternating minimization any variant phase retrieval problems non convex setting 
real time inference gamma process model neural spiking simultaneous measurements ever increasing populations neurons growing need sophisticated tools recover signals individual neurons experiments proceeds step process threshold detect putative spikes ii cluster single units neurons extend previous bayesian models neural spiking jointly detect cluster neurons using gamma process model importantly develop online approximate inference scheme enabling real time analysis performance previous state art exploratory data data partial ground truth novel data several features model contribute improved performance including accounting colored noise ii overlapping spikes iii tracking dynamics using channels enable novel experiments simultaneously measuring many thousands neurons possibly adapting stimuli dynamically probe ever deeper brain 
understanding dropout dropout relatively algorithm training neural networks relies stochastically dropping neurons during training order avoid adaptation feature introduce general formalism studying dropout units connections arbitrary probability values analyze averaging properties dropout linear non linear networks deep neural networks averaging properties dropout characterized recursive equations including approximation expectations normalized weighted geometric means provide estimates bounds approximations corroborate results simulations show simple cases dropout performs stochastic gradient descent regularized error function 
power binary hashing approximating binary similarity using distance short binary shown similarity symmetric accurate using distinct code maps approximating similarity distance distinct binary codes distance 
estimation optimization data sparse study stochastic optimization problems data sparse sense dual current understanding high dimensional statistical learning optimization highlight difficulties terms increased sample complexity sparse data potential benefits terms allowing design algorithms concretely derive matching upper lower bounds minimax rate optimization learning sparse data exhibit algorithms achieving rates algorithms adaptive achieve best possible rate data observed show leveraging sparsity leads minimax optimal parallel asynchronous algorithms providing experimental evidence theoretical results large scale learning tasks 
multi agent control framework adaptation brain computer interfaces closed loop brain computer interface bci adaptive used learn parameters suited decoding user neural response feedback user provides information permits neural tuning adapt present approach model process adaptation encoding model neural signal decoding algorithm multi agent formulation linear quadratic gaussian control problem simulation characterize decoding performance improves neural encoding adaptive optimize qualitatively resembling experimentally demonstrated closed loop improvement propose novel modified update rule fact encoder changing show improve simulated adaptation dynamics modeling approach offers promise insights adaptation improving user learning bci control practical settings 
modeling overlapping communities node develop probabilistic approach accurate network modeling using node framework mixed membership stochastic blockmodel model integrates basic properties nodes social networks connection popular nodes develop scalable algorithm posterior inference based novel variant stochastic variational inference evaluate link prediction accuracy algorithm real world networks nodes benchmark networks demonstrate algorithm predicts better using benchmark networks show node essential achieving high accuracy presence degree distribution noisy links characteristics real networks 
learning limited demonstrations propose approach learning demonstration leverages expert data expert examples very achieve integrating approximate policy iteration algorithm key idea approach expert examples used generate linear constraints optimization similar fashion large margin classification prove upper bound true bellman error approximation computed algorithm iteration show empirically algorithm outperforms policy iteration state art algorithm supervised learning variety scenarios including very demonstrations available experiments include simulations real robotic task 
complexity approximation binary evidence lifted inference lifted inference algorithms exploit symmetries probabilistic models speed inference show impressive performance calculating probabilities relational models resort non lifted inference computing conditional probabilities reason conditioning evidence breaks many model symmetries standard lifting techniques theoretical results show example conditioning evidence corresponds binary relations hard suggesting lifting expected worst case paper balance result identifying rank evidence key parameter characterizing complexity conditioning lifted inference particular show conditioning binary evidence bounded rank efficient opens approximating evidence low rank matrix factorization investigate theoretically empirically 
efficiency restricted boltzmann machines paper question kinds distributions efficiently represented restricted boltzmann machines rbms characterize log likelihood function type neural network called network series simulation results relate networks types better understood show surprising result networks efficiently compute any function depends number input parity provide known example particular type distribution provably efficiently represented efficiently computed network assuming realistic exponential upper bound size weights formally demonstrating relatively simple distribution represented efficiently results provide rigorous justification potentially expressive generative models deeper ones 
memory limited streaming pca consider streaming pass principal component analysis pca high dimensional regime limited memory dimensional samples presented sequentially goal produce dimensional subspace best approximates points standard algorithms require memory algorithm better memory since output requires memory storage complexity meaningful understood context computational sample complexity sample complexity high dimensional pca typically studied setting spiked covariance model dimensional points generated population covariance equal identity white noise plus low dimensional perturbation spike signal recovered understood spike recovered number samples scales dimension algorithms provably achieve memory complexity algorithms memory complexity provable bounds sample complexity comparable present algorithm achieves uses memory meaning storage any kind able compute dimensional spike sample complexity algorithm its kind theoretical analysis focuses spiked covariance model simulations show algorithm successful general models data 
approximate efficient lp solver lp many problems machine learning solved solution appropriate linear program propose scheme based quadratic program relaxation allows parallel stochastic coordinate descent approximately solve large linear programs efficiently software order magnitude faster linear programming solver yields similar solution quality results include novel perturbation analysis quadratic penalty formulation linear programming convergence result derive running time quality guarantees 
linear decision rule simple decision heuristics many attempts understand success simple decision heuristics examined heuristics approximation linear decision rule research identified structures heuristics cumulative develop ideas examine empirical relevance natural environments structures prevalent making possible simple rules reach accuracy levels linear decision rule using information 
relationship binary classification bipartite ranking binary class probability estimation investigate relationship fundamental problems machine learning binary classification bipartite ranking binary class probability estimation known good binary model used obtain good binary classification model obtain good bipartite ranking model using model directly ranking model known binary classification model does necessarily yield model however known directions formally relationships involve regret transfer bounds paper introduce notion weak regret transfer bounds mapping needed transform model problem another depends underlying probability distribution practice estimated data show weaker sense good bipartite ranking model used construct good classification model suitable point surprisingly construct good binary model scores ranking model 
bayesian inference random functions applications sequential inference graphical models propose general formalism random functions property exact approximate bayesian posterior updates viewed specific instances convergence theory random functions presented application general theory analyze convergence behaviors exact approximate message passing algorithms arise sequential change point detection problem formulated latent variable directed graphical model sequential inference algorithm its supporting theory illustrated simulated examples 
compressive feature learning paper addresses problem unsupervised feature learning data method principle minimum length uses dictionary based compression scheme extract succinct feature set specifically method finds set word minimizes cost reconstructing formulate document compression binary optimization task show solve approximately sequence linear programs efficient solve parallelizable method unsupervised features may extracted once used variety tasks demonstrate performance features range scenarios including unsupervised exploratory analysis supervised compressed feature space orders magnitude smaller full gram space matches accuracy achieved full feature space dimensionality reduction only results faster training times elucidate structure unsupervised learning tasks reduce amount training data necessary supervised learning 
moment based uniform deviation bounds means fit points minimizing means cost corresponding fit source question distributions bounded moments particular difference sample cost distribution cost essential contribution mechanism uniformly control face unbounded parameter sets cost functions source distributions demonstrate mechanism soft clustering variant means cost considered namely log likelihood gaussian mixture subject constraint covariance matrices bounded spectrum lastly rate provided means instances cluster structure 
fast template evaluation vector quantization applying linear integral part many object detection systems accounts significant portion computation time describe method achieves substantial speedup best current methods loss accuracy method combination approximating scores vector feature windows number speedup techniques including cascade procedure allows speed accuracy ways choosing number vector quantization levels choosing windows method directly any recognition system relies linear demonstrate method speed original exemplar svm order magnitude part models orders magnitude loss accuracy 
context sensitive active sensing humans humans animals utilize active sensing self motion focus sensory cognitive resources relevant stimuli events environment understanding computational basis natural active sensing important brain developing powerful artificial systems recently goal directed context sensitive bayesian control strategy active sensing termed context dependent active controller proposed contrast previously proposed algorithms human active vision tend optimize abstract statistical objectives therefore adapt changing behavioral context task goals directly minimizes behavioral costs automatically adapts different task conditions however limited model human active sensing given its computational requirements especially complex real world situations propose myopic approximation takes behavioral costs account achieves significant reduction complexity only step present data human active visual search experiment compare performance various models human behavior its myopic variant achieve better fit human data maximizes expected cumulative future information gain summary work provides novel experimental results differentiate theoretical models human active sensing novel active sensing algorithm context sensitivity optimal controller achieving significant computational savings 
convex relaxation tensor completion study problem learning tensor set linear measurements prominent methodology problem based extension trace norm regularization used extensively learning low rank matrices tensor setting paper highlight limitations approach propose alternative convex relaxation euclidean unit ball describe technique solve associated regularization problem builds alternating direction method multipliers experiments synthetic dataset real datasets indicate proposed method improves significantly tensor trace norm regularization terms estimation error remaining computationally tractable 
variational planning graph based mdps markov decision processes mdps extremely useful modeling solving sequential decision making problems graph based mdps provide compact representation mdps large numbers random variables however complexity exactly solving graph based mdp usually grows exponentially number variables limits application present variational framework describe solve planning problem mdps derive exact approximate planning algorithms particular exploiting graph structure graph based mdps propose factored variational value iteration algorithm value function approximated multiplication local scope value functions solved minimizing kullback leibler kl divergence kl divergence optimized using belief propagation algorithm complexity exponential only cluster size graph experimental comparison different models shows algorithm outperforms existing approximation algorithms finding good policies 
convex layer modeling latent variable prediction models multi layer networks impose auxiliary latent variables inputs outputs allow automatic inference implicit features useful prediction unfortunately models difficult train because inference latent variables performed parameter optimization highly non convex problem proposing another local training method develop convex relaxation hidden layer conditional models admits global training approach extends current convex modeling approaches handle nested nonlinearities separated non trivial adaptive latent layer resulting methods able acquire layer models represented any single layer model features improving training quality local heuristics 
sketching structured matrices faster nonlinear regression motivated extend fast randomized techniques nonlinear p$ regression consider class structured regression problems problems involve matrices arise naturally various statistical modeling settings including classical polynomial fitting problems recently developed randomized techniques scalable kernel methods show structure exploited solution regression problem achieving running times faster input sparsity present empirical results practical value modeling framework speedup benefits randomized regression 
efficient reinforcement learning posterior sampling provably efficient learning algorithms introduce optimism poorly understood states actions encourage exploration study alternative approach efficient exploration posterior sampling reinforcement learning algorithm proceeds repeated episodes known start episode updates prior distribution markov decision processes takes sample posterior follows policy optimal sample during episode algorithm simple computationally efficient allows agent encode prior knowledge natural way establish bound expected regret time episode length state action spaces bound algorithm based optimism close state art any reinforcement learning algorithm show simulation significantly outperforms existing algorithms similar regret bounds 
model selection high dimensional regression generalized condition high dimensional regression model response variable linearly related covariates sample size smaller assume only small subset covariates corresponding coefficients non zero consider model selection problem identifying active covariates popular approach estimate regression coefficients lasso regularized squares known correctly identify active set only irrelevant covariates roughly orthogonal relevant ones called condition paper study lasso selector simple stage method solves lasso performs ordinary squares restricted lasso active set formulate condition assumption substantially weaker prove gauss lasso correctly recovers active set 
efficient exploration value function generalization deterministic systems consider problem reinforcement learning episodes finite horizon deterministic system solution propose optimistic constraint propagation algorithm designed synthesize efficient exploration value function generalization establish true value function lies given hypothesis class selects optimal actions episodes eluder dimension given hypothesis class establish efficiency asymptotic performance guarantees apply true value function does given hypothesis space special case hypothesis space pre specified functions disjoint sets 
bellman error based feature generation using random projections sparse spaces paper addresses problem automatic generation features value function approximation reinforcement learning bellman error basis functions shown improve error policy evaluation function approximation convergence rate similar value iteration propose simple fast robust algorithm based random projections generates sparse feature spaces provide finite sample analysis proposed method prove projections logarithmic dimension original space guarantee error empirical results demonstrate strength method domains choosing good state representation challenging 
learning using language recursive reasoning agents language users remarkably good making inferences context children learning language display substantial acquiring meanings unknown words cases related language users terms language learners learn meanings words based inferences words used inference word learning independently characterized probabilistic terms current work unifies describe model language learners assume jointly approximate shared external reason recursively goals others using model captures phenomena word learning inference additionally leads insights systems mechanisms inferences become incorporated word meanings 
learning stochastic describe class algorithms inference bayesian networks setting computation support rapid online inference wide range queries approach based learning inverse factorization model joint distribution factorization turns observations root nodes algorithms information estimate local conditional distributions constitute factorization stochastic used invert computation steps leading observation sampling order quickly likely explanation show estimated converge asymptotically number prior posterior training samples before convergence describe inverse mcmc algorithm uses stochastic block proposals metropolis hastings sampler explore efficiency sampler variety parameter bayes nets 
learning invariant representations applications face verification approach computer object recognition modeling brain ventral stream involves unsupervised learning representations invariant common transformations however applications ideas usually limited transformations scaling since solve convolution theory transformation invariance propose model capturing common convolutional networks special cases used arbitrary identity preserving transformations model learned videos transforming objects any grouping images sets object series complex empirical tests study invariance properties model respect different transformations empirically confirm theoretical predictions case transformations apply model non transformations expected performs face verification tasks requiring invariance relatively smooth transformations depth changes illumination direction surprisingly clutter transformations map image face background image face different background motivated empirical findings tested model face verification benchmark tasks computer vision literature labeled faces dataset achieving strong performance highly unconstrained cases 
optimization learning games sequences provide several applications optimistic mirror descent online learning algorithm based idea sequences recover mirror algorithm prove extension smooth functions apply results point type problems second prove version optimistic mirror descent close relation exponential weights algorithm used strongly players finite zero sum matrix game converge minimax equilibrium rate log addresses question consider partial information version problem apply results approximate convex programming show simple algorithm approximate max flow problem 
adaptivity local smoothness dimension kernel regression present result kernel regression procedure adapts locally point unknown local dimension metric unknown regression function result holds high probability simultaneously points metric space unknown structure 
adaptive dropout training deep neural networks recently shown dropping hidden activities probability deep neural networks perform very describe model binary belief network neural network used decrease information content its hidden units setting activities zero dropout network trained jointly neural network approximately computing local expectations binary dropout variables computing derivatives using back propagation using stochastic gradient descent interestingly experiments show dropout network parameters neural network parameters suggesting good dropout network activities according magnitude evaluated mnist datasets method used achieve lower classification error rates learning methods including standard dropout denoising auto encoders restricted boltzmann machines example model achieves error set better state art results obtained using convolutional architectures 
hierarchical modular optimization convolutional networks achieves representations similar macaque human ventral stream humans recognize visually presented objects rapidly accurately understand ability seek construct models ventral stream series cortical areas thought object recognition tool assess quality model ventral stream representation dissimilarity matrix uses set visual stimuli measures distances produced brain fmri responses neural firing rates models features previous work shown known models ventral stream fail capture pattern observed cortex ventral area human ventral stream work construct models ventral stream using novel optimization procedure category level object recognition problems produce resembling macaque human ventral stream model novel optimization procedure develops long functional hypothesis ventral visual stream series processing stages optimized visual object recognition 
stochastic gradient riemannian langevin dynamics probability simplex paper investigate langevin monte carlo methods probability simplex propose method stochastic gradient riemannian langevin dynamics simple implement applied online apply method latent dirichlet allocation online setting demonstrate achieves substantial performance improvements state art online variational bayesian methods 
distributed representations words recently introduced continuous gram model efficient method learning high quality distributed vector representations capture large number precise syntactic semantic word relationships paper present several improvements gram model expressive enable learn higher quality vectors rapidly show subsampling words obtain significant speedup learn higher quality representations measured tasks introduce negative sampling simplified variant noise contrastive estimation learns accurate vectors words compared hierarchical softmax inherent limitation word representations word order represent example meanings easily combined obtain motivated example present simple efficient method finding show vector representations accurately learned gram model 
regularized spectral clustering degree corrected stochastic blockmodel spectral clustering fast popular algorithm finding clusters networks recently chaudhuri proposed variations algorithm artificially node degrees improved statistical performance current paper extends previous theoretical results canonical spectral clustering algorithm way any assumption minimum degree provides guidance choice tuning parameter moreover results show star shape eigenvectors consistently observed empirical networks explained degree corrected stochastic blockmodel extended partition model statistical model allow highly heterogeneous degrees paper characterizes several variations spectral clustering algorithm terms models 
analyzing harmonic structure graph based learning show explicitly implicitly various known graph based models exhibit common significant harmonic structure its target function value vertex approximately weighted average values its neighbors understanding structure analysis loss defined structure reveal important properties target function graph paper show variation target function upper lower bounded ratio its harmonic loss cost develop analytical tool analyze popular models graph based learning random walks partially random walks times pseudo inverse graph eigenvectors matrices analysis explains several open questions models reported literature furthermore provides theoretical practical simulations synthetic real datasets support analysis 
recurrent linear models simultaneously recorded neural populations population neural recordings long range temporal structure best understood terms shared underlying low dimensional dynamical process advances recording technology provide access ever larger fraction population standard computational approaches available identify dynamics scale poorly size dataset describe scalable approach discovering low dimensional dynamics simultaneously recorded spike trains neural population method based recurrent linear models relates closely models based recurrent neural networks formulate neural data kalman based likelihood calculation latent linear dynamical systems lds models incorporate linear observation process show describe motor cortical population data better directly coupled linear models latent linear dynamical system models linear observations introduce linear model capture low dimensional instantaneous correlations neural populations describes cortical recordings better ising gaussian models fit exactly quickly seen generalization low rank gaussian model case factor analysis computational tractability allow scale very high dimensional neural data 
scalable influence estimation continuous time networks information spread influence estimation problem very challenging since time sensitive nature problem issue scalability need addressed simultaneously paper propose randomized algorithm influence estimation continuous time networks algorithm estimate influence every node network nodes edges accuracy using logarithmic factors computations used greedy influence maximization algorithm proposed method guaranteed set nodes influence optimal value experiments synthetic real world data show proposed method easily scale networks millions nodes significantly improves previous state terms accuracy estimated influence quality selected nodes maximizing influence 
bayesian inference learning gaussian process state space models mcmc state space models successfully used many areas science engineering model time series dynamical systems present fully bayesian approach inference learning nonlinear nonparametric state space models place gaussian process prior transition dynamics resulting flexible model able capture complex dynamical phenomena however enable efficient inference dynamics model infer directly joint smoothing distribution tailored markov chain monte carlo samplers once approximation smoothing distribution computed state transition predictive distribution formulated analytically sparse gaussian process models greatly reduce computational complexity approach 
big sparse inverse covariance estimation variables regularized gaussian maximum likelihood estimator mle shown strong statistical guarantees recovering sparse inverse covariance matrix high dimensional settings however requires solving difficult non smooth log determinant program number parameters scaling quadratically number gaussian variables state art methods scale problems variables paper develop algorithm solve dimensional regularized gaussian mle problems parameters using single machine bounded memory order carefully exploit underlying structure problem include novel block coordinate descent method blocks chosen clustering scheme minimize repeated computations allowing inexact computation specific components modifications able theoretically analyze procedure show achieve super linear quadratic convergence rates 
fast convergence pca prove finite sample convergence rates any pca algorithm using sub quadratic time memory iteration algorithm learning rule efficient known scheme estimating top principal component analysis non convex problem yields expected high probability convergence rates novel technique relate guarantees existing rates stochastic gradient descent strongly convex functions extend results include experiments demonstrate convergence behaviors predicted analysis 
multisensory encoding decoding identification investigate spiking neuron model multisensory integration multiple stimuli different sensory modalities encoded single neural circuit multisensory receptive fields cascade population biophysical spike generators demonstrate stimuli different dimensions faithfully encoded spike domain derive tractable algorithms decoding stimulus common pool spikes show identification multisensory processing single neuron dual recovery stimuli encoded population multisensory neurons prove only projection circuit input stimuli identified provide example multisensory integration using natural audio video discuss performance proposed decoding identification algorithms 
adaptive matching adaptive problem individual shares data integer value indicate level desired privacy problem leads generalization matching setting novel algorithms theory provided implement type relaxation achieves better utility admits theoretical privacy guarantees strong importantly variable level individual empirical results confirm improved utility benchmark social data sets 
optimal integration visual speed different spatiotemporal frequency channels does human visual system compute speed coherent motion stimulus contains motion energy different spatiotemporal frequency propose speed result optimal integration speed information independent spatiotemporal frequency tuned channels formalize hypothesis bayesian model treats activity independent cues optimally combined prior expectation slow speeds model behavioral data speed task measured subjects speed different contrasts spatial various combinations single speed combined stimuli independent relative phase underlying components perceptual biases thresholds always smaller combined stimuli supporting cue combination hypothesis proposed bayesian model fits data accounting perceptual biases thresholds simple combined stimuli fits improved assume responses subject divisive normalization line evidence results provide important step complete model visual motion perception predict speeds stimuli arbitrary spatial structure 
matrix factorization binary components motivated application computational biology consider constrained low rank matrix factorization problems constraints factors addition non convexity shared general matrix factorization schemes problem complicated combinatorial constraint set size dimension data points rank factorization despite provide line work non negative matrix factorization algorithm provably recovers underlying factorization exact case operations order worst case obtain result theory centered fundamental result 
learning pass expectation propagation messages expectation propagation popular approximate posterior inference algorithm provides fast accurate alternative sampling based methods however framework theory allows complex non gaussian factors significant practical barrier using because doing requires implementation message update operators difficult require hand crafted approximations work study question possible automatically derive fast accurate updates learning discriminative model neural network random forest map message inputs message outputs address practical arise process provide empirical analysis several challenging diverse factors space factors approach appears promising 
robust low rank kernel embeddings multivariate distributions kernel embedding distributions led many advances machine learning however latent low rank structures prevalent real world distributions rarely taken account setting furthermore prior work kernel embedding literature addressed issue robust embedding latent low rank information paper propose hierarchical low rank decomposition kernels embeddings exploit low rank structures data being robust model illustrate empirical evidence estimated low rank embeddings lead improved performance density estimation 
