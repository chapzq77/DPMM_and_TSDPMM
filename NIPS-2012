locally uniform comparison image descriptor matching pairs images using popular descriptors sift faster variant called heart many computer vision algorithms including recognition structure motion real time mobile applications very fast accurate descriptors related methods random sampling pairwise comparisons pixel image patch introduce locally uniform comparison image descriptor simple description method based permutation distances ordering rgb values patches computable linear time respect patch size does require point computation analysis reveals underlying issue limits potential related approaches compared experiments demonstrate faster its accuracy directly comparable being order magnitude faster 
learning distributions support measure machines paper presents kernel based discriminative learning framework probability measures relying large collections vectorial training examples framework learns using collection probability distributions constructed represent training data representing probability distributions mean embeddings reproducing kernel hilbert space able apply many standard kernel based learning techniques straightforward fashion accomplish construct generalization support vector machine svm called support measure machine analyses provides several insights relationship traditional svms based insights propose flexible svm svm different kernel functions training example experimental results synthetic real world data demonstrate effectiveness proposed framework 
finding exemplars pairwise sparse recovery given pairwise data points consider problem finding subset data points called exemplars efficiently describe data collection formulate problem row sparsity regularized trace minimization problem solved efficiently using convex programming solution proposed optimization program finds probability data point associated obtain range regularization parameter solution proposed optimization program changes selecting representative selecting data points data points distributed multiple clusters according show data cluster select only cluster unlike metric based methods algorithm does require pairwise metrics applied triangle inequality demonstrate effectiveness proposed algorithm synthetic data real world datasets images 
feature clustering accelerating parallel coordinate descent large scale regularized loss minimization problems arise numerous applications compressed sensing high dimensional supervised learning including classification regression problems high performance algorithms implementations critical efficiently solving problems building previous work coordinate descent algorithms regularized problems introduce novel family algorithms called block greedy coordinate descent includes special cases several existing algorithms greedy greedy give unified convergence analysis family block greedy algorithms analysis suggests block greedy coordinate descent better exploit features clustered maximum inner product features different blocks small theoretical convergence analysis supported experimental results using data diverse real world applications algorithmic approaches convergence analysis provide only advance field researchers systematically explore design space algorithms solving large scale regularization problems 
multi scale hyper time hardware human motor nervous system based spiking neurons using central goal long term progression diseases typical years progression child purpose quantitative models only provide multi scale details ranging neuron spikes models need evaluated hyper time significantly faster real time producing useful predictions designed platform digital hardware multi scale hyper time human motor nervous systems platform constructed scalable distributed field devices devices operate time overall system accelerated real time physiological component implemented using models studies flexibly validity easily maximizing speed implemented combinational logic iterative circuits paper presents methodology building modules correspondence components loop results activities shown paper discusses approximating neural organizing neurons sparse platform allows introducing various neural emerging motor analyzed motor predict long term 
active learning model evidence using bayesian numerical integration key component many problems scientific computing statistical modelling machine learning bayesian model based method numerical integration relative standard monte carlo methods offers increased sample efficiency robust estimate uncertainty estimated integral propose novel bayesian approach numerical integration non negative case computing marginal likelihood predictive distribution constant probabilistic model approach approximately model hyperparameters closed form introduces active learning scheme optimally select function evaluations opposed using monte carlo samples demonstrate method number synthetic benchmarks real scientific problem 
coupling nonparametric mixtures latent dirichlet processes mixture distributions used model complex data paper develop method jointly estimates mixture models multiple data sets exploiting statistical dependencies specifically introduce set latent dirichlet processes sources component models atoms data set construct nonparametric mixture model combining sub sampled versions latent dps mixture model may acquire atoms different latent dps may shared multiple mixtures multi multi association distinguishes proposed method prior rely tree chain structures allowing mixture models coupled flexibly addition derive sampling algorithm jointly infers model parameters present experiments document analysis image modeling 
bayesian hierarchical reinforcement learning describe approach incorporating bayesian priors framework hierarchical reinforcement learning define priors environment model task pseudo rewards since models composite tasks complex mixed model based model learning approach optimal hierarchical policy show empirically approach results improved convergence non bayesian baselines given priors ii task hierarchies bayesian priors complementary sources information using sources better alone taking advantage structural decomposition induced task hierarchy significantly reduces computational cost bayesian reinforcement learning framework task pseudo rewards learned being manually specified leading automatic learning hierarchically optimal recursively optimal policies 
dynamic factor graphs maximum marginal prediction study problem maximum marginal prediction probabilistic graphical models task occurs example bayes optimal decision rule hamming loss typically performed stage procedure estimates variable marginal probability forms prediction states maximal probability work propose simple effective technique accelerating inference sampling based above stage procedure directly estimate posterior probability decision variable allows identify point time sufficiently certain any individual decision whenever case variable underlying factor graph consequently any time only samples variable decision uncertain need created experiments scenarios multi label classification image inpainting shows adaptive sampling drastically accelerate prediction accuracy 
local supervised learning space partitioning develop novel approach supervised learning based adaptively partitioning feature space different regions learning local region specific classifiers formulate empirical risk minimization problem incorporates partitioning classification single global objective show space partitioning supervised learning problem consequently any discriminative learning method utilized conjunction approach nevertheless consider locally linear schemes learning linear partitions linear region classifiers locally linear schemes only approximate complex decision boundaries ensure low training error provide tight control fitting generalization error train locally linear classifiers using lda logistic regression perceptrons scheme scalable large data sizes high dimensions present experimental results demonstrating improved performance state art classification techniques benchmark datasets show improved robustness label noise 
generative model parts based object segmentation shape boltzmann machine recently introduced state art model background object shape extend account object parts model multinomial capture local global statistics part shapes accurately combine appearance model form fully generative model images objects parts based image obtained simply performing probabilistic inference model apply model challenging datasets exhibit significant shape appearance variability obtains results comparable state art 
super bit sensitive hashing sign random projection sensitive hashing probabilistic dimension reduction method provides unbiased estimate angular similarity suffers large variance its estimation work propose super bit sensitive hashing easy implement random projection vectors theoretically guaranteed provides unbiased estimate angular similarity smaller variance estimate extensive experiments real data validate given length binary code may achieve significant mean squared error reduction estimating pairwise angular similarity moreover shows superiority approximate nearest neighbor retrieval experiments 
bethe partition function log graphical models bethe approximation corresponding any fixed point belief propagation algorithm attractive pairwise binary graphical model provides lower bound true partition function work demonstrating any graphical model binary variables potential functions necessarily pairwise log bethe partition function always lower bounds true partition function proof result follows variant theorem may independent interest 
bayes consider sequential prediction algorithms given predictions set models inputs nature data changing time different models predict different segments data typically achieved mixing weights round bit initial prior kind weak however models segment small subset data likely predicted models predicted fitting sparse composite models achieved mixing bit past posteriors self updating method efficient gives superior performance many natural data sets important because introduces long term memory any model done past recovered quickly bayesian mixing bit initial prior bayesian interpretation known mixing past posteriors build framework online learning literature give mixing past posteriors update proper bayesian foundation apply method studied learning problem obtain efficient update achieves significantly better bound 
metric manifold kernel matrices application matrix geometric means symmetric positive definite matrices remarkably pervasive scientific including machine learning optimization consider fundamental task measuring distances matrices task nontrivial whenever application demands distance function respect non euclidean geometry matrices unfortunately typical non euclidean distance measures riemannian metric computationally complicated difficulties introduce metric matrices metric only respects non euclidean geometry offers faster computation being complicated support claims theoretically series theorems relate metric experimentally studying nonconvex problem computing matrix geometric means based squared distances 
leaf node prediction hierarchical multilabel classification hierarchical classification prediction paths may required always leaf nodes called leaf node prediction particularly useful leaf nodes stronger semantic meaning internal nodes however methods hierarchical multiclass classification performing hierarchical multilabel classification difficult paper propose novel algorithm considers global hierarchy structure ii used hierarchies trees show efficiently maximize joint posterior probability node labels simple greedy algorithm moreover extended minimization expected symmetric loss experiments performed number real world data sets tree dag structured label hierarchies proposed method outperforms hierarchical flat multilabel classification methods 
coding efficiency rate non poisson neuronal firing statistical features neuronal spike trains known non poisson investigate extent non feature efficiency information firing rates purpose introduce leibler kl divergence measure efficiency information encoding assume spike trains generated time processes show kl divergence determines lower bound degree rate temporal variation firing rates sparse data show kl divergence lower bound depends only variability spikes terms coefficient variation significantly higher order moments distributions specific models commonly used describing stochastic nature spikes gamma inverse gaussian distributions time process distribution achieves kl divergence followed gamma distributions 
theorem hilbert spaces necessary sufficient condition theorem property lies foundation regularization theory kernel methods class regularization functionals said admit linear theorem every member class lie finite dimensional subspace data states certain classes regularization functionals differentiable regularization term admit linear theorem any choice data only regularization term function paper extend result assumptions regularization term particular main result paper implies sufficiently large family regularization functionals functions only lower regularization terms guarantee existence theorem any choice data 
exploration model based reinforcement learning empirically estimating learning progress formal exploration approaches model based reinforcement learning estimate accuracy currently learned model consideration empirical prediction error example pac mdp approaches base model amount collected data bayesian approaches assume prior transition dynamics propose extensions approaches drive exploration solely based empirical estimates learner accuracy learning progress provide theoretical analysis behavior extensions standard stationary finite state action case provide experimental studies demonstrating robustness exploration measures cases non stationary environments original approaches domain assumptions 
supervised learning similarity functions address problem general supervised learning data only similarity function data points existing work learning kernels concentrated solely binary multiclass classification problems propose model generic handle any supervised learning task model previously proposed classification give criterion similarity functions given supervised learning task adapt known technique provide efficient algorithms supervised learning using good similarity functions demonstrate effectiveness model important supervised learning problems real valued regression ordinal regression ranking show method guarantees bounded generalization error furthermore case real valued regression give natural used conjunction result sparse vector recovery guarantees sparse predictor bounded generalization error finally report results learning algorithms regression ordinal regression tasks using non similarity functions demonstrate effectiveness algorithms especially sparse selection algorithm achieves significantly higher accuracies baseline methods reduced computational costs 
processing structured prediction human machine performance far inferior comparison show problem speech separation problem effectively approached structured prediction account temporal dynamics speech employ conditional random fields crfs classify speech time frequency unit sound mixture capture complex nonlinear relationship input output state transition feature functions crfs learned deep neural networks formulation problem classification allows directly optimize measure correlated human speech proposed system substantially outperforms existing ones variety 
robustness risk markov decision processes uncover relations robust mdps risk sensitive mdps objective robust mdp minimize function expectation cost worst case parameters uncertainties objective risk sensitive mdp minimize risk measure cost parameters known show risk sensitive mdp minimizing expected exponential utility equivalent robust mdp minimizing worst case expectation penalty deviation uncertain parameters values measured kullback leibler divergence show risk sensitive mdp minimizing risk measure composed certain coherent risk measures equivalent robust mdp minimizing worst case expectation possible uncertain parameters values characterized concave function 
dynamical graph learning object shape modeling detection paper studies novel discriminative part based model represent recognize object shapes define model consisting layers leaf nodes collaborative edges localizing local parts nodes specifying switch leaf nodes root node encoding global verification discriminative learning algorithm extended proposed train model dynamical manner model structure configuration leaf nodes associated nodes automatically determined optimizing multi layer parameters during iteration advantages method graph model enables handle large class variance background object shape detection images ii proposed learning algorithm able obtain graph representation requiring supervision initialization validate proposed method several challenging databases horse shape people outperforms state approaches 
adaptive sampling monte carlo integration differentiable functions consider problem adaptive sampling monte carlo integration differentiable function given finite number evaluations function construct sampling scheme samples regions function samples spread domain notion shares low discrepancy prove estimate returned algorithm accurate estimate optimal oracle strategy variations function everywhere return provide finite sample analysis 
distributed non stochastic experts consider online distributed non stochastic experts problem distributed system consists node connected sites sites required communicate time step nodes pick expert set information experts round goal distributed system minimize regret time horizon simultaneously keeping communication minimum extreme solutions problem full communication essentially non distributed setting obtain optimal log regret bound cost communication ii communication runs independent regret log communication paper shows difficulty simultaneously achieving regret asymptotically better communication better give novel algorithm achieves non trivial trade regret communication any value consider variant model expert model show label efficient forecaster already gives strategy near optimal regret vs communication trade 
learning image descriptors boosting trick paper apply boosting learn complex non linear local visual feature representations drawing inspiration its successful application visual object detection main goal local feature descriptors represent salient image region remaining invariant viewpoint changes representation improved using machine learning however past approaches mostly limited learning linear feature mappings original input kernelized input feature space kernelized methods proven effective learning non linear local feature descriptors rely choice appropriate kernel function selection difficult non intuitive propose boosting trick obtain non linear mapping input high dimensional feature space non linear feature mapping obtained boosting trick highly intuitive employ gradient based weak resulting learned descriptor closely resembles known sift demonstrated experiments resulting descriptor learned directly intensity patches achieving state art performance 
fast weighted statistics paper novel computationally fast alternative algorithm weighted statistics univariate multivariate data proposed avoid any real linked problem finite group action problem computational cost reduction efficient method developed list symmetry order index function data function recursively computational complexity analysis shows reduction computational cost nn level low order polynomial level 
multi task vector field learning multi task learning mtl aims improve generalization performance learning multiple related tasks simultaneously identifying shared information tasks existing mtl methods focus learning linear models supervised setting propose novel semi supervised nonlinear approach mtl using vector fields vector field smooth mapping manifold spaces viewed derivative functions manifold argue vector fields provide natural way exploit geometric structure data shared differential structure tasks crucial semi supervised multi task learning paper develop multi task vector field learning learns prediction functions vector fields simultaneously following key properties vector fields learned close gradient fields prediction functions task vector field required parallel possible expected low dimensional subspace vector fields tasks share low dimensional subspace formalize idea regularization framework provide convex relaxation method solve original non convex problem experimental results synthetic real data demonstrate effectiveness proposed approach 
nonparametric bayesian inverse reinforcement learning multiple reward functions present nonparametric bayesian approach inverse reinforcement learning irl multiple reward functions previous irl algorithms assume behaviour data obtained agent optimizing single reward function assumption hard practice approach based integrating dirichlet process mixture model bayesian irl provide efficient metropolis hastings sampling algorithm utilizing gradient posterior estimate underlying reward functions demonstrate approach outperforms previous ones experiments number problem domains 
automatic feature collaborative filtering approaches collaborative filtering concentrated estimating algebraic statistical model using model predicting missing ratings paper observe different models relative advantages different regions input space motivates approach using linear combinations collaborative filtering algorithms non constant combination coefficients based kernel smoothing resulting model computationally scalable outperforms wide selection state art collaborative filtering algorithms 
selective labeling error bound minimization many practical machine learning problems acquisition labeled data expensive time motivates study problem follows given label budget select data points label learning performance optimized propose selective labeling method analyzing generalization error laplacian regularized squares particular derive generalization error bound trained subsampled data propose select subset data points label minimizing upper bound since minimization combinational problem relax continuous domain solve projected gradient descent experiments benchmark datasets show proposed method outperforms state art methods 
volume regularization binary classification introduce large volume box classification binary prediction maintains subset weight vectors specifically axis aligned boxes learning algorithm seeks box large volume contains weight vectors accurate training set versions learning process cast convex optimization problems shown solve efficiently formulation yields natural pac bayesian performance bound shown minimize quantity directly aligned algorithm outperforms svm recently proposed algorithm majority nlp datasets optical recognition datasets 
image denoising inpainting deep neural networks present novel approach low level vision problems combines sparse coding deep networks pre trained denoising auto propose alternative training scheme successfully adapts originally designed unsupervised feature learning tasks image denoising blind inpainting method achieves state art performance image denoising task importantly blind image inpainting task proposed method provides solutions complex problems before specifically automatically complex patterns image simple patterns pixels missing random moreover proposed method does need information regarding region requires inpainting given priori experimental results demonstrate effectiveness proposed method tasks image denoising blind inpainting show training scheme effective improve performance unsupervised feature learning 
max margin structured output regression spatio temporal action structured output learning successfully applied object mapping image object bounding box captured its extension action videos however challenging because needs predict locations action patterns spatially temporally identifying sequence bounding boxes track action video problem becomes intractable due exponentially large size structured video space actions propose novel structured learning approach spatio temporal action mapping video spatio temporal action trajectory learned intractable inference learning problems addressed leveraging efficient max path search method makes feasible optimize model structured space experiments challenging benchmark datasets show proposed method outperforms state art methods 
action model based multi agent plan recognition multi agent plan recognition aims recognize dynamic team structures team behaviors observed team traces activity sequences set intelligent previous approaches required team activity sequences team given input however team ensure difficult costly paper relax constraint team required provided assume set action models available models already created describe domain effects effects actions propose novel approach recognizing multi agent team based action models team encode resulting problem satisfiability problem solve problem using state art weighted max sat solver approach allows observed plan traces empirical studies demonstrate algorithm effective efficient comparison state art methods based plan 
visual recognition using embedded feature selection self similarity category level object detection crucial need informative object representations demand led feature descriptors increasing dimensionality occurrence statistics self similarity paper propose object representation based self similarity goes currently popular approximation objects using however descriptors using second order statistics ours exhibits high dimensionality improving high dimensionality becomes critical issue due generalization ability curse dimensionality given only limited amount training data sophisticated learning algorithms popular kernel methods able noisy dimensions high dimensional data consequently natural need feature selection using present informative features particularly self similarity therefore suggest embedded feature selection method svms reduces complexity improves generalization capability object models successfully integrating proposed self similarity representation together embedded feature selection widely used state art object detection framework show general approach 
non parametric approximate dynamic programming kernel method paper presents novel non parametric approximate dynamic programming algorithm enjoys dimension independent approximation sample complexity guarantees particular establish theoretically computationally proposal serve viable alternative state art parametric algorithms carefully specifying approximation architecture accomplish developing kernel based mathematical program computational study controlled network show non parametric procedure competitive parametric approaches 
optimal regularized dual averaging methods stochastic optimization paper considers wide spectrum regularized stochastic optimization problems loss function regularizer non smooth develop novel algorithm based regularized dual averaging method simultaneously achieve optimal convergence rates convex strongly convex loss particular strongly convex loss achieves optimal rate iterations improves best known rate previous stochastic dual averaging algorithms addition method constructs final solution directly proximal mapping averaging previous widely used sparsity inducing regularizers norm advantage encouraging sparser solutions develop multi stage extension using proposed algorithm achieves uniformly optimal rate strongly convex loss 
variational hierarchical em algorithm clustering hidden markov models paper derive novel algorithm cluster hidden markov models according probability distributions propose variational hierarchical em algorithm clusters given collection groups similar terms distributions represent ii characterizes group center novel hmm representative group illustrate benefits proposed algorithm hierarchical clustering motion capture sequences automatic tagging 
truncation online variational inference bayesian nonparametric models present truncation online variational inference algorithm bayesian nonparametric models unlike traditional online variational inference algorithms require model variational distribution method adapts model complexity fly experiments dirichlet process mixture models hierarchical dirichlet process topic models large scale data sets show better performance previous online variational inference algorithms 
context sensitive decision forests object detection paper introduce context sensitive decision forests perspective exploit information popular decision forest framework object detection problem tree structured classifiers ability access intermediate prediction classification regression information during training inference time intermediate prediction available sample allows develop context based decision criteria used prediction process addition introduce novel split criterion combination based way constructing trees allows accurate regression selection hence improves current context information experiments demonstrate improved results task detection challenging data set compared state art methods 
learning invariant representations energy prediction accurate prediction space crucial rational design inherently graph non vectorial nature data gives rise unique difficult machine learning problem paper adopt learning scratch approach energies predicted directly raw geometry study suggests benefit setting flexible priors enforcing results improve state art factor statistical methods step accuracy 
bandit algorithms boost brain computer interfaces motor task selection brain controlled brain computer bci allows users computer using bci based motor motor tasks moving right hand control signals performances bci vary greatly users depend tasks used making problem appropriate task selection important issue study presents procedure automatically select fast possible discriminant motor task brain controlled develop purpose adaptive algorithm based stochastic bandit theory training stage thereby allowing exploration greater variety tasks time inefficient tasks focusing promising ones algorithm results faster task selection efficient bci training session comparing proposed method standard practice task selection fixed time budget leads improve classification rate fix classification rate reduction time training 
multiplicative forests continuous time processes learning temporal dependencies variables continuous time important challenging task continuous time bayesian networks effectively model processes limited number conditional intensity matrices grows exponentially number parents variable develop partition based representation using regression trees forests parameter spaces linearly number node using multiplicative assumption show update forest likelihood closed form producing efficient model updates results show multiplicative forests learned temporal trajectories large gains performance scalability 
patient risk associated time series classification task patient risk events temporal processes including nature timing activities overall evolution patient time many temporal aspect modeling patient risk considering only patient current aggregate state explore representing patient risk time series doing patient risk becomes time series classification task task differs applications time series analysis speech processing since time series extracted defining extracting approximate risk processes evolving approximate risk patient once obtained signals explore different approaches time series classification goal identifying high risk patterns apply classification specific task identifying patients risk testing positive achieve area operating curve held set several patients stage approach risk outperforms classifiers consider only patient current state p$ 
nyström method vs random fourier features theoretical empirical comparison random fourier features nyström method successfully applied efficient kernel learning work investigate fundamental difference approaches difference affect generalization performances unlike approaches based random fourier features basis functions cosine functions sampled distribution independent training data basis functions used nyström method randomly sampled training examples therefore data dependent exploring difference show large gap spectrum kernel matrix approaches based nyström method yield better generalization error bound random fourier features based approach empirically verify theoretical findings wide range large data sets 
multiclass learning approaches theoretical comparison implications theoretically analyze compare following popular multiclass classification methods vs pairs tree based classifiers error output codes randomly generated code matrices multiclass svm methods classification based reduction binary classification consider case binary classifier comes class dimension particular class halfspaces d$ analyze estimation error approximation error methods analysis reveals interesting practical relevance regarding success different approaches various conditions proof technique employs tools theory analyze approximation error hypothesis classes sharp contrast previous uses theory only deal estimation error 
stochastic gradient descent only projection many variants stochastic gradient descent proposed large scale convex optimization require solution iteration ensure obtained solution feasible domain complex domains positive semidefinite cone projection step computationally expensive making stochastic gradient descent large scale optimization problems address limitation developing novel stochastic gradient descent algorithm does need intermediate projections only projection iteration needed obtain feasible solution given domain theoretical analysis shows high probability proposed algorithms achieve convergence rate general convex optimization rate strongly convex optimization mild conditions domain objective function 
neuronal spike generation mechanism noise explore hypothesis neuronal spike generation mechanism analog digital low pass filtered synaptic currents encodes spike trains linearly post synaptic neurons encode analog current waveform sampling rate spike generation mechanism its rate consistent experimental observation precision spike generation mechanism order magnitude greater cut frequency dendritic low pass filtering achieve additional reduction error analog digital conversion rely noise noise used neurons introduce correlations spike timing reduce low frequency transmission error cost high frequency sampling rate using experimental data different classes neurons demonstrate biological neurons utilize noise argue spike generation mechanism may improve energy efficiency carry finally neurons may viewed set predictors various subsets depending statistics input current 
deep spatio temporal architectures learning protein structure prediction prediction fundamental problem protein structure prediction despite considerable research prediction methods largely unreliable introduce novel deep machine learning architecture consists stack learning modules prediction idea implemented dimensional stack neural networks nn index spatial coordinates map time temporal dimension introduced capture fact protein instantaneous process progressive networks level stack trained supervised fashion predictions produced previous level hence addressing problem vanishing gradients typical deep architectures increased accuracy generalization approach established rigorous comparison classical machine learning approaches prediction deep approach leads accuracy difficult long range above state art many variations architectures training algorithms possible room improvements furthermore approach applicable problems strong underlying spatial temporal components 
assessing clinical interaction patient expected intervention inherent effects intervention effects clinical nature intervention trial blind practice perfect impossible ensure verify current standard follow trial allows trial express belief concerning intervention used compute measure extent trial estimated extent exceeds threshold trial sufficiently otherwise trial failed paper several important contributions firstly identify series fundamental problems practice discuss context commonly used measures secondly motivated problems formulate novel method handling adopt post trial feedback interpret collected data using original approach fundamentally different previously proposed unlike previous approaches ours any parameters robust small changes data any strong assumptions used interpret feedback 
scalable nonconvex proximal splitting study large scale nonsmooth optimization problems particular focus nonconvex problems composite objectives class problems includes extensively studied convex composite objective problems special case tackle composite nonconvex problems introduce powerful framework based asymptotically errors avoiding common convenient assumption vanishing errors framework derive batch incremental nonconvex proximal splitting algorithms knowledge framework develop analyze incremental nonconvex proximal splitting algorithms ability handle errors illustrate theoretical framework showing applies difficult large scale nonsmooth nonconvex problems 
learning discover social networks social networks big cluttered currently good way social sites allow users manually social twitter however construct updated whenever user network grows define novel machine learning task identifying users social pose problem node clustering problem user network network connections develop model detecting combines network structure user profile information learn its members specific user profile similarity metric modeling node membership multiple allows detect overlapping hierarchically nested experiments show model accurately identifies diverse set data twitter obtain hand labeled ground truth data 
majorization crfs latent partition function plays key role probabilistic modeling including conditional random fields graphical models maximum likelihood estimation optimize partition functions article introduces quadratic variational upper bound inequality facilitates majorization methods optimization complicated functions iterative solution simpler sub problems bounds efficient compute partition function involves graphical model small tree latent likelihood settings large scale problems low rank versions bound provided outperform order methods several learning applications shown reduce fast convergent update rules experimental results show advantages state art optimization methods 
weighted kernel estimators multivariate entropy estimation problem estimation entropy functionals probability densities received attention information theory machine learning statistics communities kernel density plug estimators simple easy implement widely used estimation entropy however kernel plug estimators suffer curse dimensionality rate convergence slow order number samples rate parameter paper shown sufficiently smooth densities kernel plug estimators combined weighted convex combination resulting weighted estimator superior parametric rate convergence order furthermore shown optimal weights determined solving convex optimization problem does require training data knowledge underlying density therefore performed novel result remarkable individual kernel plug estimators belonging suffer curse dimensionality appropriate averaging achieve parametric convergence rates 
efficient high dimensional maximum entropy modeling symmetric partition functions application maximum entropy principle sequence modeling methods conditional random fields crfs however approaches generally limited modeling paths discrete spaces low dimensionality consider problem modeling distributions paths continuous spaces high dimensionality problem inference generally intractable main contribution show maximum entropy modeling high dimensional continuous paths tractable long constrained features certain kind low dimensional structure case show associated partition function symmetric symmetry compute partition function efficiently compressed form empirical results given showing application method maximum entropy modeling high dimensional human motion capture data 
trained sparse code gradients detection finding natural images fundamental problem serves basis many tasks image segmentation object recognition core detection technologies set hand designed gradient features used existing approaches including state art global operator work show detection accuracy significantly improved computing sparse code gradients measure contrast using patch representations automatically learned sparse coding svd orthogonal matching pursuit efficient dictionary learning encoding multi scale pooling power transforms code oriented local before computing gradients applying linear svm extracting rich representations pixels avoiding sparse code gradients effectively learn measure local improve measure metric benchmark moreover learning approach easily adapt novel sensor data style rgb cameras sparse code gradients depth images surface normals lead promising detection using depth depth color verified depth dataset work combines concept oriented gradients sparse representation opens future learning detection segmentation 
analyzing objects cluttered images present approach detecting analyzing configuration objects real world images occlusion focus application finding analyzing stage model stage reasons shape appearance variation due class variation different changes viewpoint using view based model describe compositional representation models large number effective views shapes using small number local view based model propose candidate detections estimates shape estimates refined second stage using explicit model shape viewpoint model capture class variation weak perspective camera model capture viewpoint learn model parameters annotations demonstrate state art accuracy detection viewpoint estimation shape reconstruction challenging images pascal voc dataset 
object detection viewpoint estimation cuboid model paper addresses problem category level object detection given image aim localize objects tight oriented bounding boxes propose novel approach extends part based model reason model represents object class cuboid composed faces parts allowed respect box model appearance face parallel coordinates effectively factoring appearance variation induced viewpoint model reasons face called aspects train cuboid model jointly share weights aspects attain efficiency inference entails sliding box scoring object hypotheses inference search space variables continuous model demonstrate effectiveness approach indoor outdoor scenarios show approach outperforms state art object detection 
structured learning gaussian graphical models consider estimation multiple high dimensional gaussian graphical models corresponding single set nodes several distinct conditions assume aspects networks shared structured differences specifically network differences generated node perturbations nodes perturbed networks edges nodes networks corresponds simple model mechanism underlying many gene regulatory network due activity specific propose solve problem using structured joint graphical lasso convex optimization problem based novel symmetric overlap norm penalty solve using alternating directions method multipliers algorithm proposal illustrated synthetic data application brain gene expression data 
pivot steps simplex algorithm classification present simplex algorithm linear programming linear classification formulation complexity parameter linear classification problems called margin prove margin values practical interest simplex variant performs number pivot steps worst case its overall running time near linear contrast general linear programming sub polynomial pivot rule known 
shifting weights adapting object detectors image video typical object detectors trained images perform poorly video clear domain types data paper tackle problem adapting object detectors learned images work videos problem unsupervised domain adaptation given labeled data source domain image only unlabeled data target domain video approach self domain adaptation seeks iteratively adapt detector training detector automatically discovered target domain examples starting iteration algorithm adapts considering increased number target domain examples number source domain examples discover target domain examples vast amount video data introduce simple robust approach scores trajectory tracks bounding boxes show rich features specific target domain framework show promising results trecvid multimedia event detection video datasets illustrate benefit approach adapt object detectors video 
scalable matrix decomposition algorithm lower time complexity tighter bound matrix decomposition important extension nyström approximation general matrix approximates any data matrix terms small number its columns rows paper propose novel randomized algorithm expected relative error bound proposed algorithm advantages existing relative error algorithms tighter theoretical bound lower time complexity avoid maintaining data matrix main memory finally experiments several real world datasets demonstrate significant improvement existing relative error algorithms 
semi supervised domain adaptation non parametric copulas framework based theory copulas proposed address semi supervised domain adaptation problems presented method any multivariate density product marginal distributions copula functions therefore changes factors adapt density model different learning domains importantly introduce novel copula model allows factorization non parametric manner experimental results regression problems real world data illustrate efficacy proposed approach compared state art techniques 
identification recurrent patterns activation brain networks identifying patterns neuroimaging recordings brain activity related psychological state individual unsupervised pattern recognition problem main challenges however analysis fmri data defining meaningful feature space representing spatial patterns time dealing high dimensionality data robustness various fmri time series paper present network aware feature space represent states general network enables comparing clustering states manner meaningful terms network connectivity structure computationally efficient low dimensional relatively robust structured random noise feature space obtained spherical relaxation distance metric measures cost network transform function another theoretical empirical demonstrate accuracy efficiency approximation especially large problems application presented identifying distinct brain activity patterns fmri feature space applied problem identifying patterns detecting outliers measurements many different types networks including sensor control social networks 
density difference estimation address problem estimating difference probability densities naive approach step procedure estimating densities separately computing difference however step procedure does necessarily work because step performed regard second step small estimation error incurred stage big error second stage paper propose single procedure directly estimating density difference separately estimating densities derive non parametric finite sample error bound proposed single density difference estimator show achieves optimal convergence rate show proposed density difference estimator utilized distance approximation finally experimentally demonstrate usefulness proposed method robust distribution comparison class prior estimation change point detection 
variational inference crowdsourcing crowdsourcing become popular paradigm labeling large datasets however given rise computational task aggregating crowdsourced labels provided collection unreliable approach problem transforming standard inference problem graphical models applying approximate variational methods including belief propagation bp mean field show bp algorithm generalizes majority voting algorithm method closely related commonly used em algorithm cases performance algorithms critically depends choice prior distribution workers reliability choosing prior properly bp em perform surprisingly simulated real world datasets competitive state art algorithms based complicated modeling assumptions 
mcmc continuous time discrete state systems propose simple novel framework mcmc inference continuous time discrete state systems jump trajectories construct exact mcmc sampler systems sampling random time given trajectory system trajectory given step performed efficiently using properties poisson process second step discrete time mcmc techniques based forward backward algorithm compare approach particle mcmc based sampler show its advantages 
bci prior information enables unsupervised brain computer interfaces bci based need long training times many stimulus contribution introduce set unsupervised hierarchical probabilistic models tackle problems simultaneously incorporating prior knowledge sources information training subjects transfer learning information words being language models show due prior knowledge performance unsupervised models cases supervised models training session 
learning canonical views internet image collections human object recognition robust viewpoint research human indicates preferred view objects discovered years canonical view only small number categories validated experimentally moreover why humans prefer canonical view views remains paper internet image collections learn canonical start manually finding common view results returned internet search engines objects used experiments results show likely view search corresponds view preferred human subjects experiments present simple method likely view image collection apply hundreds categories using data collected present strong evidence prominent formal canonical views provide novel constraints 
learning high density regions generalized high dimensional data propose efficient generalized nonparametric statistical detecting distributional change high dimensional data implement introduce novel hierarchical minimum volume sets estimator represent distributions tested work motivated need detect changes data streams especially efficient context provide theoretical show its superiority existing methods 
multiresolution gaussian processes propose multiresolution gaussian process capture long range non markovian dependencies allowing changes multiresolution hierarchically collection smooth gps defined element random nested partition long range dependencies captured top level partition points define changes due inherent gps analytically gps compute conditional likelihood observations given partition tree allows efficient inference partition employ graph theoretic techniques apply multiresolution analysis recordings brain activity 
localizing single view images paper seek detect localize single view images contrast approaches rely detecting vanishing points scene line segments form build discriminative parts based detector models appearance cuboid internal edges enforcing consistency cuboid model model invariant different aspect able detect many different object categories introduce database images cuboid annotations variety indoor outdoor show qualitative quantitative results collected database model performs baseline detectors constraints alone task localizing cuboid 
newton methods sparse inverse covariance estimation propose classes second order optimization methods solving sparse inverse covariance estimation problem approach call newton lasso method minimizes piecewise quadratic model objective function every iteration generate step employ fast iterative thresholding method solve second approach call based newton method phase algorithm identifies face minimizes smooth quadratic approximation objective function using conjugate gradient method methods exploit structure efficiently compute search direction avoid explicitly show quasi newton methods effective context describe limited memory bfgs variant based newton method present numerical results suggest techniques described paper attractive properties useful tools solving sparse inverse covariance estimation problem comparisons method implemented quic software package presented 
learning scratch unsupervised joint alignment images demonstrated improve performance recognition tasks face verification alignment reduces variability due factors pose only requiring weak supervision form poorly aligned examples however prior work unsupervised alignment complex real world images required careful selection feature representation based hand crafted image descriptors order achieve appropriate smooth optimization landscape paper propose novel combination unsupervised joint alignment unsupervised feature learning specifically incorporate deep learning alignment framework deep learning obtain features represent image based network depth tuned statistics specific data being aligned addition modify learning algorithm restricted boltzmann machine incorporating group sparsity penalty leading topographic learned filters improving subsequent alignment results apply method labeled faces database using aligned images produced proposed unsupervised algorithm achieve significantly higher accuracy face verification obtained using original face images prior work unsupervised alignment prior work supervised alignment match accuracy best available method 
plasticity bayesian spiking networks expectation maximization posterior constraints spiking network models bayesian inference unsupervised learning frequently assume inputs arrive special employ complex computations neuronal activation functions synaptic plasticity rules show rigorous mathematical treatment processes previously received little attention context overcome common theoretical limitations neural implementation performance existing models particular show plasticity understood balancing posterior constraint during probabilistic inference learning expectation maximization link dynamics theory variational inference show nontrivial terms typically appear during probabilistic inference large class models demonstrate approach spiking architecture bayesian inference learning finally sketch mathematical framework extended recurrent network architectures theory provides novel perspective processes synaptic plasticity cortical points essential role during inference learning spiking networks 
clustering aggregation maximum weight independent set formulate clustering aggregation special instance maximum weight independent set problem given dataset graph constructed union input generated different underlying clustering algorithms different parameters represent distinct clusters weighted internal index measuring separation edges corresponding clusters overlap optimal clustering obtained selecting optimal subset non overlapping clusters partitioning dataset together formalize intuition problem graph finding subset non problem exhibits special structure since clusters input clustering form partition dataset corresponding clustering form maximal independent set graph propose variant simulated method takes advantage special structure algorithm close distinct local optimum problem utilizes local search heuristic explore its neighborhood order extensive experiments many challenging datasets show approach clustering aggregation automatically optimal number clusters does require any parameter tuning underlying clustering algorithms combine advantages different underlying clustering algorithms achieve superior performance robust input 
topology constraints graphical models graphical models very useful tool describe understand natural phenomena gene expression climate change social interactions topological structure graphs networks fundamental part analysis many cases main goal study however little work done incorporating prior topological knowledge estimation underlying graphical models sample data work propose extensions basic joint regression model network estimation explicitly incorporate graph topological constraints corresponding optimization approach proposed extension includes eigenvector constraint thereby important prior topological property second developed extension formation certain motifs triangle ones particular known exist example genetic regulatory networks underlying formulations serve examples introduction topological constraints network estimation complemented examples diverse datasets demonstrating importance incorporating critical prior knowledge 
kernel latent svm visual recognition latent svms class powerful tools successfully applied many applications computer vision however limitation rely linear models many computer vision tasks linear models suboptimal nonlinear models learned kernels typically perform better therefore desirable develop kernel version paper propose kernel latent svm learning framework combines latent svms kernel methods develop iterative training algorithm learn model parameters demonstrate effectiveness using different applications visual recognition formulation very general applied solve wide range applications computer vision machine learning 
learning partially observable models using temporally abstract decision trees paper introduces trees partial models partially observable environments trees given specific predictions learn decision tree main idea trees temporally abstract features identify split features key events spread arbitrarily far past whereas previous decision tree based methods limited finite experiments demonstrate trees learn high quality predictions complex partially observable environments high dimensional observations game 
regularized policy learning present novel regularized policy convergent learning method termed able learn sparse representations value functions low computational complexity algorithmic framework underlying integrates key ideas policy convergent gradient methods convex concave point formulation non smooth convex optimization enables order solvers feature selection using online convex regularization detailed theoretical experimental analysis presented variety experiments presented illustrate policy convergence sparse feature selection capability low computational cost algorithm 
multi criteria anomaly detection using depth analysis consider problem identifying patterns data set exhibit behavior anomaly detection anomaly detection algorithms dissimilarity data samples calculated single criterion euclidean distance however many cases may exist single dissimilarity measure captures possible patterns case multiple criteria defined multiple criteria taking linear combination importance different criteria known advance algorithm may need multiple times different choices weights linear combination paper introduce novel non parametric multi criteria anomaly detection method using depth analysis uses concept optimality detect multiple criteria having run algorithm multiple times different choices weights proposed approach scales linearly number criteria provably better linear combinations criteria 
control regularization neural systems correlated noise processes learn reliable rules generalize novel situations brain capable imposing form regularization suggest theoretical computational combination noise provides plausible mechanism regularization nervous system functional role regularization considered general context coupled computational systems inputs correlated noise noise inputs shown impose regularization induces time varying correlations noise variables degree regularization calibrated time resulting qualitative behavior matches experimental data visual cortex 
calibrated elastic regularization matrix completion paper problem matrix completion estimate matrix observations small subset propose calibrated spectrum elastic method sum nuclear frobenius penalties develop iterative algorithm solve convex minimization problem iterative algorithm missing entries incomplete matrix current estimating matrix scaled soft thresholding singular value decomposition matrix resulting matrix converges calibration step follows correct bias frobenius penalty proper coherence conditions suitable penalties levels prove proposed estimator achieves error bound nearly optimal order noise level provides unified analysis noisy matrix completion problems simulation results presented compare proposal previous ones 
objects driven context visual search paradigm object class detection sliding simple effective propose strategies search objects intelligently explore space making sequential observations locations based previous observations strategies adapt class being content particular image driving force exploiting context statistical relation appearance window its relative object observed training set addition being elegant sliding demonstrate experimentally pascal voc dataset strategies evaluate orders magnitude time achieving higher detection accuracy 
timely object recognition large visual multi class detection framework results crucial method timely multi class detection aims give best possible performance any single point start time deadline time goal formulate dynamic closed loop policy infers image order decide detector contrast previous work method significantly greedy strategies able learn actions values evaluate method novel measure computed area average precision vs time curve experiments conducted pascal voc object detection dataset execution only detectors run method obtains better random ordering better performance intelligent baseline measure method obtains better performance code available easily detectors classifiers boxes learns execution traces using reinforcement learning 
belief propagation empirical success belief propagation approximate inference algorithm inspired numerous theoretical algorithmic advances continuous non gaussian domains performing belief propagation remains challenging task innovations nonparametric kernel belief propagation useful come substantial computational cost offer little theoretical guarantees tree structured models work present bp performing efficient inference distributions parameterized gaussian copulas network any univariate tree structured networks approach guaranteed exact powerful class non gaussian models importantly method efficient standard gaussian bp its convergence properties depend complexity univariate nonparametric representation used 
deep representations codes image auto task assigning set relevant tags image challenging due size variability consequently existing algorithms focus assignment fix large number hand crafted features describe image characteristics paper introduce hierarchical model learning representations full sized color images pixel level removing need feature representations subsequent feature selection benchmark model recognition dataset achieving state art performance features combined outperform compete existing approaches distinct image descriptors furthermore using bit codes hamming distance training only small reduction performance efficient storage fast comparisons experiments using architectures always outperform ones 
spectral algorithm latent dirichlet allocation topic modeling generalization clustering posits observations words document generated multiple latent factors topics opposed just increased power comes cost challenging unsupervised learning problem estimating topic word distributions only words observed topics hidden work provides simple efficient learning procedure guaranteed recover parameters wide class topic models including latent dirichlet allocation lda lda procedure correctly recovers topic word distributions parameters dirichlet prior topic mixtures using only statistics third order moments may estimated documents containing just words method called excess correlation analysis based spectral decomposition low order moments singular value moreover algorithm scalable since carried only k$ matrices number latent factors topics typically smaller dimension observation word space 
learning halfspaces zero loss time accuracy tradeoffs given study time complexity required learn error rate optimal margin error rate polynomial time sample complexity achievable using hinge loss time impossible learning possible time immediate question paper achievable derive positive results polynomial time exponential time particular show cases problem polynomial time results naturally extend adversarial online learning model pac learning noise model 
matrix reconstruction local max norm introduce family matrix norms local max norms generalizing existing methods max norm trace norm nuclear norm weighted smoothed weighted trace norms extensively used literature regularizers matrix reconstruction problems show family used weighted unweighted trace norm max norm simulated data large scale ratings data improved accuracy relative existing matrix norms provide theoretical results showing learning guarantees norms 
analog optical computers computing powerful flexible machine learning technique easily implemented hardware recently using time architecture hardware computers performance comparable digital implementations operating speeds allowing real time information using systems present main performance bottleneck layer uses slow digital designed analog suitable time computers capable working real time tested experimentally standard benchmark task its performance better non methods room improvement present work thereby overcomes major limitations future development hardware computers 
accuracy top introduce notion classification accuracy based top values scoring function relevant criterion number problems arising search engines define algorithm optimizing convex surrogate corresponding loss show its solution obtained solving several convex optimization problems present margin based guarantees algorithm based functions hypothesis set finally report results several experiments evaluating performance algorithm comparison bipartite setting several algorithms seeking high precision top algorithm achieves better performance precision top 
minimizing sparse high order energies submodular vertex inference high order graphical models become increasingly important years consider energies simple sparse high order potentials previous work area uses message passing transforms high order potential pairwise case fundamentally different approach transforming entire original problem small instance submodular vertex problem vertex instances standard pairwise methods run faster times effective original problem evaluate approach synthetic data show algorithm useful fast hierarchical clustering model estimation framework 
perfect dimensionality recovery variational bayesian pca variational bayesian vb approach best tractable approximations bayesian estimation demonstrated perform many applications however its good performance fully understood theoretically example vb sometimes produces sparse solution practical advantage vb sparsity observed rigorous bayesian estimation paper focus probabilistic pca give theoretical insight empirical success vb specifically situation noise variance unknown derive sufficient condition perfect recovery true pca dimensionality large scale limit size observed matrix goes infinity analysis obtain bounds noise variance estimator simple closed form solutions parameters themselves actually very useful better implementation vb pca 
mirror descent fixed share regret mirror descent regularizer known achieve shifting regret bounds logarithmic dimension done using carefully designed projection weight sharing technique novel unified analysis show approaches essentially equivalent bounds notion regret generalizing shifting adaptive discounted related analysis captures extends generalized weight sharing technique refined several ways including improvements small losses adaptive tuning parameters 
near optimal differentially private principal components principal components analysis pca standard tool identifying good low dimensional approximations data sets high dimension many current data sets interest contain private sensitive information individuals algorithms operate data sensitive privacy risks outputs differential privacy framework developing tradeoffs privacy utility outputs paper investigate theory empirical performance differentially private approximations pca propose method explicitly utility output demonstrate real data large performance gap existing methods method show sample complexity procedures differs scaling data dimension method nearly optimal terms scaling 
inverse reinforcement learning structured classification paper inverse reinforcement learning irl problem inferring reward demonstrated expert behavior optimal introduce algorithm principle called feature expectation expert score function multi class classifier approach produces reward function expert policy provably near optimal contrary existing irl algorithms does require solving direct rl problem moreover appropriate heuristic succeed only trajectories sampled according expert behavior illustrated car driving 
augmented svm automatic space partitioning combining multiple non linear dynamics non linear dynamical systems used extensively building generative models human behavior its applications range modeling brain dynamics encoding motor many schemes proposed encoding robot using dynamical systems single attractor placed target state space enable perturbations any planning always directed towards single target work focus combining several distinct resulting multi stable show its applicability reach grasp tasks represent several points target object exploiting multiple provides flexibility recovering perturbations increases complexity underlying learning problem present augmented svm svm model inherits region partitioning ability known svm classifier augmented novel constraints derived individual constraints modify original svm dual optimal solution results class support vectors ensure resulting multi stable minimum deviation original dynamics stable finite region show implementations simulated mobile platform model capable real time motion generation able adapt fly perturbations 
efficient bayes adaptive reinforcement learning using sample based search bayesian model based reinforcement learning formally elegant approach learning optimal behaviour model uncertainty trading exploration exploitation ideal way unfortunately finding resulting bayes optimal policies since search space becomes paper introduce tractable sample based method approximate bayes optimal planning exploits monte carlo tree search approach prior bayesian model based rl algorithms significant margin several known benchmark problems because avoids expensive applications bayes rule search tree sampling models current beliefs illustrate advantages approach showing working infinite state space domain reach previous work bayesian exploration 
dimensionality dependent pac bayes margin bound margin important concepts machine learning previous margin bounds svm boosting dimensionality independent major advantage dimensionality explain excellent performance svm feature spaces high infinite dimension paper address problem dimensionality intrinsic margin bounds prove dimensionality dependent pac bayes margin bound bound monotone increasing respect dimension keeping factors fixed show bound strictly previously known pac bayes margin bound feature space finite dimension bounds tend equivalent dimension goes infinity addition show bound linear classifiers recovered bound mild conditions conduct extensive experiments benchmark datasets bound useful model selection significantly dimensionality independent pac bayes margin bound bound linear classifiers 
latent graphical model selection efficient methods locally tree graphs graphical model selection refers problem estimating unknown graph structure given observations nodes model consider challenging instance problem nodes latent hidden characterize conditions tractable graph estimation develop efficient methods provable guarantees consider class ising models markov locally tree graphs regime correlation decay propose efficient method graph estimation establish its structural consistency number samples scales minimum edge potential depth distance hidden node nearest observed nodes parameter depends minimum maximum node edge potentials ising model proposed method practical implement provides flexibility control number latent variables output graph present necessary conditions graph estimation any method show method nearly matches lower bound sample requirements 
learning mixtures tree graphical models consider unsupervised estimation mixtures discrete graphical models class variable hidden mixture component potentially different markov graph structure parameters observed variables propose novel method estimating mixture components provable guarantees output tree mixture model serves good approximation underlying graphical model mixture sample computational requirements method scale component mixture variate graphical models wide class models includes tree mixtures mixtures bounded degree graphs 
hamming distance metric learning motivated large scale multimedia applications propose learn mappings high dimensional data binary codes preserve semantic similarity binary codes suited large scale applications storage efficient exact sub linear knn search framework applicable broad families mappings uses flexible form triplet ranking loss overcome optimization discrete mappings minimizing piecewise smooth upper bound empirical loss inspired latent structural svms develop loss augmented inference algorithm quadratic code length show strong retrieval performance mnist promising classification results using knn binary codes 
spiking saturating differentially expand single neuron computation capacity integration inputs non linear multiple inputs produce local sum input response taken separately sum spiking smaller saturating decomposing dendritic tree independent dendritic spiking units greatly extends its computational capacity neuron maps layer neural network enabling compute linearly non boolean functions implemented dendritic architectures saturating equally expand computational questions binary neuron model boolean algebra confirm spiking enable neuron compute using architecture based normal form second prove saturating spiking enable neuron compute using architecture based normal form contrary based architecture based architecture leads dendritic unit tuning does imply neuron tuning observed experimentally third show based architecture saturating consequently show important family implemented architecture require exponential number saturating dendritic units whereas family implemented architecture architecture always require linear number spiking dendritic unit minimization explain why neuron its spike 
clustering nonnegative matrix factorization using graph random walk nonnegative matrix factorization nmf promising relaxation technique clustering analysis however conventional nmf methods directly approximate pairwise similarities using square error yield performance data manifolds because capture only immediate similarities data samples propose nmf clustering method approximated matrix its smoothed version using random walk method relationships data samples furthermore introduce novel regularization proposed objective function order improve spectral clustering learning objective optimized multiplicative majorization minimization algorithm scalable implementation learning matrix extensive experimental results real world datasets show method strong performance terms cluster 
delay dynamical synapses time delay pervasive neural information processing achieve real time tracking critical transmission processing neural system present study show dynamical synapses short term depression enhance continuous attractor network extent system tracks time varying stimuli timely manner state network track instantaneous moving stimulus zero lead effectively constant time agreement experiments direction systems parameter regions perfect tracking correspond network states static move moving respectively demonstrating strong correlation tracking performance intrinsic dynamics network speed stimulus natural speed network state delay becomes effectively independent stimulus 
imagenet classification deep convolutional neural networks trained large deep convolutional neural network classify high images imagenet training set different classes data achieved top top error rates considerably better previous state art results neural network parameters neurons consists convolutional layers followed max pooling layers globally connected layers final way softmax training faster used non saturating neurons very efficient implementation convolutional reduce overfitting globally connected layers regularization method proved very effective 
recognizing activities dynamics work consider problem modeling dynamic structure human activities attributes space video sequence represented semantic feature space feature encodes probability occurrence activity given time generative model binary dynamic system proposed learn distribution dynamics different activities space non linear dynamic system extends binary principal component analysis pca classical linear dynamic systems combining binary observation variables hidden markov state process way integrates representation power semantic modeling ability dynamic systems capture temporal structure time varying processes algorithm learning parameters inspired popular learning method dynamic textures proposed similarity measure generalizes kernel introduced used design activity classifiers proposed method shown outperform similar classifiers derived kernel dynamic system state art approaches dynamics based based action recognition 
compressive sensing mri wavelet tree sparsity compressive sensing magnetic resonance imaging mri image good quality only small number measurements significantly reduce time according structured sparsity theory measurements reduced tree sparse data standard sparse data length however existing algorithms utilized mri total variation wavelet sparse regularization side algorithms proposed tree sparsity regularization validated benefit tree structure mri paper propose fast convex optimization algorithm improve mri wavelet sparsity gradient sparsity tree sparsity considered model real images original complex problem simpler efficiently solved iterative scheme numerous experiments conducted show proposed algorithm outperforms state art mri algorithms gain better reconstructions results real images general tree based solvers algorithms 
training sparse natural image models fast gibbs sampler extended state space present learning strategy based efficient blocked gibbs sampler sparse linear models particular placed statistical image modeling models important role discovering sparse representations gibbs sampler faster general purpose sampling schemes requiring tuning parameters using gibbs sampler persistent variant expectation maximization able extract highly sparse distributions latent sources data applied natural images algorithm learns source distributions spike distributions evaluate likelihood quantitatively compare performance linear model its complete counterpart product experts model represents another generalization complete linear model contrast previous claims representations lead significant improvements linear model models 
bayesian approach policy learning trajectory preference queries consider problem learning control policies trajectory preference queries expert particular learning agent present expert short runs pair policies state expert indicates preferred trajectory agent goal latent target policy expert queries possible tackle problem propose novel bayesian model querying process introduce methods exploit model actively select expert queries experimental results benchmark problems indicate model effectively learn policies trajectory preference queries active query selection substantially efficient random selection 
generic ranking algorithm ranking fundamental task machine learning broadly applicable many real world problems information retrieval team product search paper consider generic setting aim top ranking list based arbitrary relevance function arbitrary similarity function examples formulate optimization problem show general hard show large volume parameter space proposed objective function enjoys property enables design scalable greedy algorithm near optimal solution experimental results real data sets demonstrate effectiveness proposed algorithm 
multilabel classification ranking partial feedback present novel multilabel ranking algorithm working partial information settings algorithm based order descent methods relies upper confidence bounds trade exploration exploitation analyze algorithm partial adversarial setting adversarial multilabel probabilities generalized linear models show regret bounds improve several ways existing results effectiveness upper confidence scheme full information baselines real world multilabel datasets obtaining comparable performance 
function svms finding large dense lovasz function graph fundamental tool optimization approximation algorithms computing involves solving extremely expensive sized graphs paper establish lovasz function equivalent kernel learning problem related class svm interesting connection opens many opportunities bridging graph theoretic algorithms machine learning show exist graphs call graphs lovasz function approximated class svm leads novel svm techniques solve algorithmic problems large graphs identifying size random graph classic approach problem involves computing function however scalable due computation show random graph example graph consequence svm based approach easily identifies large graphs competitive state art introduce notion common orthogonal labeling extends notion orthogonal single graph used defining function multiple graphs problem finding optimal common orthogonal cast multiple kernel learning problem used identify large common dense region multiple graphs proposed algorithm achieves order magnitude scalability compared state art 
multi task averaging present multi task learning approach jointly estimate means multiple independent data sets proposed multi task averaging algorithm results convex combination single task averages derive optimal amount regularization show effectively estimated simulations real data experiments demonstrate maximum likelihood estimators approach estimating amount regularization rivals cross performance computationally efficient 
unsupervised structure discovery semantic analysis approaches classification retrieval tasks largely rely detection based discriminative models models assumption mapping directly whereas process likely complex present generative model maps hierarchical manner increasingly higher level model layers being generic sound units clear semantic second layer attempts patterns generic sound units evaluate model large scale retrieval task trecvid report significant improvements standard baselines 
marginalized particle gaussian process regression present novel marginalized particle gaussian process regression provides fast accurate online bayesian filtering framework model latent function using state space model established data procedure recursively filters estimation hidden function values gaussian mixture meanwhile provides online method training hyperparameters number weighted particles demonstrate estimated performance simulated real large data sets results show robust estimation algorithm high computational efficiency outperforms state art sparse methods 
angular quantization based binary codes fast similarity search paper focuses problem learning binary embeddings efficient retrieval high dimensional non negative data data typically arises large number vision applications counts frequencies used features cosine distance commonly used measure dissimilarity vectors work introduce novel spherical quantization scheme generate binary embedding data analyze its properties number quantization scheme grows exponentially data dimensionality resulting low quantization propose very efficient method computing binary embedding using large number linear transformation learned minimize quantization error adapting method input data resulting improved embedding experiments image retrieval applications show superior performance proposed method existing state art methods 
optimal kernel choice large scale sample tests abstract given samples distributions sample determines hypothesis based value statistic measuring distance samples choice statistic maximum mean discrepancy distance embeddings probability distributions reproducing kernel hilbert space kernel used obtaining embeddings critical high power correctly distinguishes unlike distributions high probability means parameter selection sample based proposed given level upper bound probability making type error kernel chosen maximize power minimize probability making type ii error statistic threshold optimization kernel parameters obtained cost linear sample size properties kernel selection procedures suited data streams observations memory experiments kernel selection approach yields powerful earlier kernel selection heuristics 
factoring nonnegative matrices linear programs paper describes approach computing nonnegative matrix linear programming key idea data driven model factorization salient features data used express remaining features precisely given data matrix algorithm identifies matrix satisfies linear constraints matrix selects features used compute low rank nmf theoretical analysis demonstrates approach type guarantees nmf algorithm contrast earlier work proposed method better noise extends general noise models leads efficient scalable algorithms experiments synthetic real datasets provide evidence approach superior practice optimized implementation algorithm factor multi matrix 
large scale distributed deep networks work unsupervised feature learning deep learning shown being able train large models dramatically improve performance paper consider problem training deep network parameters using tens thousands cores developed software framework called utilize computing clusters thousands machines train large models framework developed algorithms large scale distributed training asynchronous stochastic gradient descent procedure large number model ii framework supports variety distributed batch optimization procedures including distributed implementation bfgs bfgs increase scale speed deep network training successfully used system train deep network larger previously reported literature achieves state art performance imagenet visual object recognition task images categories show techniques dramatically accelerate training sized deep network speech recognition focus report performance methods applied training large neural networks underlying algorithms applicable any gradient based machine learning algorithm 
statistical consistency ranking methods rank differentiable probability space paper statistical consistency ranking methods recently proven many commonly used pairwise ranking methods inconsistent weighted pairwise loss viewed true loss ranking low noise setting result interesting surprising given pairwise ranking methods shown very effective practice paper argue aforementioned result depending kind assumptions used give assumption labels objects rank lie rank differentiable probability space prove pairwise ranking methods become consistent assumption especially actually stronger similar low noise setting studies provide theoretical empirical findings pairwise ranking methods before gap theory applications 
wavelet based multi scale shape features arbitrary cortical discrimination hypothesis testing signals cortical surface fundamental component variety studies neuroscience goal identify regions exhibit changes function clinical condition study clinical questions interest move towards identifying very early diseases corresponding statistical differences group level become weaker increasingly hard identify indeed multiple comparisons adopted account correlated statistical tests surface points very regions may contrast hypothesis tests point wise measurements paper case performing statistical analysis multi scale shape descriptors characterize local topological context signal surface vertex descriptors based results harmonic analysis show wavelet theory extends non euclidean settings weighted graphs provide strong evidence descriptors successfully pick group wise differences traditional methods fail yield results primary application show framework allows performing cortical surface smoothing space unit 
convex formulation learning scale networks submodular relaxation key problem statistics machine learning determination network structure data consider case structure graph reconstructed known scale show cases natural formulate structured sparsity inducing priors using submodular functions lovasz extension obtain convex relaxation tractable classes gaussian graphical models leads convex optimization problem efficiently solved show method results improvement accuracy reconstructed networks synthetic data show prior encourages scale reconstructions dataset 
sparsity robust estimation linear models unknown variance paper develop novel approach problem learning sparse representations context sparsity unknown noise level propose algorithm termed scaled aforementioned learning task means second order cone program special put particular instance sparsity corresponding learning presence outliers establish finite sample risk bounds carry experimental evaluation synthetic real data 
prior probability influences decision making unifying probabilistic model does brain combine prior knowledge sensory evidence making decisions competing descriptive models proposed based experimental data posits additive decision variable static effect prior however model inconsistent data motion discrimination task temporal integration uncertain sensory evidence explain data second model proposed assumes time varying influence prior present model decision making incorporates prior knowledge principled way show additive model time varying prior model emerge naturally decision making viewed framework partially observable markov decision processes pomdps decision making model reduces computing beliefs given observations prior information bayesian manner selecting actions based beliefs maximize expected sum future rewards show model explain data previously explained using additive model data time varying influence prior knowledge decision making 
high order multi task feature learning identify alzheimer disease progression prediction alzheimer disease disorder characterized progressive memory cognitive functions regression analysis studied relate neuroimaging measures cognitive however measures predictive power infer trajectory cognitive performance time explored important topic research propose novel high order multi task learning model address issue proposed model explores temporal correlations existing data features regression tasks structured sparsity inducing norms addition sparsity model enables selection small number mri measures maintaining high prediction accuracy empirical studies using baseline mri cognitive data cohort promising results 
symmetric correspondence topic models analysis topic modeling widely used approach analyzing large collections small number topic models recently explored discover latent topics parallel comparable documents topic models originally proposed structured data applicable documents correspondence latent dirichlet allocation model however requires pivot language specified advance propose topic model symmetric correspondence lda incorporates hidden variable control pivot language extension experimented comparable datasets extracted demonstrate effective existing topic models 
effective split merge monte carlo methods nonparametric models sequential data applications bayesian nonparametric methods require learning inference algorithms efficiently explore models unbounded complexity develop markov chain monte carlo methods process hidden markov model bp hmm enabling discovery shared activity patterns large video motion capture databases introducing split merge moves based sequential allocation allow large global changes shared feature structure develop data driven jump moves discover unique behaviors proposals apply any choice conjugate likelihood observed data show success multinomial gaussian autoregressive emission models together innovations allow tractable analysis hundreds time series previous inference required initialization burn iterations just sequences 
efficient sampling bipartite matching problems bipartite matching problems characterize many situations ranging ranking information retrieval correspondence vision exact inference real world applications problems intractable making efficient approximation methods essential learning inference paper propose novel sequential matching sampler based generalization plackett luce model effectively large moves space matchings allows sampler match difficult target distributions common problems highly multimodal distributions present experimental results bipartite matching problems ranking image correspondence show sequential matching sampler efficiently approximates target distribution significantly outperforming sampling approaches 
learning visual motion recurrent neural networks present dynamic nonlinear generative model visual motion based latent representation binary gaussian variables trained sequences images model learns represent different movement directions different variables online approximate inference scheme mapped dynamics networks neurons stimuli moving light neurons model show patterns responses analogous direction selective simple cells primary visual cortex model neurons show speed tuning equally range motion directions speeds aligned constraint line preferred speed show computations enabled specific pattern recurrent connections learned model 
learned trading accuracy speed users want natural language processing nlp systems fast accurate quality comes cost speed field manually exploring various speed accuracy tradeoffs particular problems datasets aim explore space automatically focusing case based parsing unfortunately reinforcement learning techniques fail learn good policies state space simply large explore attempt applying imitation learning algorithms fails far good successfully imitate features moreover specifically tuned known reward function propose hybrid reinforcement apprenticeship learning algorithm only features automatically learn weights achieve competitive accuracies significant improvements speed state art baselines 
value pursuit iteration value pursuit iteration approximate value iteration algorithm finds close optimal policy reinforcement learning planning problems large state spaces main features nonparametric algorithm finds good sparse approximation optimal value function given dictionary features algorithm number features second iteration algorithm set functions based currently learned value function dictionary increases representation power dictionary way directly relevant goal having good approximation optimal value function theoretically study provide finite sample error upper bound 
compressive neural representation sparse high dimensional probabilities paper shows sparse high dimensional probability distributions represented neurons exponential compression representation novel application compressive sensing sparse probability distributions usual sparse signals compressive measurements correspond expected values nonlinear functions distributed variables expected values estimated sampling quality compressed representation limited only quality sampling since compression preserves geometric structure space sparse probability distributions probabilistic computation performed compressed domain functions requirements compressive sensing implemented simple perceptrons perceptrons simple model feedforward computation neurons results show mean activity relatively small number neurons accurately represent high dimensional joint distribution implicitly any noise correlations novel hypothesis neurons encode probabilities brain 
graphical models generalized linear models undirected graphical models markov networks gaussian graphical models ising models variety applications many settings however data may follow gaussian binomial distribution assumed models introduce class graphical models based generalized linear models assuming node wise conditional distributions arise exponential families models allow estimate networks wide class exponential distributions poisson negative binomial exponential fitting penalized select neighborhood node major contribution paper rigorous statistical analysis showing high probability neighborhood graphical models recovered exactly provide examples high throughput networks learned graphical models multinomial poisson distributed data 
extension compressive sensing phase retrieval problem compressive sensing active research fields past years development only applies linear models limits its application many ideas difference paper presents novel extension phase retrieval problem intensity measurements linear system used recover complex sparse signal propose novel solution using lifting technique hard problem nonsmooth semidefinite program analysis shows inherits many desirable properties guarantees exact recovery provide scalable numerical solvers accelerate its implementation source code algorithms provided 
regularized hashing multimodal data hashing based methods provide very promising approach large scale similarity search obtain compact hash codes seeks learn hash functions data automatically paper study hash function learning context multimodal data propose novel multimodal hash function learning method called regularized hashing based regularization framework hash functions bit hash codes learned solving dc difference convex functions programs learning multiple bits boosting procedure bias introduced hash functions sequentially empirically compare state art multimodal hash function learning methods available data sets 
convergence energy landscape cut clustering unsupervised clustering noisy high dimensional data points important difficult problem continuous relaxations balanced cut problems yield excellent clustering results paper provides rigorous convergence results algorithms solve cut minimization algorithm descent algorithm second inverse power method algorithm pro descent algorithm better theoretical convergence properties practice algorithm perform equally completely characterize local minima problem terms original balanced cut problem relate convergence algorithms 
dynamic programming continuous state observation pomdps partially observable markov decision processes pomdps provide powerful model real world sequential decision making problems years point based value iteration methods proven extremely effective techniques ﬁnding approximately optimal dynamic programming solutions pomdps initial set belief states known however point based work provided exact point based continuous state observation spaces tackle paper key insight may number possible observations only number observation relevant optimal decision making set reachable belief states known important contributions show previous exact dynamic pro solutions continuous state mdps generalized state pomdps discrete observations show solution extended recently developed methods continuous state observations derive minimal relevant observation partitioning potentially correlated multivariate observation spaces demonstrate proof concept results multi variate state observation control 
bayesian probabilistic subspace addition modeling data matrices paper introduces probabilistic subspace addition model simultaneously capturing dependent structures rows columns briefly assumes matrix generated additive combination linear mappings features row wise column wise latent subspaces consequently captures dependencies entries able model non gaussian density variational inference proposed approximate bayesian learning updating posteriors formulated problem solving equations furthermore extended missing values adapting its modelling tensor data comparison several state art approaches experiments demonstrate effectiveness efficiency bayesian sparse modeling matrix tensor data missing values 
scaled gradients manifolds matrix completion paper describes gradient methods based scaled metric manifold low rank matrix completion proposed methods significantly improve canonical gradient methods especially conditioned matrices maintaining established global exact recovery guarantees connection form subspace iteration matrix completion scaled gradient descent procedure established proposed conjugate gradient method based scaled gradient outperforms several existing algorithms matrix completion competitive recently proposed methods 
matrix induced regularization multi kernel learning applications neuroimaging multiple kernel learning generalizes svms setting simultaneously trains linear classifier optimal combination given base kernels model complexity typically controlled using various norm vector base kernel mixing coefficients existing methods however exploit potentially useful information kernels input set interact higher order kernel pair relationships easily obtained unsupervised similarity geodesics supervised correlation errors domain knowledge driven mechanisms features used construct show norm penalty arbitrary quadratic function impose desired covariance structure mixing coefficient selection bias learning concept formulation significantly generalizes widely used norm objectives explore model’s utility experiments challenging neuroimaging problem goal predict conversion disease exploiting aggregate information several distinct imaging modalities model outperforms state art values briefly discuss terms learning bounds complexity 
privacy aware learning study statistical risk minimization problems version privacy data learner local privacy framework show sharp upper lower bounds convergence rates statistical estimation procedures consequence exhibit precise tradeoff amount privacy data preserves utility measured convergence rate any statistical estimator 
finite sample convergence rates zero order stochastic optimization methods consider derivative algorithms stochastic optimization problems only noisy function values gradients analyzing finite sample convergence rates show pairs function values available algorithms gradient estimates based random perturbations suffer factor convergence rate traditional stochastic gradient methods dimension problem complement algorithmic development information theoretic lower bounds minimax convergence rate problems show bounds sharp respect problem dependent quantities improved constant factors 
hierarchical region selection driven paper aims step making term reinforcement learning theoretically focusing driven learning consider setting fixed partition continuous space being given process defined being unknown sequentially decide cell partition select sample cell order minimize loss function inspired previous work driven learning loss cell consists term measuring simple worst case quadratic sampling error penalty term proportional range variance cell corresponding problem formulation extends setting known active learning multi armed bandits case arm continuous region show adaptation algorithms problem hierarchical sampling algorithms optimization used order solve problem resulting procedure called hierarchical region selection driven horse provided together finite time regret analysis 
sparse prediction support norm derive novel norm corresponds convex relaxation sparsity combined penalty show norm provides tighter relaxation elastic good lasso elastic sparse prediction problems studying norm bound looseness elastic light providing its 
active learning multi index function models consider problem actively learning multi index functions form point evaluations assume function defined d$ twice differentiable everywhere rank matrix d$ propose randomized active sampling scheme estimating functions uniform approximation guarantees theoretical developments leverage techniques low rank matrix recovery enables derive estimator function sample complexity bounds characterize noise robustness scheme provide empirical evidence high dimensional scaling sample complexity bounds accurate 
learning multiple tasks using shared hypotheses work consider setting very large number related tasks examples individual task learning task having large generalization error learning tasks together using single hypothesis potentially large inherent error consider learning small shared hypotheses task mapped single hypothesis hard association derive dimension generalization bounds model based number tasks shared hypothesis dimension hypotheses class conducted experiments synthetic problems sentiment strongly support approach 
line reinforcement learning using incremental kernel based stochastic factorization ability learn policy sequential decision problem continuous state space using line data long challenge paper presents reinforcement learning algorithm called extends benefits kernel based learning line scenario kernel based method proposed algorithm stable good convergence properties however unlike similar algorithms space complexity independent number sample result process arbitrary amount data present theoretical results showing approximate any level accuracy value function learned equivalent batch non parametric kernel based reinforcement learning order show effectiveness proposed algorithm practice apply challenging balancing task ability process large number crucial achieving high success rate 
forward backward activation algorithm hierarchical hidden markov models hierarchical hidden markov models sophisticated stochastic models enable capture hierarchical context sequence data however existing parameter estimation methods require large computations time complexity model inference depth hierarchy number states level sequence length paper propose inference method time complexity key idea algorithm application forward backward algorithm state activation probabilities notion state activation offers simple hierarchical transition behavior enables conduct model inference efficiently present experiments demonstrate proposed method works efficiently estimate parameters existing methods method gibbs sampling method 
communication efficient algorithms statistical optimization study communication efficient algorithms distributed statistical optimization large scale data algorithm averaging method distributes data samples evenly machines performs separate minimization subset averages estimates provide sharp analysis average mixture algorithm showing set conditions combined parameter achieves mean squared error whenever guarantee matches best possible rate achievable centralized algorithm having access samples second algorithm novel method based appropriate form requiring only single round communication mean squared error robust amount parallelization complement theoretical results experiments large scale problems learning rank dataset 
identifiability latent trees paper explores unsupervised learning parsing models directions models infinite general technique numerically checking identifiability based rank matrix apply several standard dependency parsing models second models estimate parameters em suffers local optima work using spectral methods directly applied since topology tree varies sentences develop strategy additional complexity restricted classes parsing models 
bayesian nonparametric models data develop bayesian nonparametric extension popular plackett luce choice model handle infinite number choice items framework based theory random atomic measures prior specified gamma process derive posterior simple effective gibbs sampler posterior simulation develop time varying extension model apply model times 
feature aware label space dimension reduction multi label classification label space dimension reduction efficient effective paradigm multi label classification many classes existing approaches compressive sensing principal label space transformation exploit only label part dataset feature part paper propose novel approach considers label feature parts approach called conditional principal label space transformation based minimizing upper bound popular hamming loss minimization step approach carried efficiently simple singular value decomposition addition approach extended kernelized version allows sophisticated feature combinations experimental results verify proposed approach effective existing ones many real world datasets 
stochastic optimization sparse statistical recovery optimal algorithms high dimensions develop analyze stochastic optimization algorithms problems expected loss strongly convex optimum approximately sparse previous approaches able exploit only structures convergence rate strongly convex objectives dimensions convergence rate optimum sparse algorithm based successively solving series regularized optimization problems using dual averaging algorithm establish error solution iterations natural extensions approximate sparsity results apply locally losses including logistic exponential hinge squares losses statistical minimax results show convergence rates optimal constants effectiveness approach numerical simulations compare several baselines squares regression problem 
graphical gaussian vector image paper proposes novel image representation called graphical gaussian vector counterpart local feature matching approaches method model distribution local features gaussian markov random field efficiently represent spatial relationship local features consider parameter feature vector image using concepts information geometry proper parameters metric obtained finally define image feature embedding metric parameters directly applied scalable linear classifiers method obtains superior performance state art methods standard object recognition datasets comparable performance scene dataset proposed method simply local auto correlations local features able achieve high classification accuracy high efficiency 
joint modeling matrix associated latent binary features methodology developed joint analysis matrix documents documents associated matrix rows columns documents modeled focused topic model inferring latent binary features topics document matrix decomposition developed latent binary features associated rows columns low rank constraint matrix decomposition topic model coupled sharing latent binary feature vectors associated model applied call data associated documents defined state art results prediction piece based only observed coupling demonstrated yield insight properties matrix decomposition call data 
proper losses learning partial labels paper discusses problem posterior class probabilities partially data instance assumed belonging several candidate categories being true generalize concept proper loss scenario establish necessary sufficient condition loss function proper show direct procedure construct proper loss partial labels conventional proper loss problem characterized mixing probability matrix relating true class data observed labels interesting result full knowledge matrix required losses constructed proper subset probability simplex 
iterative thresholding algorithm sparse inverse covariance estimation sparse graphical modelling inverse covariance selection important problem machine learning seen significant advances years major focus methods perform model selection high dimensions numerous convex regularization approaches proposed literature however clear methods optimal any defined sense major gap regard rate convergence proposed optimization methods address iterative thresholding algorithm numerically solving penalized maximum likelihood problem sparse inverse covariance estimation presented proximal gradient method considered paper shown converge linear rate result its kind numerically solving sparse inverse covariance estimation problem convergence rate provided closed form related condition number optimal point numerical results demonstrating proven rate convergence presented 
selecting diverse features spectral regularization study problem diverse feature selection linear regression selecting small subset diverse features predict given objective diversity useful several reasons robustness noise propose several spectral regularizers capture notion diversity features show submodular set functions regularizers added objective function linear regression result approximately submodular functions approximately efficient greedy local search algorithms provable guarantees compare algorithms traditional greedy regularization schemes show obtain diverse set features result regression problem being stable perturbations 
monte carlo methods maximum margin supervised topic models effective strategy exploit side information discovering predictive topic representations impose discriminative constraints induced information posterior distributions topic model strategy adopted number supervised topic models employs max margin posterior constraints however unlike likelihood based supervised topic models posterior inference carried using bayes rule max margin posterior constraints monte carlo methods infeasible directly applicable thereby limited choice inference algorithms based variational approximation strict mean field assumptions paper develop efficient monte carlo methods weaker assumptions max margin supervised topic models based importance sampler collapsed gibbs sampler respectively convex dual formulation report thorough experimental results compare approach favorably existing alternatives accuracy efficiency 
parametric local metric learning nearest neighbor classification study problem learning local metrics nearest neighbor classification previous works local metric learning learn number local unrelated metrics independence approach increased flexibility its considerable risk overfitting present parametric local metric learning method learn smooth metric matrix function data manifold using approximation error bound metric matrix function learn local metrics linear combinations basis metrics defined points different regions instance space constrain metric matrix function imposing linear combinations manifold regularization makes learned metric matrix function vary geodesics data manifold metric learning method excellent performance terms predictive power scalability experimented several large scale classification problems tens thousands instances compared several state art metric learning methods global local svm automatic kernel selection outperforms significant manner 
linear time active learning algorithm link classification present very efficient active learning algorithms link classification networks algorithms motivated stochastic model edge labels obtained perturbations initial sign assignment consistent clustering nodes provide theoretical analysis model showing achieve optimal constant factor number any graph order querying order edge labels generally show algorithm achieves optimality factor order querying order edge labels running time algorithm order 
bayesian gaussian processes gaussian processes model output observations regression tasks parametric nonlinear transformation gaussian process nonlinear transformation part probabilistic model shown enhance performance providing better prior model several data sets order learn its parameters maximum likelihood used work show possible non parametric nonlinear transformation integrate resulting bayesian able work scenarios maximum likelihood failed low data regime data values classification demonstrate superior performance bayesian gps several real data sets 
nonparametric reduced rank regression propose approach multivariate nonparametric regression generalizes reduced rank regression linear models additive model estimated dimension dimensional response shared dimensional predictor variable control complexity model employ functional form nuclear norm resulting set function estimates low rank algorithms derived using nonparametric form nuclear norm subdifferential oracle inequalities excess risk derived exhibit scaling behavior procedure high dimensional setting methods illustrated gene expression data 
multiresolution analysis symmetric group generally accepted way define permutations address issue introducing notion based multiresolution analysis symmetric group corresponding wavelet functions describe fast wavelet transform complexity small sparse signals contrast complexity typical discuss potential applications ranking sparse approximation multi object tracking 
hashing existing hashing methods adopt projection functions project original data several dimensions real values projected dimensions bit zero thresholding typically variances different projected dimensions different existing projection functions principal component analysis pca using number bits different projected dimensions because larger variance dimensions carry information viewpoint widely accepted many researchers verified theory experiment because methods proposed projection equal variances different dimensions paper propose novel method called hashing learn projection functions produce projected dimensions variances equal variances experimental results real data sets show outperform its counterpart different variances different dimensions viewpoint projections variances better variances 
lifting gibbs sampling algorithm statistical relational learning models combine power order logic tool handling relational structure probabilistic graphical models tool handling uncertainty probabilistic inference algorithms subject research main idea algorithms improve speed accuracy scalability existing graphical models inference algorithms exploiting symmetry order representation paper consider blocked gibbs sampling variation classic gibbs sampling algorithm order level propose achieve partitioning order atoms relational model set disjoint clusters exact inference polynomial cluster given assignment atoms cluster propose approach constructing clusters determining complexity show used trade accuracy computational complexity principled manner experimental evaluation shows gibbs sampling superior algorithm terms accuracy convergence 
connections tracking model visual tracking recently proposed model based hypothesis tracking tracking achieved top tuning based target features discriminant center surround mechanisms time work identify main predictions hypothesis true tracking reliability larger salient non salient tracking reliability dependence defining variables namely feature contrast dependence variables tracking implemented common low level neural mechanisms confirm predictions results set human behavior studies connection tracking show third prediction holds constructing common plausible architecture computationally solve tracking architecture fully standard physiological models known attentional control area explaining results human behavior experiments 
convex multi view subspace learning subspace learning seeks low dimensional representation data enables accurate reconstruction however many applications data obtained multiple sources single source object viewed cameras different document images conditional independence separate sources constraints shared latent representation improve quality learned low dimensional representation paper present convex formulation multi view subspace learning conditional independence reducing dimensionality formulation develop efficient algorithm recovers optimal data reconstruction exploiting implicit convex regularizer recovers corresponding latent representation reconstruction model jointly optimally experiments illustrate proposed method produces high quality results 
spectral learning linear dynamics linear observations application neural population data latent linear dynamical systems linear observation models arise variety applications example modelling spiking activity populations neurons show spectral learning methods linear systems gaussian observations usually called subspace identification context extended estimate parameters dynamical system models observed non gaussian noise models approach obtain estimates parameters dynamical model neural population data observed spike counts poisson distributed log rates determined latent dynamical process possibly driven inputs show extended system identification algorithm consistent accurately recovers correct parameters large simulated data sets smaller computational cost approximate expectation maximisation em due non iterative nature subspace identification smaller data sets provides effective initialization em leading robust performance faster convergence benefits shown extend real neural data 
statistical learning statistical learning sequential prediction different related study quality predictions mapping relations transferring ideas active area provide another piece showing important concept sequential prediction loss natural counterpart statistical setting call stochastic just ordinary characterizes fast rates worst case regret sequential prediction stochastic characterizes fast rates statistical learning show special case log loss stochastic reduces known usually condition used existing convergence theorems minimum description length bayesian inference case loss reduces margin condition case model consideration contains possible predictors equivalent ordinary 
waveform driven plasticity devices model implementation devices recently proposed efficient implementations synapses systems plasticity devices change defined applied behavior resembles biological synapses plasticity triggered mechanisms determined local however learning devices far approached mostly level focus finding any waveform achieves spike timing dependent plasticity stdp regard biological said important forms plasticity bridging gap plasticity model driven neuron explains large number experimental observations adapt characteristics recently introduced based approach show stdp time learning window superior previous based stdp implementations demonstrate measurements possible short long term plasticity device form known triplet plasticity best knowledge implementations triplet plasticity any physical device 
filter model visual early stages visual processing thought temporally varying signals because typical correlation time natural stimuli extent temporal receptive fields neurons greater neuronal time constants done stages combining contributions multiple neurons propose model temporal visual filter signal processing device stage wise temporal signals stage wise architecture filter maps naturally visual cells cells its filter weights learned using hebbian rules stage wise sequential manner moreover predictions neural activity filter model consistent physiological measurements neurons fly second order visual neurons therefore filter model useful abstraction may visual system function 
semantic kernel forests multiple learning features complex visual recognition problems labeled image exemplars alone object specifying categories semantic relationships learning process relationships relevant given visual classification task does single capture relevant light issues propose discriminative feature learning approach leverages multiple hierarchical representing different semantic views object categories classes reflect another reflect learn tree semantic kernels node kernel optimized distinguish classes its children nodes using resulting semantic kernel forest learn class specific kernel combinations select only relationships relevant recognize object class learn weights introduce novel hierarchical regularization term exploits structure demonstrate method challenging object recognition datasets show multiple views yields significant accuracy improvements 
causal discovery scale mixture model variance dependencies conventional causal discovery structural equation models directly applied observed variables meaning causal effect represented function direct causes themselves however many real world problems significant dependencies variances energies indicates may possibly level variances energies paper propose probabilistic causal scale mixture model variance dependencies represent specific type generating mechanism observations particular causal mechanism including temporal causal relations variances energies represented structural vector autoregressive model prove identifiability model non gaussian assumption processes propose algorithms estimate involved parameters discover causal structure experiments real world data conducted show applicability proposed model algorithms 
natural images gaussian mixtures simple gaussian mixture models learned pixels natural image patches recently shown surprisingly strong modeling statistics natural images provide depth analysis simple rich model show model able compete successful models natural images log likelihood scores denoising performance sample quality provide analysis model learns natural images function number mixture components including covariance structure contrast variation structures textures boundaries finally show salient properties learned natural images derived simplified model explicitly models occlusion explaining its surprising success relative models 
dual space analysis sparse linear model sparse linear generalized linear models combine standard likelihood function sparse prior unknown coefficients priors expressed maximization zero mean gaussians different variance hyperparameters standard map estimation type involves maximizing hyperparameters coefficients empirical bayesian alternative type ii marginalizes coefficients maximizes hyperparameters leading tractable posterior approximation underlying cost functions related dual space framework allows type type ii objectives expressed coefficient space perspective useful because analyses extensions development space consider estimation trade parameter balancing sparsity data fit parameter effectively variance natural estimators exist assessing problem hyperparameter variance space natural ideas type ii solve intuitive type contrast analyses update rules sparsity properties local global solutions extensions general likelihood models leverage coefficient space techniques developed type apply type ii example allows prove type ii inspired techniques successful recovering sparse coefficients restricted isometry properties rip lead popular reconstructions facilitates analysis type ii non gaussian likelihood models lead intractable 
active comparison prediction models address problem comparing risks given predictive models instance baseline model possible fixed labeling budget problem occurs whenever models compared held training data possibly because training data reflect desired distribution case instances drawn labeled cost devise active comparison method selects instances according sampling distribution derive sampling distribution maximizes power statistical applied observed empirical risks thereby minimizes likelihood choosing inferior model empirically investigate model selection problems several classification regression tasks study accuracy resulting values 
online regret bounds continuous reinforcement learning derive sublinear regret bounds reinforcement learning continuous state space proposed algorithm combines state aggregation upper confidence bounds implementing face uncertainty existence optimal policy satisfies poisson equation only assumptions rewards transition probabilities 
semi crowdsourced clustering generalizing labeling robust distance metric learning main challenges data clustering define appropriate similarity measure objects addresses challenge defining pairwise similarity based annotations obtained crowdsourcing despite its encouraging results key limitation only cluster objects annotations available address limitation propose approach clustering called semi crowdsourced clustering effectively combines low level features objects annotations subset objects obtained crowdsourcing key idea learn appropriate similarity measure based low level features objects annotations only small portion data clustered difficulty learning pairwise similarity measure significant amount noise inter worker variations annotations obtained crowdsourcing address difficulty developing metric learning algorithm based matrix completion method empirical study real world image data sets shows proposed algorithm outperforms state art distance metric learning algorithms clustering accuracy computational efficiency 
learning curves multi task gaussian process regression study average case performance multi task gaussian process regression captured learning curve average bayes error chosen task versus total number examples tasks product input dependent covariance function form inter task covariance matrix show accurate approximations learning curve obtained arbitrary number tasks study asymptotic learning behaviour large surprisingly multi task learning asymptotically essentially examples tasks only degree inter task correlation near its maximal value effect extreme learning smooth target functions described squared exponential kernels demonstrate learning many tasks learning curves separate initial phase bayes error task reduced value learning tasks seen examples final decay occurs only once number examples proportional number tasks 
kernel offer regularized kernel extension multi set orthogonal problem method called kernel scope include nonlinear measures similarity enables alignment multiple datasets large number base features direct application fmri data analysis kernel suited multi subject alignment large including entire cortex conducted experiments using real world multi subject fmri data 
multiple choice learning learning produce multiple structured outputs paper addresses problem generating multiple hypotheses prediction tasks involve interaction users successive components cascade given set multiple hypotheses components users ability automatically rank results retrieve best standard approach handling scenario learn single model produce best maximum map hypotheses model contrast formulate multiple choice learning task multiple output structured output prediction problem loss function captures natural problem present max margin formulation minimizes upper bound loss function experimental results problems image segmentation protein side chain prediction show method outperforms conventional approaches used scenario leads substantial improvements prediction accuracy 
mixing properties conditional markov chains unbounded feature functions conditional markov chains known linear chain conditional random fields literature class discriminative models distribution sequence hidden states conditional sequence observable variables large sample properties conditional markov chains studied paper extends work directions mixing properties models unbounded feature functions being established second necessary conditions model identifiability maximum likelihood estimates being given 
persistent learning densities bounded support present novel method learning densities bounded support enables incorporate topological constraints particular show emerging techniques computational algebraic topology notion persistent combined kernel based methods machine learning purpose density estimation proposed formalism facilitates learning models bounded support principled way incorporating persistent techniques approach able encode algebraic topological constraints addressed current state art probabilistic models study behaviour method synthetic examples various sample sizes benefits proposed approach real world data set learning motion model show learn model respects underlying topological structure trajectories car 
non stationary policies stationary infinite horizon markov decision processes consider infinite horizon stationary discounted markov decision processes known stationary optimal policy using value policy iteration error iteration known compute stationary policies optimal guarantee tight develop variations value policy iteration computing non stationary policies optimal significant improvement usual situation close surprisingly shows problem near optimal non stationary policies simpler near optimal stationary policies 
efficient spike coding multiplicative adaptation spike response model neural adaptation ability neurons maximize encoded information wide dynamic range input stimuli adaptation intrinsic feature neuronal models model challenge integrate adaptation models neural computation computational models adaptive spike response model implement adaptation spike based addition fixed size fast spike triggered threshold dynamics slow spike triggered currents adaptation shown accurately model neural spiking behavior limited dynamic range taking cue models adaptation propose multiplicative adaptive spike response model spike triggered adaptation dynamics scaled adaptation state time spiking show unlike additive adaptation model firing rate multiplicative adaptation model maximum spike rate variance switching experiments model quantitatively experimental data wide dynamic range furthermore dynamic threshold models adaptation suggest straightforward interpretation neural activity terms dynamic signal encoding shifted weighted exponential kernels show encoding rectified filtered stimulus signals multiplicative adaptive spike response model achieves high coding efficiency maintains efficiency changes dynamic signal range several orders magnitude changing model parameters 
map inference chains using column generation linear chains trees basic building blocks many applications graphical models exact inference models performed dynamic programming computation expensive non trivial target variable domain sizes due quadratic dependence size standard message passing algorithms problems inefficient because compute scores hypotheses strong negative local evidence reason significant previous interest search its variants however methods provide only approximate inference paper presents efficient exact inference algorithms based combination column generation pre computed bounds model cost structure improving worst case performance impossible however method substantially speeds real world typical case inference chains trees experiments show method twice fast exact part speech tagging times faster joint part speed recognition task algorithm techniques approximate inference faster best inference opportunities connections inference learning 
bayesian nonparametric modeling attempts related conditions database contains large amount information regarding way conditions depression representative sample population present paper seeking hidden causes attempts propose model subjects using nonparametric latent model based indian buffet process due nature data need adapt observation model discrete random variables propose generative model observations drawn multinomial distribution given matrix implementation efficient gibbs sampler accomplished using approximation allows integrate weighting factors multinomial likelihood model finally experiments database show model properly captures hidden causes model attempts 
random fields large scale spectral approach statistical network modeling statistical models networks typically strong prior assumptions concerning form modeled distributions moreover vast majority currently available models explicitly designed capturing specific graph properties power law degree distributions makes application domains behavior target quantities known priori key contribution paper introduce statistic based laplacian spectrum graphs allows any parametric assumption concerning modeled network properties second defined statistic develop random field model allows efficient estimation edge distributions large scale random networks analyzing dependence structure involved random fields estimate several real world networks showing achieve higher modeling accuracy known statistical approaches 
plausible reinforcement learning working memory tasks key function abstraction information environment neurons association cortex play important role process during learning neurons become tuned relevant features represent information required persistent activity however known neurons acquire task relevant tuning introduce biologically plausible learning scheme explains neurons become selective relevant information animals learn trial error propose action selection stage attentional signals earlier processing levels feedback signals interact feedforward signals form synaptic tags connections stimulus response mapping globally signal synapses determine sign strength plasticity learning scheme generic because train networks different tasks simply varying inputs rewards explains neurons association cortex learn store task relevant information non linear stimulus response mapping tasks learn optimally integrate probabilistic evidence perceptual decision making 
efficient monte carlo regret minimization games many player actions regret minimization popular iterative algorithm computing strategies extensive form games monte carlo variants reduce iteration time cost sampled portion tree previous effective instances very slow games many player actions since sample every action given player paper present algorithm average strategy sampling samples subset player actions according player average strategy algorithm inspired tighter bound number iterations required converge given solution quality addition prove similar tighter bound popular variants finally validate work demonstrating converges faster previous algorithms limit 
mixtures discrete mixtures used broad applications ranging unsupervised settings fully supervised multi task learning indeed finite mixtures infinite mixtures relying dirichlet processes become standard tool important issue arises using discrete mixtures low separation components particular different components introduced very similar hence redundant leads many clusters similar performance unsupervised learning leading computational problems complex model supervised settings arise absence penalty components placed close together bayesian approach used learn number components solve problem propose novel prior components process automatically redundant components characterize prior theoretically propose markov chain monte carlo sampling algorithm posterior computation methods illustrated using synthetic examples data set 
fully bayesian inference neural models negative binomial spiking characterizing information carried neural populations brain requires accurate statistical models neural spike responses negative binomial distribution provides convenient model spike counts responses greater poisson variability describe powerful data augmentation framework fully bayesian inference neural models negative binomial spiking approach relies recently described latent variable representation negative binomial distribution gamma mixture normals framework provides tractable conditionally gaussian representation posterior used design efficient em gibbs sampling based algorithms inference regression dynamic factor models apply model neural data primate retina show substantially outperforms poisson regression held data reveals latent structure underlying spike count correlations simultaneously spike trains 
slice normalized dynamic markov logic networks markov logic widely used tool statistical relational learning uses weighted order logic knowledge base specify markov random field conditional random field many applications markov logic network trained domain used different paper focuses dynamic markov logic networks domain time points typically varies training testing previously marginal probabilities truth assignments ground atoms change extends reduces domains show addition problem standard way markov logic theory may result time underlying markov chain furthermore problems significant given domain show practical problem generating samples sequential conditional random field slice relying samples previous slice high computational cost general case due need estimate normalization factor sample propose discriminative model slice normalized dynamic markov logic networks suffers issues supports efficient online inference directly model influences variables time slice causal direction contrast fully directed models experimental results show improvement accuracy previous approaches online inference dynamic markov logic networks 
meta gaussian information bottleneck present information bottleneck problem terms copula using equivalence mutual information negative copula entropy focusing gaussian copula extend analytical solution available multivariate gaussian case distributions gaussian dependence structure arbitrary marginal densities called meta gaussian distributions opens applications continuous data provides solution robust outliers 
diffusion decision making adaptive nearest neighbor classification paper light fundamental connections diffusion decision making model neuroscience cognitive nearest neighbor classification show conventional nearest neighbor classification viewed special problem diffusion decision model asymptotic situation applying optimal strategy associated diffusion decision model adaptive rule developed determining appropriate values nearest neighbor classification making sequential probability bayesian analysis propose different criteria adaptively nearest experiments synthetic real datasets demonstrate classification criteria 
perturbed variation introduce discrepancy score distributions gives similarity research done determine samples come exactly distribution research considered problem determining finite samples come similar distributions score gives intuitive interpretation similarity optimally distributions best fit score defined distributions efficiently estimated samples provide convergence bounds estimated score develop hypothesis testing procedures data sets come similar distributions statistical power procedures presented simulations compare score capacity detect similarity known measures real data 
communication computation tradeoffs consensus based distributed optimization study scalability consensus based distributed optimization algorithms considering questions many given problem communicate communication central analysis problem specific value communication computation tradeoff show organizing communication nodes regular yields pairs nodes communicate complete graph optimal number depends surprisingly speedup obtained terms time reach fixed level accuracy frequently computation experiments real cluster solving metric learning non smooth convex minimization tasks demonstrate strong agreement theory practice 
noise parameter estimation diffusion processes stochastic differential equations natural tool modelling systems inherently noisy contain uncertainties modelled stochastic processes crucial process using build mathematical models ability estimate parameters models observed data past significant progress problem far having solution describe novel method approximating diffusion process show useful markov chain monte carlo mcmc inference algorithms noise drives diffusion process terms term controlled set variables second term small enables form linear gaussian approximation decomposition allows diffusion process interest cast form amenable sampling mcmc methods explain why many state art inference methods fail highly nonlinear inference problems demonstrate experimentally method performs situations results show method promising tool inference parameter estimation problems 
online allocation partitioning piecewise constant mean approximation setting active learning multi armed bandit goal learner estimate equal precision mean finite number arms results show possible derive strategies based finite time confidence bounds competitive best possible strategy consider extension problem case arms cells finite partition continuous sampling space goal build piecewise constant approximation noisy function piece region fixed order maintain local quadratic error approximation cell equally low extension trivial show simple algorithm based upper confidence bounds proved adaptive function near optimal way chosen minimax optimal order class functions 
learning map inference discrete graphical models present formulation binary classification problems relying convex losses svms logistic regression boosting non convex continuous formulations encountered neural networks deep belief networks framework entails non convex discrete formulation estimation amounts finding map configuration graphical model potential functions low dimensional discrete surrogates loss argue discrete formulation naturally account number issues typically encountered convex continuous non convex paradigms reducing learning problem map inference problem immediately guarantees available many inference settings learning problem empirically demonstrate number experiments approach promising dealing issues label noise having global optimality guarantees due discrete nature formulation allows direct regularisation cardinality based penalties pseudo norm providing ability perform feature selection trade principled manner number open problems arising formulation 
mechanistic model early sensory processing based sparse representations early stages sensory systems face challenge information numerous smaller number projection neurons called communication bottleneck efficient limited compression may achieved using predictive coding redundant components stimulus case retina suggested feedforward connections linear prediction generated nearby implement compression resulting center surround receptive fields however feedback circuits common early sensory circuits furthermore dynamics may nonlinear circuits implement predictive coding solving dynamics nonlinear feedback circuits signal processing algorithm called bregman iteration show nonlinear predictive coding implemented feedback response step stimulus activity time constructs sparse accurate representations stimulus temporally evolving prediction analysis provides powerful theoretical framework interpret understand dynamics early sensory processing variety physiological experiments yields novel predictions regarding relation activity stimulus statistics 
multi stage multi task feature learning multi task sparse feature learning aims improve generalization performance exploiting shared features tasks successfully applied many applications including computer vision existing multi task sparse feature learning algorithms formulated convex sparse regularization problem usually suboptimal due its looseness approximating type regularizer paper propose non convex formulation multi task sparse feature learning based novel regularizer solve non convex optimization problem propose multi stage multi task feature learning algorithm moreover present detailed theoretical analysis showing achieves better parameter estimation error bound convex formulation empirical studies synthetic real world data sets demonstrate effectiveness comparison state art multi task sparse feature learning algorithms 
parts motion based segmentation objects develop method discovering parts object aligned capturing various dimensional adapt distance dependent process allow nonparametric discovery potentially unbounded number parts simultaneously spatially connected segmentation allow analysis datasets object instances varying shapes model part variability affine transformations placing matrix normal inverse prior affine transformations develop gibbs sampler marginalizes transformation uncertainty analyzing dataset humans captured infer parts provide quantitatively better motion predictions conventional clustering methods 
classification using constrained variational gaussian process dynamical system paper describes acoustic model based variational gaussian process dynamical system classification proposed model overcomes limitations classical hmm modeling real speech data nonlinear nonparametric model model prior dynamics function enables representing complex dynamic structure speech prior emission function successfully models global dependency observations introduce variance constraint original sparse approximation error kernel matrix effectiveness proposed model demonstrated extensive experimental results including parameter estimation classification performance synthetic benchmark datasets 
bayesian estimation discrete entropy mixtures breaking priors consider problem estimating shannon entropy sampled regime number possible may unknown infinite processes generalization dirichlet processes provide tractable prior distributions space infinite discrete distributions major applications bayesian non parametric statistics machine learning show provide natural priors bayesian entropy estimation due remarkable fact moments induced posterior distribution computed analytically derive posterior mean bayes squares estimate variance priors moreover show fixed dirichlet process prior implies prior meaning prior strongly determines entropy estimate sampled regime derive family continuous mixing measures resulting mixture processes produces approximately flat prior explore theoretical properties resulting estimator show performs data sampled exponential power law distributions 
geometric metric learning multi metric learning techniques learn local metric different parts feature space approach simple classifiers competitive state art because distance measure locally adapts structure data learned distance measure however non metric multi metric learning generalizing tasks dimensionality reduction regression principled way prove appropriate changes multi metric learning corresponds learning structure riemannian manifold show structure gives principled way perform dimensionality reduction regression according learned metrics provide practical algorithm computing geodesics according learned metrics algorithms computing exponential logarithmic maps riemannian manifold together tools many euclidean algorithms advantage multi metric learning illustrate approach regression dimensionality reduction tasks involve predicting measurements human body shape data 
learning architecture sum product networks using clustering variables sum product network recently proposed deep model consisting network sum product nodes shown competitive state art deep models certain difficult tasks image completion designing network architecture suitable task hand open question propose algorithm learning architecture data idea cluster variables opposed data instances order identify variable subsets strongly interact another nodes network towards explaining interactions experimental evidence shows learning architecture significantly improves its performance compared using previously proposed static architecture 
tracking optimal regression function paper possibility option context squares regression shown using theoretically possible learn regressors track best regressor hypothesis class only bounded portion domain moreover volume training set size certain conditions develop efficient exact implementation selective regressors case linear regression empirical evaluation real world datasets theoretical analysis indicates selective regressors provide substantial advantage reducing estimation error 
bayesian nonparametric models bipartite graphs develop novel bayesian nonparametric model random bipartite graphs model based theory completely random measures able handle potentially infinite number nodes show model properties particular may exhibit power law behavior derive posterior indian buffet generative process network simple efficient gibbs sampler posterior simulation model shown fitted several real world social networks 
reducing statistical time series problems binary classification show binary classification methods developed work data used solving statistical problems seemingly unrelated classification concern highly dependent time series specifically problems time series clustering testing sample problem addressed algorithms construct solving problems based metric time series distributions evaluated using binary classification methods consistency proposed algorithms proven general assumptions theoretical results illustrated experiments synthetic real world data 
tractable objectives robust policy optimization robust policy optimization risk aversion plays role real world decision making faced uncertainty effects actions policy maximizes expected utility unknown parameters system may carry risk performance prefer lower utility expectation order avoid reduce likelihood levels utility parameter paper bayesian approach parameter uncertainty unlike methods avoid making any distributional assumptions form uncertainty focus identifying optimization objectives solutions efficiently approximated introduce measures very general class objectives robust policy optimization existing approaches including ones known intractable introduce broad family robust policies approximated efficiently finally frame objectives context player zero sum extensive form game employ regret algorithm approximate optimal policy computation only polynomial number states actions mdp 
classification calibration dimension general multiclass losses study consistency properties surrogate loss functions general multiclass classification problems defined general loss matrix extend notion classification calibration studied binary multiclass classification problems certain specific learning problems general multiclass setting derive necessary sufficient conditions surrogate loss classification calibrated respect loss matrix setting introduce notion classification calibration dimension multiclass loss matrix measures prediction space possible design convex surrogate classification calibrated respect loss matrix derive upper lower bounds quantity results analyze various loss matrices particular application provide different result analyzing difficulty designing dimensional convex surrogates consistent respect pairwise subset ranking losses classification calibration dimension may prove useful tool study design surrogate losses general multiclass learning problems 
collaborative gaussian processes preference learning present model based gaussian processes gps learning pairwise preferences expressed multiple users inference simplified using preference kernel gps allows combine supervised learning user preferences unsupervised dimensionality reduction multi user systems model only exploits collaborative information shared structure user behavior may incorporate user features available approximate inference implemented using combination expectation propagation variational bayes finally present efficient active learning strategy querying preferences proposed technique performs favorably real world data state art multi user preference learning algorithms 
approximating parameterized optimization problems consider abstract class optimization problems parameterized single parameter show solution path parameter always approximated accuracy set size lower bound size shows upper bound tight constant factor devise algorithm step size oracle computes approximate path size finally provide implementation oracle soft margin support vector machines parameterized semi definite program matrix completion 
gradient based kernel method feature extraction variable selection propose novel kernel approach dimension reduction supervised learning feature extraction variable selection constructs small number features predictors finds subset predictors method linear feature extraction proposed using gradient regression function based development kernel method comparison existing methods proposed wide applicability strong assumptions regressor type variables uses computationally simple applicable large data sets second combination sparse penalty method extended variable selection following approach experimental results show proposed methods successfully effective features variables parametric models 
versus choice decision making alternative choice tasks choice paradigms commonly used study sensory cognitive processing choice behavior thought sensory component removing need response selection consistent bias towards response higher rates task suggests possible fundamental differences sensory cognitive processes tasks existing mechanistic models choice tasks mostly variants diffusion model related leaky competing models capture various aspects behavior address bias adjustment response implicit cost structure response requires response deadline response immediately current trial show bayes risk minimizing decision policy minimizes error rate average decision delay naturally exhibits experimentally observed bias optimal decision policy formally equivalent time varying threshold initially stimulus near response deadline initial rise due temporal advantage choosing response fixed delay response show fitting simpler fixed threshold optimal model result higher threshold decision making previously observed direct fit data approximations bias observed decision making may arise rational cost structure need imply additional differences underlying sensory cognitive processes 
versus edge representations towards scalable modeling networks paper argue representing networks motifs particularly important network problems current model based approaches handle poorly due computational incurred using edge representations approaches require edges edges missing edges provided input consequence approximate inference algorithms models usually require time iteration application larger real world networks contrast modeling requires computation providing equivalent better inference quality vertex containing edges number motifs i$ degree vertex smaller low maximum degree networks using representation develop novel mixed membership network model approximate inference algorithm suitable large networks low max degree networks high maximum degree motifs naturally subsampled node fashion allowing faster inference small cost accuracy empirically demonstrate approach compared edge based model faster improved accuracy mixed membership community detection conclude large scale demonstration node network infeasible network models inference cost 
relax value algorithms show principled way deriving online learning algorithms minimax analysis various upper bounds minimax value previously thought non shown yield algorithms allows recover known methods derive ones capturing methods follow perturbed forecaster understanding inherent complexity learning problem leads development algorithms illustrate approach present several algorithms including family randomized methods idea random play versions follow perturbed algorithms presented methods based dimension efficient methods matrix completion trace norm algorithms problems learning prediction static experts 
minimax multi task learning generalized loss compositional paradigm mtl since its multi task learning mtl minimize task wise mean empirical risks introduce generalized loss compositional paradigm mtl includes spectrum formulations spectrum minimax mtl mtl formulation minimizes maximum tasks empirical risks certain relaxation minimax mtl obtain mtl formulations minimax mtl classical mtl full paradigm loss compositional operating vector empirical risks incorporates minimax mtl its relaxations many mtl formulations special cases show theoretically minimax mtl avoid worst case outcomes drawn tasks learning learn setting results several mtl formulations synthetic real problems mtl settings encouraging 
optimal neural tuning curves arbitrary stimulus distributions infomax minimum loss work study stimulus distribution influences optimal coding individual neuron closed form solutions optimal tuning curve provided neuron poisson statistics given stimulus distribution consider variety optimality criteria including maximizing maximizing mutual information minimizing estimation error general p$ norm generalize rao lower bound show p$ loss functional fisher information asymptotic limit proving moment convergence certain functions poisson random variables manner show optimal tuning curve depends loss function equivalence maximizing mutual information minimizing p$ loss limit goes zero 
algorithms learning markov field policies present graph based approach incorporating domain knowledge reinforcement learning applications domain knowledge given weighted graph kernel matrix indicates states similar optimal actions introduce bias policy search process deriving distribution policies policies provided graph low probabilities distribution corresponds markov random field present reinforcement apprenticeship learning algorithms finding policy distributions illustrate advantage proposed approach problems balancing smooth robot grasp objects 
affine independent variational inference present method approximate inference broad class non conjugate probabilistic models particular family generalized linear model target densities describe rich class variational approximating densities best fit target minimizing kullback leibler divergence approach based using fourier representation show results efficient scalable inference 
learning wisdom minimax entropy important way large training sets noisy labels propose minimax entropy principle improve quality labels method assumes labels generated probability distribution workers items labels maximizing entropy distribution method naturally infers item worker infer ground truth minimizing entropy distribution show minimizes kullback leibler kl divergence probability distribution unknown truth show simple coordinate descent scheme optimize minimax entropy empirically results substantially better previously published methods problem 
clustering sparse graphs develop algorithm cluster sparse unweighted graphs partition nodes disjoint clusters higher density clusters low clusters sparsity mean setting cluster cluster edge densities very small possibly vanishing size graph sparsity makes problem hence difficult solve any clustering involves tradeoff minimizing kinds errors missing edges clusters present edges clusters insight sparse case penalized differently analyze algorithm performance natural classical widely studied partition model called stochastic block model show algorithm cluster sparser graphs smaller clusters previous methods seen empirically 
sketch based linear value function approximation hashing common method reduce large potentially infinite feature vectors fixed size reinforcement learning hashing used conjunction coding represent states continuous spaces hashing promising approach value function approximation large discrete domains feature vectors constructed combining set atomic features unfortunately typical hashing value function approximation results biased value estimates due possibility work data stream led development sketch unbiased estimator approximating inner work application data structure linear value function approximation reinforcement learning setting sketch leads biased value estimates show bias orders magnitude standard hashing provide empirical results rl benchmark domains games highlight superior learning performance hashing 
multimodal learning deep boltzmann machines propose deep boltzmann machine learning generative model multimodal data show model extract meaningful representation multimodal data learned representation useful classification information tasks hence notion semantic similarity model probability density space multimodal inputs sampling conditional distributions data possible representation data modalities missing experimental results data consisting images show multimodal dbm learn good generative model joint space image inputs useful information retrieval unimodal multimodal queries demonstrate model significantly outperform svms lda discriminative tasks finally compare model deep learning methods including autoencoders deep belief networks show achieves significant gains 
learning target prior conventional approaches supervised parametric learning relations data target variables provided training sets consisting pairs data target variables work describe learning scheme parametric learning target variables modeled prior model relations data target variables estimated set data training term method learning target priors specifically learning seeks parameter maximizes log likelihood training set compared conventional semi supervised learning approach efficient prior knowledge target variables form probabilistic distributions reduces training data learning compared bayesian approach learned parametric regressor efficiently implemented deployed tasks running efficiency critical line bci signal decoding demonstrate effectiveness proposed approach parametric regression tasks bci signal decoding pose estimation video 
slice sampling normalized kernel weighted completely random measure mixture models number dependent nonparametric processes proposed model non stationary data unknown latent dimensionality however inference algorithms slow general highly specific given model formulation paper describe wide class nonparametric processes including several existing models present slice sampler allows efficient inference class models 
scalable inference overlapping communities develop scalable algorithm posterior inference overlapping communities large networks algorithm based stochastic variational inference mixed membership stochastic naturally network estimating its community structure apply algorithm large real world networks nodes converges several orders magnitude faster state art algorithm finds hundreds communities large real world networks true communities benchmark networks equal better accuracy compared scalable algorithms 
online dictionary learning application novel document detection given pervasive social twitter become leading source breaking key task identification detection novel documents stream documents scalable manner motivated challenge introduce problem online dictionary learning unlike traditional dictionary learning uses squared loss penalty used measuring reconstruction error present efficient online algorithm problem based alternating directions method multipliers establish sublinear regret bound algorithm empirical results stream twitter data shows online dictionary learning algorithm novel document detection gives order magnitude speedup previously known batch algorithm any significant loss quality results algorithm online dictionary learning independent interest 
systematic approach extracting semantic information functional mri data paper introduces novel classification method functional magnetic resonance imaging datasets tens classes method designed predictions using information many brain locations possible feature selection does decomposing pattern brain activation differently informative sub regions provide results complex semantic processing dataset show method competitive state art feature selection suggest method may used perform group exploratory analyses complex class structure 
learning optimal spike based representations neural networks learn represent address question assuming neural networks seek generate optimal population representation fixed linear define loss function quality population derive dynamical equations neurons synapses minimize loss dynamical equations yield network integrate fire neurons hebbian plasticity show learning initially regular highly correlated spike trains towards poisson distributed independent spike trains lower firing rates learning rule drives network asynchronous balanced regime inputs network represented optimally given show network dynamics synaptic plasticity jointly received unit possible doing minimize prediction error inputs outputs turn spikes only whenever prediction error exceeds certain value thereby implementing predictive coding scheme work suggests several features reported cortical networks high trial trial variability spike timing dependent plasticity simply efficient spike based code 
collaborative ranking parameters primary application filtering small set items user entails ranking approaches however formulate problem rating prediction ranking perspective work present method collaborative ranking leverages main approaches neighborhood model based novel method highly efficient only parameters optimize single hyperparameter tune state art collaborative ranking methods show parameters learned dataset yield excellent results very different dataset any 
rational inference relative preferences statistical decision theory assumes relative different humans described assigning option specific scalar utility functions however assumption observed human behavior including studies preferences shown change systematically simply variation set choice presented paper show interpreting relative comparison available any particular decision instance results rational theory value inference explains heretofore intractable rational choice behavior human subjects characterize conditions rational agent selecting optimal dynamic value inference framework preferences encoded using static ordinal utility function 
topographic unsupervised learning natural sounds auditory cortex computational modelling primary auditory cortex primary visual cortex due properties greater disorder recently demonstrated considered disorder given however adopt efficient coding strategy disorder natural sound statistics provide computational model disorder used model originally proposed smooth map contrast natural images natural sounds exhibit distant correlations learned map auditory model predicted harmonic relationships cells furthermore mechanism used model complex cells nonlinear responses similar results understanding sensory different modalities novel integrated manner 
approximating sequential incomplete information multi unit demand many large markets sequential domains include online spectrum domains face highly complex decision making problems preferences outcomes depend outcomes limited information factors drive outcomes preferences past actions work formulate problem price prediction learning optimization define concept stable price predictions show approximate equilibrium sequential characterized profile strategies approximately optimize respect approximately stable price predictions show formulation compare known theoretical simpler domains approximate complex domain analytical solutions heretofore unknown 
conquer method sparse inverse covariance estimation paper consider regularized sparse inverse covariance matrix estimation problem very large number variables face high dimensionality limited number samples work shown estimator strong statistical guarantees recovering true structure sparse inverse covariance matrix underlying graph structure corresponding gaussian markov random field proposed algorithm problem smaller sub problems uses solutions sub problems build good approximation original problem derive bound distance approximate solution true solution based bound propose clustering algorithm attempts minimize bound practice able effective partitions variables approximate solution solution resulting solving sub problems initial point solve original problem achieve faster computational procedure example state art method quic requires solve problem nodes arises climate application proposed algorithm conquer quic dc quic only requires solve problem 
simple practical algorithm differentially private data present algorithm differentially private data based simple combination exponential mechanism multiplicative weights update rule algorithm achieves best known nearly optimal theoretical guarantees time being simple implement experimentally accurate data sets existing techniques 
bayesian active learning priors fast receptive field active learning substantially improve yield experiments adaptively selecting stimuli neuron receptive field real time bayesian active learning methods maintain posterior distribution select stimuli maximally reduce posterior entropy time step however existing methods tend rely simple gaussian priors exploit uncertainty level hyperparameters determining optimal stimulus uncertainty play substantial role particularly smooth sparse local space time paper describe novel framework active learning hierarchical conditionally gaussian priors algorithm uses sequential markov chain monte carlo sampling particle filtering mcmc hyperparameters construct mixture gaussians representation posterior selects optimal stimuli using approximate infomax criterion core elements algorithm making computationally efficient real time experiments apply algorithm simulated real neural data show provide highly accurate receptive field estimates very limited data small number hyperparameter samples 
weighted likelihood policy search model selection reinforcement learning rl methods based direct policy search dps actively discussed achieve efficient approach complicated markov decision processes mdps progress practical applications rl remains problem dps related model selection policy paper propose novel dps method weighted likelihood policy search policy efficiently learned weighted likelihood estimation naturally dps statistical inference problem various sophisticated techniques statistics applied dps problems directly hence following idea information criterion develop measurement model comparison dps based weighted log likelihood 
learning dependency structure latent factors paper study latent factor models dependency structure latent space propose general learning framework induces sparsity undirected graphical model vector latent factors novel latent factor model proposed matrix factorization problem special regularization term encourages collaborative reconstruction main benefit model simultaneously learn lower dimensional representation data model pairwise relationships latent factors explicitly line learning algorithm model feasible large scale learning problems experimental results synthetic data real world data sets demonstrate pairwise relationships latent factors learned model provide structured way exploring high dimensional data learned representations achieve state art classification performance 
provable unknown gaussian noise implications gaussian mixtures autoencoders present algorithm independent component analysis provable performance guarantees particular given samples form unknown n$ matrix chosen uniformly random n$ dimensional gaussian random variable unknown covariance give algorithm provable recovers additive running time sample complexity polynomial accomplish introduce novel step may useful contexts covariance gaussian noise known advance give general framework finding local optima function given oracle approximately finding just crucial step algorithm previous attempts allows control error columns local search 
globally convergent dual map lp relaxation solvers using margins finding exact solution map inference problem intractable many real world tasks map lp relaxations shown very effective practice however efficient methods perform block coordinate descent sub optimal points globally convergent work propose augment algorithms descent approach present method efficiently optimize descent direction subdifferential using margin based extension theorem furthermore presented approach provides methodology construct primal optimal solution its dual optimal counterpart demonstrate efficiency presented approach models protein interactions problems show approach outperforms state art solvers 
means incremental clustering method estimating number clusters learning number clusters key problem data clustering present means novel robust incremental method learn number data clusters may used any iterative clustering algorithm means family contrast many popular methods assumptions underlying cluster distributions means only assumes fundamental cluster property cluster admit unimodal distribution proposed algorithm considers cluster member applies univariate statistic hypothesis distribution distances cluster members important advantages applied univariate distance vectors ii directly applied kernel based methods since only pairwise distances involved computations experimental results artificial real datasets indicate effectiveness method its superiority analogous approaches 
regret algorithms unconstrained online convex optimization applications online convex optimization including online prediction classification unconstrained natural feasible set existing algorithms fail achieve sub linear regret setting constraints point known advance present algorithm prior knowledge offers near optimal regret bounds respect any choice particular regret respect constant prove lower bounds showing algorithm guarantees optimal setting constant factors 
bayesian models large scale hierarchical classification challenging problem hierarchical classification leverage hierarchical relations classes improving classification performance greater challenge manner computationally feasible large scale problems usually encountered practice paper proposes set bayesian methods model hierarchical dependencies class labels using logistic regression specifically child relationships modeled placing hierarchical prior children nodes centered parents thereby encouraging classes nearby hierarchy share similar model parameters present efficient variational algorithms tractable posterior inference models provide parallel handle large scale problems hundreds thousands dimensions tens thousands classes run evaluation multiple large scale benchmark datasets scalability approach shows significant performance advantage state art hierarchical methods 
recovery sparse probability measures convex programming consider problem cardinality penalized optimization convex function probability simplex additional convex constraints known classical regularizer fails promote sparsity probability simplex since norm probability simplex constant propose direct relaxation minimum cardinality problem show efficiently solved using convex programming application consider recovering sparse probability measure given moment constraints formulation becomes linear programming hence solved very efficiently sufficient condition exact recovery minimum cardinality solution derived arbitrary affine constraints develop penalized version noisy setting solved using second order cone programs proposed method outperforms known heuristics based norm second application consider convex clustering using sparse gaussian mixture compare results known soft means algorithm 
multiple operator valued kernel learning positive definite operator valued kernels generalize known notion reproducing kernels naturally multi output learning situations paper addresses problem learning finite linear combination infinite dimensional operator valued kernels suitable functional data analysis methods nonlinear contexts study problem case kernel regression functional responses norm constraint combination coefficients resulting optimization problem involved multiple scalar valued kernel learning since operator valued kernels pose technical theoretical issues propose multiple operator valued kernel learning algorithm based solving system linear operator equations using block coordinate descent procedure experimentally validate approach functional regression task context movement prediction brain computer interfaces 
approximate message passing consistent parameter estimation applications sparse learning consider estimation vector n$ measurements obtained general cascade model consisting known linear transform followed probabilistic possibly nonlinear measurement present method called adaptive generalized approximate message passing adaptive enables joint learning statistics prior measurement estimation unknown vector proposed algorithm generalization recently developed method uses expectation maximization em iterations posteriors steps computed approximate message passing techniques applied large class learning problems including learning sparse priors compressed sensing identification linear nonlinear cascade models dynamical systems neural spiking processes prove large gaussian transform matrices asymptotic behavior adaptive algorithm predicted simple set scalar state evolution equations analysis shows adaptive method yield asymptotically consistent parameter estimates implies algorithm achieves reconstruction quality equivalent oracle algorithm correct parameter values adaptive methodology provides systematic general computationally efficient method applicable large range complex linear nonlinear models provable guarantees 
better way deep boltzmann machines describe pre training algorithm deep boltzmann machines related pre training algorithm deep belief networks show certain conditions pre training procedure improves variational lower bound hidden layer dbm based analysis develop different method pre training distributes modelling work evenly hidden layers results mnist datasets demonstrate pre training algorithm allows learn better generative models 
towards learning theoretic analysis spike timing dependent plasticity paper suggests learning theoretic perspective synaptic plasticity benefits global brain introduce model arises fast time constant limit leaky integrate fire neurons spiking timing dependent plasticity stdp ii amenable theoretical analysis show encodes reward estimates spikes error bound spikes controlled spiking margin sum synaptic weights moreover efficacy spikes usefulness reward maximizing depends total synaptic strength finally based analysis propose regularized version stdp show regularization improves robustness neuronal learning faced multiple stimuli 
learning manifolds means study problem estimating manifold random samples particular consider piecewise constant piecewise linear estimators induced means analyze performance extend previous results means separate directions provide results means reconstruction manifolds secondly prove reconstruction bounds higher order approximation known results previously available results means novel technical tools established literature case results mathematical tools 
iterative ranking pair wise comparisons question aggregating pairwise comparisons obtain global ranking collection objects interest very long time ranking online system aggregating social product based settings addition obtaining ranking finding object rating interest understanding intensity preferences paper propose novel iterative rank aggregation algorithm discovering scores objects pairwise comparisons algorithm natural random walk interpretation graph objects edges present objects compared scores turn stationary probability random walk algorithm model independent establish efficacy method however consider popular luce model object associated score determines probabilistic outcomes pairwise comparisons objects bound finite sample error rates scores assumed model estimated algorithm leads order optimal dependence number samples required learn scores algorithm indeed experimental evaluation shows model independent algorithm performs maximum likelihood estimator model outperforms recently proposed algorithm 
polynomial time form robust regression despite variety robust regression methods developed current regression formulations hard allow unbounded response single leverage point present general formulation robust regression variational estimation unifies number robust regression methods allowing tractable approximation strategy develop estimator requires only polynomial time achieving certain robustness consistency guarantees experimental evaluation demonstrates effectiveness estimation approach compared standard methods 
learning probability measures respect optimal metrics study problem estimating sense optimal metrics measure assumed supported manifold embedded hilbert space precise connection optimal metrics optimal quantization learning theory derive probabilistic bounds performance classic algorithm unsupervised learning means used produce probability measure derived data analysis arrive lower bounds probabilistic bounds convergence rate empirical law large numbers unlike existing bounds applicable wide class measures 
label ranking partial based probabilistic models several machine learning methods allow uncertain predictions being common settings conventional classification studied learning rank address label ranking setting allowing learner certain pairs labels being predict partial total orders method predictions produced thresholding probabilities pairwise preferences labels induced predicted probability distribution set formally analyze approach plackett luce model showing produces proper partial orders predictions characterizing induced class partial orders theoretical results complemented experiments demonstrating practical usefulness approach 
adaptive learning smoothing functions application paper proposes efficient online learning algorithm track smoothing functions additive models key idea combine linear representation additive models recursive squares filter order quickly track changes model put weight data filter uses factor exponentially weights observations order tracking behaviour using adaptive factor updated based gradient priori errors using results theory upper bounds learning rate analyzed proposed algorithm applied years data provided utility compared state art methods achieves superior performance terms model tracking prediction accuracy 
tensor decomposition fast parsing latent variable describe approach speed inference latent variable shown highly effective natural language parsing approach based tensor formulation recently introduced spectral estimation latent variable coupled tensor decomposition algorithm known multilinear algebra literature describe error bound approximation bounds difference probabilities calculated algorithm true probabilities approximated model gives empirical evaluation real world natural language parsing data demonstrates significant speed minimal cost parsing performance 
semi supervised locally biased learning many applications information labels provided semi supervised manner specific target region large data set perform machine learning data analysis tasks nearby pre specified target region locally biased problems particularly challenging popular eigenvector based machine learning data analysis tools root reason inherently global quantities paper address issue providing methodology construct semi supervised graph laplacian illustrate locally biased used perform locally biased machine learning semi supervised capture successively directions maximum variance conditioned being correlated input set nodes assumed provided semi supervised manner provide several empirical examples demonstrating semi supervised used perform locally biased learning 
exponential concentration mutual information estimation application forests prove exponential concentration inequality plug estimator shannon mutual information previous results mutual information estimation only bounded expected error advantage having exponential inequality combined union bound guarantee accurate estimators mutual information many pairs random variables simultaneously application show result optimally estimate density function graph distribution markov forest graph 
augment conquer negative binomial processes developing data augmentation methods unique negative binomial distribution seemingly disjoint count mixture models process framework develop fundamental properties models derive efficient gibbs sampling inference show gamma process reduced hierarchical dirichlet process normalization its unique theoretical structural computational advantages variety processes distinct sharing mechanisms constructed applied topic modeling connections existing algorithms showing importance inferring probability parameters 
transferring model based reinforcement learning study automatically select adapt multiple representations world support model based reinforcement learning address challenges transfer learning heterogeneous environments varying tasks present efficient online framework sequence tasks learns set relevant representations used future tasks pre defined mapping strategies introduce general approach support transfer learning different state spaces demonstrate potential system improved faster convergence near optimum policy benchmark domains 
minimization continuous bethe approximations positive variation develop convergent minimization algorithms bethe variational approximations explicitly constrain marginal estimates families valid distributions existing message passing algorithms define fixed point iterations corresponding stationary points bethe energy greedy dynamics distinguish local minima fail converge continuous estimation problems linked creation marginal estimates gaussians negative variance approach leverages methods understood convergence properties uses bound projection methods ensure marginal approximations valid iterations derive general algorithms discrete gaussian pairwise markov random fields showing improvements standard belief propagation apply method hybrid model discrete continuous variables showing improvements expectation propagation 
non linear metric learning paper introduce novel metric learning algorithms explicitly designed non linear easy approaches achieve goal fundamentally different ways inherits computational benefits linear mapping linear metric learning uses non linear distance explicitly capture similarities data sets applies gradient boosting learn non linear mappings directly function space takes advantage approach robustness speed towards single additional hyper parameter various benchmark data sets demonstrate methods only match current state art terms knn classification error case obtain best results learning settings 
lda sparse multi dimensional models multi dimensional latent variable models capture many latent factors corpus topic perspective sentiment introduce lda multi dimensional latent variable model document different factors word depends dimensional vector latent variables model incorporates structured word priors learns sparse product factors experiments research show model learn latent factors research topic scientific focus methods vs applications modeling improvements reduce improve human discovered factors 
sampling particle gibbs present novel method family particle mcmc methods refer particle gibbs sampling similarly existing backward simulation procedure backward sampling considerably improve mixing kernel using separate forward backward sweeps however achieve effect single forward apply framework challenging class non markovian state space models develop truncation strategy models applicable principle any backward simulation based method particularly suited framework particular show simulation study yield order magnitude improved accuracy relative due its robustness truncation error several application examples discussed including rao particle smoothing inference state space models 
modelling relationships processes present bayesian nonparametric model discovers implicit social structure interaction time series data social groups implicitly actions members groups many models social networks explicitly relationships infer social structure consider particular class processes stochastic point process able model groups individuals extend infinite relational model using processes its edges making events associated edges dependent time model outperforms general processes structured poisson process based models predicting email turn taking 
expectation propagation gaussian process dynamical systems rich complex time series data generated markets videos neural recordings common feature modern data analysis explaining phenomena underlying diverse data sets requires flexible accurate models paper promote gaussian process dynamical systems rich model class appropriate analysis particular present message passing algorithm approximate inference based expectation propagation inference general passing problem forward backward smoothing obtain accurate posterior distributions latent structures resulting improved pre performance compared state art cases general iterative message passing algorithm hence provide unifying approach message passing 
quasi newton proximal splitting method describe efficient implementations useful class functions implementations exploit piece wise linear nature dual problem second part paper applies previous result convex minimization problems leads elegant quasi newton method optimization method favorably state art alternatives algorithm extensive applications including signal processing sparse regression recovery machine learning classification 
exact stable recovery sequences signals sparse differential minimization consider problem recovering sequence vectors k$ k$ sparse k$ typically smaller based linear measurements k$ k$ k$ measurement matrix noise respectively assuming k$ restricted isometry property rip certain order depending only k$ show absence noise convex program minimizes weighted sum norm successive differences subject linear measurement constraints recovers sequence k$ exactly interesting result because convex program equivalent standard compressive sensing problem highly structured aggregate measurement matrix does satisfy rip requirements standard sense achieve exact recovery presence bounded noise propose constrained convex program recovery derive bounds reconstruction error sequence theoretical analysis simulations application real video data support validity proposed approach acquisition recovery signals time varying sparsity 
efficient reinforcement learning high dimensional linear quadratic systems study problem adaptive control high dimensional linear quadratic system previous work established asymptotic convergence optimal various adaptive control schemes recently asymptotic regret bound shown p$ dimension state space work consider case matrices describing dynamic system sparse dimensions large present adaptive control scheme achieves regret bound particular algorithm average cost times optimum cost comparison previous work dense dynamics algorithm needs samples before estimate unknown dynamic any significant accuracy result prominent applications emerging area computational particular online social networks 
multilabel classification using bayesian compressed sensing paper present bayesian framework multilabel classification using compressed sensing key idea compressed sensing multilabel classification project label vector lower dimensional space using random transformation learn regression functions projections approach considers components single probabilistic model thereby jointly optimizing compression learning tasks derive efficient variational inference scheme provides joint posterior distribution unobserved labels key benefits model naturally handle datasets missing labels measure uncertainty prediction uncertainty estimate provided model naturally allows active learning paradigms oracle provides information labels maximally informative prediction task experiments show significant boost prior methods terms prediction performance benchmark datasets fully labeled missing labels case finally highlight various useful active learning scenarios enabled probabilistic model 
scaling inference constrained continuous markov random fields consensus optimization probabilistic graphical models powerful tools analyzing constrained continuous domains however finding models computationally expensive paper improve scalability inference class graphical models piecewise linear piecewise quadratic dependencies linear constraints continuous domains derive algorithms based consensus optimization framework demonstrate superior performance state art show empirically large scale preference modeling problem algorithms scale linearly number dependencies constraints 
stochastic gradient method exponential convergence rate finite training sets propose stochastic gradient method optimizing sum finite set smooth functions sum strongly convex standard stochastic gradient converge sublinear rates problem proposed method incorporates memory previous gradient values order achieve linear convergence machine learning context numerical experiments indicate algorithm dramatically outperform algorithms terms optimizing training error reducing error quickly 
query complexity derivative optimization derivative optimization attractive objective function derivatives available evaluations costly moreover function evaluations noisy approximating gradients finite differences difficult paper gives quantitative lower bounds performance noisy function evaluations fundamental gap optimization performance based noisy evaluations versus noisy gradients challenges conventional wisdom method finite differences comparable stochastic gradient however situations situations propose algorithm proved near optimal class strongly convex objective functions feature algorithm only uses boolean valued function comparisons evaluations makes algorithm useful range applications including optimization based comparisons human subjects example remarkably show based noisy function evaluations boolean valued function comparisons convergence rate 
object selective features unsupervised feature learning work unsupervised feature learning focused goal discovering high level features unlabeled images progress direction cases standard large amount labeled data order construct detectors sensitive object classes complex patterns data paper aim hypothesis unsupervised feature learning methods provided only unlabeled data learn high level invariant features sensitive commonly objects prior results suggest possible object class large data many labeled datasets unclear similar accomplished dealing completely unlabeled data major however scale succeed small datasets small numbers learned features propose large scale feature learning system enables carry experiment learning features tens unlabeled images based scalable clustering algorithms means clustering simple system discover features sensitive commonly object class human faces combine detectors invariant significant global large scale 
burn bias bayesian inference provides unifying framework addressing problems machine learning artificial problems human mind unfortunately exact bayesian inference intractable simplest models therefore machines approximate bayesian inference approximate inference algorithms achieve wide range time accuracy tradeoffs optimal investigate time accuracy tradeoffs using metropolis hastings algorithm mind inference algorithm accurate decisions possible long before markov chain posterior distribution during known burn therefore strategy optimal subject mind bounded processing speed costs may perform iterations resulting samples biased towards initial value resulting cognitive process model provides rational basis adjustment heuristic model quantitative predictions tested published data numerical estimation tasks theoretical empirical results suggest bias consistent approximate bayesian inference 
nonparametric online variational inference hierarchical dirichlet processes variational methods provide computationally scalable alternative monte carlo methods large scale bayesian nonparametric learning practice however conventional batch online variational methods quickly become local optima paper consider nonparametric topic model based hierarchical dirichlet process hdp develop novel online variational inference algorithm based split merge topic updates derive simpler faster variational approximation hdp show intelligently splitting merging components variational posterior achieve substantially better predictions data conventional online batch variational algorithms analysis large datasets batch analysis infeasible show split merge updates better capture nonparametric properties underlying model allowing learning topics 
neural autoregressive topic model describe model learning meaningful representations documents unlabeled collection documents model inspired recently proposed softmax undirected graphical model word counts shown learn better generative model meaningful document representations specifically inspiration conditional mean field recursive equations softmax order define neural network architecture estimates probability word given document given previously observed words paradigm allows expensive softmax distribution words hierarchical distribution paths binary tree words result model training complexity scales size linearly softmax experiments show model competitive generative model documents document representation learning algorithm 
unifying perspective parametric policy search methods markov decision processes parametric policy search algorithms methods choice markov decision processes expectation maximisation natural gradient being considered current state art field article provide unifying perspective algorithms showing step directions parameter space closely related search direction approximate newton method analysis leads naturally consideration approximate newton method alternative gradient based method markov decision processes able show algorithm numerous desirable properties naive application newton method viable alternative expectation maximisation natural gradient empirical results suggest algorithm excellent convergence robustness properties performing strongly comparison expectation maximisation natural gradient 
monte carlo propose novel method scalable parallelization algorithms monte carlo simulation avoids transmission particles nodes particle particular show reduce communication particle weights machine efficiently maintaining implicit global coherence parallel simulation explain methods efficiently maintain particles any particle reconstructed demonstrate using examples bayesian computational gain parallelization using significantly cost particle reconstruction timing experiments show reconstruction particles indeed efficient compared transmission particles 
near optimal map inference determinantal point processes determinantal point processes recently proposed computationally efficient probabilistic models diverse sets variety applications including document image search pose estimation many dpp inference operations including normalization sampling tractable however finding likely configuration map required practice decoding hard approximate inference because dpp probabilities log submodular greedy algorithms used past empirical success however methods only give approximation guarantees special case monotone kernels paper propose algorithm approximating map problem based continuous techniques submodular function maximization method involves novel continuous relaxation log probability function contrast multilinear extension used general submodular functions evaluated exactly efficiently obtain practical algorithm approximation guarantee general class non monotone algorithm extends map inference complex constraints making possible combine markov random fields weighted matchings models demonstrate approach outperforms greedy methods synthetic real world data 
probabilistic low rank subspace clustering paper consider problem clustering data points low dimensional subspaces presence outliers pose problem using density estimation formulation associated generative model based probability model develop iterative expectation maximization em algorithm derive its global solution addition develop bayesian methods based variational bayesian vb approximation capable automatic dimensionality selection method based alternating optimization scheme second method makes results vb matrix factorization leading fast effective estimation methods extended handle sparse outliers robustness handle missing values experimental results suggest proposed methods very effective clustering identifying outliers 
issue models behavior develop probabilistic model data uses uncover positions specific issues model used explore voting patterns expected deviation depends being derive approximate posterior inference algorithms based variational methods years data demonstrate improvement predictive performance model utility interpreting inherently multi dimensional space 
density propagation improved bounds partition function given probabilistic graphical model its density states function any likelihood value gives number probability introduce novel message passing algorithm called density propagation dp estimating function show dp exact tree structured graphical models general strict generalization sum product max product algorithms density states tree decomposition introduce family upper lower bounds partition function any tree upper bound based grained density state information provably tight previously known bounds based log partition function strictly stronger general condition holds conclude empirical evidence improvement convex relaxations mean field based bounds 
learning sat boolean satisfiability sat canonical complete decision problem important problems computer practice real world sat sentences drawn distribution may result efficient algorithms solution sat instances likely shared characteristics work approaches exploration family sat solvers learning problem particular relate polynomial time sat subset notion margin sentences mapped feature function hilbert space provided mapping based polynomial time computable statistics show margin data points implies polynomial time solver sat subset based algorithm furthermore show simple style learning rule optimal sat solver bounded number training updates derive linear time computable set features show analytically margins exist important polynomial special cases sat empirical results show order magnitude improvement state art sat solver hardware verification task 
learning networks heterogeneous influence information disease influence networks entities natural systems human analyzing transmission networks plays important role understanding diffusion processes predicting events future however underlying transmission networks hidden incomplete observe only time cascades events paper attempt address challenging problem hidden network only cascades structure discovery problem complicated fact influence different entities network heterogeneous described simple parametric model therefore propose kernel based method capture diverse range different types influence any prior assumption synthetic real cascade data show model better recover underlying diffusion network drastically improve estimation influence functions entities 
multiclass learning simplex coding paper novel framework multiclass learning defined suitable coding decoding strategy namely simplex coding allows generalize multiple classes relaxation approach commonly used binary classification framework relaxation error analysis developed avoiding constraints considered hypotheses class moreover show setting possible derive provably consistent regularized methods training tuning complexity independent number classes tools convex analysis introduced used scope paper 
hash clustering exponential families clustering key component data analysis despite its importance scalable algorithms rich statistical models simpler means clustering paper present sampler capable estimating mixtures exponential families its heart lies novel proposal distribution using random projections achieve high throughput generating proposals crucial clustering models large numbers clusters 
topic embeddings introduce joint model network content context designed exploratory analysis email networks visualization topic specific communication patterns model model network attributes uses multinomial distributions words mixture components explaining latent euclidean positions mixture components explaining network attributes validate model achieving state art performance link prediction task achieving semantic coherence equivalent latent dirichlet allocation demonstrate capability model descriptive exploratory analysis inferred topic specific communication patterns email dataset email corpus 
learning label trees probabilistic modelling implicit feedback user preferences items inferred explicit feedback item ratings implicit feedback research collaborative filtering concentrated explicit feedback resulting development accurate scalable models however since explicit feedback difficult important develop effective models advantage widely available implicit feedback introduce probabilistic approach collaborative filtering implicit feedback based modelling user item selection process scalability attention tree structured distributions items develop principled efficient algorithm learning item trees data identify problem widely used evaluating implicit feedback models propose way addressing using small quantity explicit feedback data 
learning recursive perceptual representations linear support vector machines svms become very popular vision part state art object recognition classification tasks require high dimensional feature spaces good performance deep learning methods compact representations current methods employ perceptrons require solving difficult non convex optimization problem propose deep non linear classifier layers svms incorporates random projection its core element method learns layers linear svms recursively transforming original data manifold random projection weak prediction computed layer method scales linear svms does rely any kernel computations nonconvex optimization exhibits better generalization ability kernel based svms especially true number training samples smaller dimensionality data common scenario many real world applications random projections key method show experiments observe consistent improvement previous complicated methods several vision speech benchmarks 
link prediction graphs autoregressive features paper consider problem link prediction time evolving graphs assume certain graph features node degree follow vector autoregressive model propose information improve accuracy prediction strategy involves joint optimization procedure space matrices matrices takes account sparsity low rank properties matrices oracle inequalities derived illustrate trade choice smoothing parameters modeling joint effect sparsity low rank property estimate computed efficiently using proximal methods generalized forward backward 
deep neural networks segment neuronal images address central problem namely automatic segmentation neuronal structures stacks em images necessary efficiently map brain structure connectivity segment biological neuron special type deep artificial neural network pixel classifier label pixel non predicted raw pixel values square window centered input layer maps window pixel neuron followed convolutional max pooling layers preserve information extract features increasing levels abstraction output layer produces calibrated probability class classifier trained gradient descent stack known ground truth tested stack size ground truth unknown authors em segmentation challenge problem specific post processing approach outperforms competing techniques large margin considered metrics error error pixel error pixel error approach only outperforming second human 
scalable genetic data discrete process present bayesian nonparametric model genetic sequence data set genetic sequences modelled using markov model partitions partitions locations genome related clusters splitting merging model thought discrete time continuous time processes important properties being scalable apply model problem showing improved computational efficiency maintaining accuracies 
gradient weights nonparametric regressors regression problems d$ unknown function varies coordinates show weighting coordinate estimated norm derivative efficient way significantly improve performance distance based regressors kernel nn regressors propose simple estimator derivative norms prove its consistency moreover proposed estimator efficiently learned online 
sparse approximate manifolds differential geometric mcmc challenges markov chain monte carlo methodology development proposal mechanisms moves distant current point accepted high probability low computational cost introduction locally adaptive mcmc methods based natural underlying riemannian geometry models goes way problems certain classes models metric tensor analytically tractable however computational efficiency due potentially high dimensional matrix operations iteration paper firstly investigate sampling based approach approximating metric tensor suggest valid mcmc algorithm extends applicability riemannian manifold mcmc methods statistical models admit analytically computable metric tensor secondly show approximation scheme consider naturally motivates regularisation improve estimates obtain sparse approximate inverse metric enables stable sparse approximations local geometry demonstrate application algorithm inferring parameters realistic system ordinary differential equations using biologically motivated robust error model expected fisher information analytically intractable 
fast variational inference conjugate exponential family present general method deriving collapsed variational inference algorithms probabilistic models conjugate exponential family method unifies many existing approaches collapsed variational inference collapsed variational inference leads lower bound marginal likelihood exploit information geometry bound derive faster optimization methods based conjugate gradients models approach very general easily applied any model mean field update equations derived empirically show significant speed probabilistic models optimized using bound 
bayesian analysis using measure factorization family trees directed graphs used identify sites genome correlated presence absence disease technologies amount data available number individuals number sites number thousands individuals meanwhile analysis methods limited individuals limits analyses many small independent disease models used analysis log estimator similarly limited because originally designed different task mind ordering sites genome before technologies order difficult interpret nontrivial extend consider interactions sites developments difficulties call creation modern methods analysis drawing advances graphical model inference theory introduce simple powerful formalism genetic disease models show disease models accurate efficient estimators technique constructing variational approximation potential applications inference large scale graphical models method allows inference larger previously analyzed literature improves disease prediction 
accelerated training matrix norm regularization boosting approach sparse learning models typically combine smooth loss nonsmooth penalty trace norm developments sparse approximation promising solution methods current approaches apply only matrix norm constrained problems provide suboptimal convergence rates paper propose boosting method regularized learning guarantees accuracy iterations performance accelerated boosting fixed rank local optimization exploiting simpler local objective previous work proposed method yields state art performance large scale problems demonstrate application latent learning provide efficient weak oracle 
controlled recognition bounds visual learning exploration describe tradeoff performance visual recognition problem control agent sensing process focus problem object otherwise known static scene propose measure control relate expected risk its conditional entropy posterior density show analytically empirically simulation using simplest known model captures image formation including scaling show agent given training set provide guarantees performance priors agent capable infinite control achieve arbitrarily good performance asymptotically 
distributed probabilistic learning camera networks missing data probabilistic approaches computer vision typically assume centralized setting algorithm access observed data points however many problems wide area benefit distributed modeling because physical computational constraints distributed models date algebraic approaches distributed svd result explicitly deal missing data work present approach estimation learning generative probabilistic models distributed context certain sensor data missing particular show traditional centralized models probabilistic pca missing data learned data distributed network demonstrate utility approach problem distributed affine structure motion experiments suggest accuracy learned probabilistic structure motion models rivals traditional centralized factorization methods being able handle challenging situations missing noisy observations 
minimizing uncertainty paper consider problem large human labeling represent execution using directed graph nodes node represents data item produced operator assume operator confidence its output data want reduce uncertainty output queries human expert query consists checking given data item correct paper consider problem optimal set queries minimize resulting output uncertainty perform detailed evaluation complexity problem various classes graphs give efficient algorithms problem trees show general dag problem intractable 
practical bayesian optimization machine learning algorithms machine learning algorithms frequently involves careful tuning learning parameters model hyperparameters unfortunately tuning requiring expert rules sometimes force search therefore automatic approaches optimize performance any given learning algorithm problem hand work consider problem framework bayesian optimization learning generalization performance modeled sample gaussian process show certain choices nature type kernel treatment its hyperparameters play crucial role obtaining good achieve expert level performance describe algorithms account variable cost learning algorithm experiments leverage presence multiple cores parallel show proposed algorithms improve previous automatic procedures reach human expert level optimization many algorithms including latent dirichlet allocation structured svms convolutional neural networks 
graphs low rank positive semidefinite graph learning approach many graph based machine learning data mining approaches quality graph critical however real world applications especially semi supervised learning unsupervised learning evaluation quality graph expensive sometimes impossible due cost ground truth paper proposed robust approach convex optimization graph input graph learn graph higher quality major concern ideal graph satisfy following constraints non negative symmetric low rank positive semidefinite develop graph learning algorithm solving convex optimization problem develop efficient optimization obtain global optimal solutions theoretical guarantees only non sensitive parameter method shown experimental results robust achieve higher accuracy semi supervised learning clustering various settings preprocessing graphs method wide range potential applications machine learning data mining 
time marginalized prior hierarchical clustering introduce prior nonparametric bayesian hierarchical clustering prior constructed time information providing prior tree structures call time marginalized allows models tree structure times providing benefits flexible priors may constructed efficient gibbs type inference used demonstrate example model density estimation show achieves competitive experimental results 
diffusion robust visual tracking weighted graph used underlying structure many algorithms semi supervised learning spectral clustering edge weights usually single similarity measure hard impossible capture relevant aspects similarity using single similarity measure case visual object matching beneficial integrate different similarity measures focus different visual representations paper novel approach integrate multiple similarity measures pro pairs similarity measures combined diffusion process tensor product graph hence similarity pair becomes function joint diffusion original similarities turn depends neighborhood structure call process diffusion however higher order graph usually means significant increase time complexity case proposed approach key feature approach time complexity diffusion process original graphs moreover necessary explicitly construct frame work finally pairs similarity measures combined weighted sum demonstrate advantages proposed approach task visual tracking different aspects appearance similarity target object frame target object frame integrated obtained method tested several challenge video sequences experimental results show outperforms state art tracking methods 
nonparametric variable clustering model factor analysis models effectively covariance structure high dimensional data solutions typically hard interpret motivates disjoint partition clustering observed variables variables cluster highly correlated introduce bayesian non parametric approach problem demonstrate advantages heuristic methods proposed date 
priors diversity generative latent variable models probabilistic latent variable models machine learning offer convenient coherent way specify prior distributions unobserved structure data unknown properties inferred posterior inference models useful exploratory analysis visualization building density models data providing features used discriminative tasks significant limitation models however prior highly redundant due assumptions internal parameters example preference prior mixture model components non overlapping topic model ensure words only appear small number topics work independence assumptions probabilistic latent variable models underlying prior determinantal point process dpp dpp allows specify preference diversity latent variables using positive definite kernel function using kernel probability distributions able define dpp probability measures show perform map inference dpp priors latent dirichlet allocation mixture models leading better intuition latent variable representation quantitatively improved unsupervised feature extraction generative aspects model 
nonparametric conjugate prior distribution maximizing noisy function propose novel bayesian approach solve stochastic optimization problems involve ﬁnding noisy nonlinear functions previous work focused representing possible functions explicitly leads step procedure doing inference function space second ﬁnding functions representation step directly model distribution devise non parametric conjugate prior natural parameter corresponds given kernel function statistic composed observed function values resulting posterior distribution directly captures uncertainty maximum unknown function 
convergence rate analysis map coordinate minimization algorithms finding maximum map assignments graphical models important task many applications since problem generally hard linear programming lp relaxations used solving relaxations efficiently important practical problem years several authors proposed message passing updates corresponding coordinate descent dual lp however generally guaranteed converge global optimum approach smooth lp perform coordinate descent smoothed dual however little known convergence rate procedure perform thorough rate analysis schemes derive primal dual convergence rates provide simple dual primal mapping yields feasible primal solutions guaranteed rate convergence empirical evaluation supports theoretical claims shows method highly competitive state art approaches yield global optima 
projection retrieval classification many applications classification systems require loop human intervention cases decision process simultaneously requiring minimal assumptions underlying data distribution tackle problem formulate axis task assumption query specific information complementary subspaces develop regression based approach called efficiently problem finding projections minimize nonparametric conditional entropy estimator experiments show method accurate identifying informative projections dataset correct ones classify query points facilitates visual evaluation users 
hierarchical spike coding sound develop probabilistic generative model representing acoustic event structure multiple scales stage hierarchy stage consists spiking representation encodes sound sparse set kernels different frequencies precisely time time frequency statistical structure stage spikes encoded second stage spiking representation fine scale statistical regularities encoded recurrent interactions stage fitted speech data model encodes acoustic features harmonic stacks sweeps frequency composed represent complex acoustic events model able sounds higher level representation provides significant improvement wavelet thresholding techniques denoising task 
human memory search random walk semantic network human mind remarkable ability store vast amount information memory remarkable ability retrieve needed understanding representations algorithms human memory search potentially useful information retrieval settings including internet search psychological studies clear regularities people search memory clusters semantically related items together findings recently taken evidence human memory search similar animals environments people making rational decision switch cluster related information becomes demonstrate results taken evidence account emerge random walk semantic network random model used internet search engines offers simpler unified account people search memory single process process exploring cluster process switching clusters 
complex inference neural circuits probabilistic population codes topic models experiments demonstrated humans animals typically reason environment ability requires neural code represents probability distributions neural circuits capable implementing operations probabilistic inference proposed probabilistic population coding framework provides efficient neural representation probability distributions broadly consistent physiological measurements capable implementing basic operations probabilistic inference biologically plausible way however experiments corresponding neural models largely focused simple tractable probabilistic computations cue combination coordinate transformations decision making result remains unclear generalize framework complex probabilistic computations address short showing very general approximate inference algorithm known variational bayesian expectation maximization implemented linear framework apply approach generic problem faced any given layer cortex namely identification latent causes complex mixtures spikes identify formal equivalent spike pattern problem topic models used document classification particular latent dirichlet allocation lda construct neural network implementation variational inference learning lda utilizes linear network relies critically non linear operations normalization super linear observed neural circuits demonstrate online learning achieved using variation rule describe work allows deal time varying correlated latent causes 
cost sensitive exploration bayesian reinforcement learning paper consider bayesian reinforcement learning actions costs addition rewards exploration constrained terms expected total cost learning maximize expected long term total reward order formalize cost sensitive exploration constrained markov decision process model environment naturally encode exploration requirements using cost function extend model based method learning environment cost constraints demonstrate cost sensitive exploration behaviour number simulated problems 
learning partially random propose novel stochastic process probability i$ being current state probability i$ follows random edge analyze its properties show its potential exploring graph structures prove proper rates random walk starting set low mostly moreover probabilities vary implementing desirable cluster assumption graph based learning remarkably partially process unifies many popular models arising variety contexts provides insights makes possible transferring findings paradigm another simulation results demonstrate its promising applications graph based learning 
changes highly dependent data unknown number change points problem multiple change point estimation considered sequences unknown number change points consistency framework suggested suitable highly dependent time series asymptotically consistent algorithm proposed order consistency established only assumption required data generated stationary time series distributions modeling independence parametric assumptions data allowed dependent dependence arbitrary form theoretical results complemented experimental evaluations 
probabilistic event cascades alzheimer disease accurate detailed models progression diseases alzheimer important reliable early determination effective paper introduce alzheimer disease probabilistic cascades model generative model latent alzheimer progression dynamics observable data contrast previous works model disease progression fixed ordering events explicitly model variability patients realistic particularly highly detailed disease progression models describe efficient learning algorithms discuss promising experimental results real cohort alzheimer patients alzheimer disease neuroimaging 
efficient direct estimation neural model sensory coding many visual auditory neurons response properties explained pooling rectified responses set self similar linear filters filters using spike triggered averaging estimates only single filter methods spike triggered covariance define multi dimensional response subspace require substantial amounts data produce unique estimates linear filters provide linear basis subspace filters define model cascade linear stage restricted set shifted convolutional common filter nonlinear stage consists filter outputs refer initial elements receptive field second linear stage computes weighted sum responses rectified present method directly fitting model spike data method performs simulated real data primate resulting model outperforms terms cross validated accuracy efficiency 
permutation hashing hashing promising large scale learning binary data preprocessing cost requires applying permutations data testing time expensive data point document image paper develop simple permutation hashing scheme address important issue true preprocessing step comes cost additional hardware implementation reducing permutations just energy efficient important perspective hashing commonly deployed search theoretical probability analysis interesting experiments similarity estimation svm logistic regression confirm theoretical results 
unsupervised learning fine grained object recognition fine grained recognition refers level recognition recognizing different animals differs recognition basic categories humans computers global similarities shape structure shared category differences details object parts suggest key identifying fine grained differences lies finding right alignment image regions contain object parts propose model purpose captures common shape patterns object parts relation shape patterns once image regions aligned extracted features used classification learning model efficient recognition results achieve significantly outperform state art algorithms 
fast bayesian inference non conjugate gaussian process regression present variational inference algorithm gaussian processes non conjugate likelihood functions includes binary multi class classification ordinal regression method constructs convex lower bound optimized using efficient fixed point update method show empirically approach faster existing methods any performance 
imitation learning imitation learning shown successful solving many challenging real world problems approaches give strong performance guarantees training policy iteratively however important guarantees depend policy imitate oracle training data substantial difference oracle ability learner policy space may fail policy low error training set cases propose demonstrates easy learn actions learner approaches oracle reduction learning demonstration online learning prove yield lower regret bound using oracle apply algorithm novel cost sensitive dynamic feature selection problem hard decision problem considers user specified accuracy cost trade experimental results datasets show method outperforms state art imitation learning methods dynamic features selection static feature selection methods 
small variance exponential family dirichlet process mixture models probabilistic non probabilistic learning algorithms arise performing small variance variance particular distributions graphical model zero instance context clustering approach yields precise connections means em algorithms paper explore small variance exponential family dirichlet process dp hierarchical dirichlet process hdp mixture models utilizing connections exponential family distributions bregman derive novel clustering algorithms asymptotic limit dp hdp mixtures feature scalability existing hard clustering methods flexibility bayesian nonparametric models focus special cases analysis discrete data problems including topic modeling demonstrate utility results applying variants algorithms problems arising vision document analysis 
latent factor model highly multi relational data many data social networks preferences knowledge multi relational describe multiple relationships entities large body work focused modeling data considered modeling multiple types relationships jointly existing approaches tend number types grows paper propose method modeling large multi relational datasets possibly thousands relations model based structure captures various orders interaction data shares sparse latent factors different relations illustrate performance approach standard tensor factorization datasets attain outperform state art results finally nlp application demonstrates scalability ability model learn efficient semantically meaningful representations 
entropy using correlated symmetric stable random projections methods efficiently estimating shannon entropy data streams important applications learning data mining network anomaly detections nonnegative data streams method compressed based maximally stable random projections provide accurate estimates shannon entropy using small storage however applicable entries data streams zero common scenario comparing streams paper propose algorithm entropy estimation general data streams allow negative entries method shannon entropy approximated finite difference correlated frequency moments estimated correlated samples symmetric stable random variables experiments confirm method able substantially better approximate shannon entropy compared prior state art 
simultaneously leveraging output task structures multiple output regression multiple output regression models require estimating multiple functions output improve parameter estimation models methods based structural regularization model parameters usually needed paper present multiple output regression model leverages covariance structure functions multiple functions related conditional covariance structure outputs contrast existing methods usually account only structures importantly unlike existing methods structures need known priori model learned data several previously proposed structural regularization based multiple output regression models turn special cases model moreover addition being rich model multiple output regression model used estimating graphical model structure set variables multivariate outputs conditioned another set variables inputs experimental results synthetic real datasets demonstrate effectiveness method 
continuous relaxations discrete monte carlo continuous relaxations play important role discrete optimization seen approximate probabilistic inference show general form gaussian integral trick makes possible transform wide class discrete variable undirected models fully continuous systems continuous representation allows gradient based monte carlo inference results ways estimating normalization constants partition functions general opens number inference difficult discrete systems demonstrate continuous relaxation inference algorithms number problems 
best arm identification unified approach fixed budget fixed confidence study problem identifying best arm stochastic multi armed bandit setting problem studied literature different fixed budget fixed confidence propose unifying approach leads meta algorithm called unified gap based exploration common structure similar theoretical analysis settings prove performance bound versions algorithm showing problems characterized notion complexity show algorithm its theoretical analysis extended account variance arms multiple bandits finally evaluate performance compare number existing fixed budget fixed confidence algorithms 
sample complexity robust pca estimate sample complexity robust estimator generalized version inverse covariance matrix estimator used convex algorithm robust subspace recovery robust pca model assumes sub gaussian underlying distribution main result shows high probability norm difference generalized inverse covariance underlying distribution its estimator size order arbitrarily small probabilistic estimate rate convergence close direct covariance inverse covariance estimation precise probabilistic estimate implies natural settings sample complexity generalized inverse covariance estimation using frobenius norm arbitrarily small whereas sample complexity direct covariance estimation frobenius norm results provide similar rates convergence sample complexity corresponding robust subspace recovery algorithm close pca best knowledge only work analyzing sample complexity any robust pca algorithm 
latent analysis hidden variable model distance metric learning describe latent variable model supervised dimensionality reduction distance metric learning model discovers linear projections high dimensional data distance similarly labeled inputs expand distance differently labeled ones model’s continuous latent variables pairs examples latent space lower dimensionality model differs significantly classical factor analysis posterior distribution latent variables always multivariate gaussian nevertheless show inference completely tractable derive expectation maximization em algorithm parameter estimation compare model approaches distance metric learning model’s main advantage its iteration em algorithm distance metric estimated solving unconstrained squares problem experiments show simple updates highly effective 
trajectory based short probabilistic planning probabilistic planning captures uncertainty plan execution modeling effects actions environment therefore probability different states given state action order compute solution probabilistic planning problem need uncertainty associated different paths initial state goal state several approaches uncertainty proposed consider paths once perform actions sampling paper introduce trajectory based short stochastic path problems novel approach uncertainty probabilistic planning problems states reachable low probability artificial estimate cost reach goal state extend theoretical results short probabilistic proving always asymptotically optimal sufficient conditions structure short empirically compare using trajectory based short previous probabilistic planning state art triangle problems trajectory based outperforms only able scale problem number problem optimal solution contains approximately states 
interpreting prediction markets stochastic approach connections prediction markets learning showing natural class understood performing stochastic mirror descent demands sequentially drawn fixed distribution provides insights price paths may belief distribution relating optimization problem being solved particular show stationary point stochastic process generated equal equilibrium classic analysis together results suggest traditional making mechanisms general purpose learning algorithms guarantees behaviour 
risk aversion multi armed bandits stochastic multi armed bandits objective solve exploration exploitation maximize expected reward many practical problems maximizing expected reward desirable objective paper introduce novel setting based principle risk aversion objective compete arm best risk return trade setting difficult standard multi arm bandit setting due part exploration risk introduces regret associated variability algorithm using variance measure risk introduce algorithms investigate theoretical guarantees report empirical results 
based online learning scheme paper provides best knowledge analysis online learning algorithms multiclass problems matrix taken performance measure work elegant results concentration inequalities concentration inequalities apply matrices precisely matrix establish generalization bounds online learning algorithm show theoretical study learning procedure learning algorithm called learning algorithm shown update equations computed analytically allowing user having any optimization package implement 
cardinality restricted boltzmann machines restricted boltzmann machine popular density model good extracting features main source models model assumption given input hidden units another sparsity hidden representation beneficial its hidden units acquire attractive properties sparse coding constraints added due belief resulting model become intractable work show dynamic programming algorithm developed used implement exact sparsity hidden units expand show pass derivatives layer exact sparsity makes possible fine tune deep belief network consisting sparse hidden layers show sparsity hidden layer improves performance pre trained representations fine tuned model 
generalization bounds domain adaptation paper provide framework study generalization bound learning process domain adaptation loss consider kinds representative domain adaptation settings domain adaptation multiple sources domain adaptation combining source target data particular introduce quantities capture inherent characteristics domains kind domain adaptation based quantities develop specific type deviation inequality inequality achieve corresponding generalization bound based uniform entropy number using generalization bound analyze asymptotic convergence rate convergence learning process kind domain adaptation meanwhile discuss factors affect asymptotic behavior learning process numerical experiments support results 
